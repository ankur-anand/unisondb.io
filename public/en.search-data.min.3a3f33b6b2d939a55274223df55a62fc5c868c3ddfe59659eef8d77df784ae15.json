[{"id":0,"href":"/docs/","title":"Documentation","section":"UnisonDB","content":"UnisonDB Documentation# Welcome to the UnisonDB documentation! This guide will help you understand, deploy, and use UnisonDB effectively.\nDocumentation Sections# Getting Started# Learn how to install, configure, and start using UnisonDB.\nArchitecture# Understand the internal design and components of UnisonDB.\nAPI Reference# Complete reference for HTTP and gRPC APIs.\n"},{"id":1,"href":"/docs/getting-started/","title":"Getting Started","section":"Documentation","content":"Getting Started with UnisonDB# This guide will walk you through installing UnisonDB, configuring it, and running it in both Server and Relayer modes.\nTable of Contents# Prerequisites Installation Running in Server Mode Running in Relayer Mode Basic Operations Next Steps Prerequisites# System Requirements# Operating System: Linux or macOS Go: Version 1.24 or higher Storage: SSD recommended for optimal performance Memory: Minimum 2GB RAM (4GB+ recommended) Required Dependencies# UnisonDB requires two system libraries:\n1. LMDB (Lightning Memory-Mapped Database)# LMDB is the underlying B+Tree storage engine. It must be installed before building UnisonDB.\nOn Ubuntu/Debian:\nsudo apt-get update sudo apt-get install liblmdb-devOn Fedora/RHEL/CentOS:\nsudo yum install lmdb-devel # or sudo dnf install lmdb-develOn macOS (using Homebrew):\nbrew install lmdbOn Arch Linux:\nsudo pacman -S lmdb2. ZeroMQ (Optional, for Change Notifications)# ZeroMQ enables real-time change notifications to local applications. It\u0026rsquo;s optional but recommended for reactive architectures.\nOn Ubuntu/Debian:\nsudo apt-get install libzmq3-devOn Fedora/RHEL/CentOS:\nsudo yum install zeromq-devel # or sudo dnf install zeromq-develOn macOS (using Homebrew):\nbrew install zmqOn Arch Linux:\nsudo pacman -S zeromqVerify Installation:\n# Check LMDB pkg-config --modversion lmdb # Check ZeroMQ pkg-config --modversion libzmqInstallation# Building from Source# UnisonDB requires CGO to be enabled for LMDB bindings.\n# Clone the repository git clone https://github.com/ankur-anand/unisondb.git cd unisondb # Enable CGO (required for LMDB) export CGO_ENABLED=1 # Build the binary go build -o unisondb ./cmd/unisondb # Verify installation ./unisondb --helpExpected output:\nNAME: unisondb - Run UnisonDB USAGE: unisondb [global options] command [command options] [arguments...] COMMANDS: replicator Run in replicator (server) mode relayer Run in relayer (replica) mode fuzzer Run fuzzer for testing (if built with -tags fuzz) help, h Shows a list of commands or help for one command GLOBAL OPTIONS: --config value, -c value Path to TOML config file (default: \u0026#34;./config.toml\u0026#34;) [$UNISON_CONFIG] --env value, -e value Environment: dev, staging, prod (default: \u0026#34;dev\u0026#34;) [$UNISON_ENV] --grpc, -G Enable gRPC server in Relayer Mode (default: false) [$UNISON_GRPC_ENABLED] --help, -h show helpBuilding with Fuzzer Support (Optional)# For testing and development, you can build with fuzzer support:\nCGO_ENABLED=1 go build -tags fuzz -o unisondb ./cmd/unisondbNote: When built with -tags fuzz:\nfuzzer command is available replicator command is disabled for safety To run fuzzer mode:\n./unisondb --config config.toml fuzzerInstallation to System Path (Optional)# # Move to system path sudo mv unisondb /usr/local/bin/ # Verify unisondb --helpRunning in Server Mode# Server Mode runs UnisonDB as a primary instance that accepts writes and serves reads.\n1. Generate TLS Certificates (Recommended)# For production or multi-node setups, generate TLS certificates for gRPC:\nUsing mkcert (easiest for local development):\n# Install mkcert brew install mkcert # macOS # or sudo apt install mkcert # Linux # Install local CA mkcert -install # Create certificate directory mkdir -p certs # Generate certificates cd certs mkcert -key-file server.key -cert-file server.crt localhost 127.0.0.1 ::1 mkcert -key-file client.key -cert-file client.crt localhost 127.0.0.1 ::1 mkcert -cert-file ca.crt -install cd ..Using OpenSSL (alternative):\nmkdir -p certs \u0026amp;\u0026amp; cd certs # Generate CA openssl genrsa -out ca.key 4096 openssl req -new -x509 -key ca.key -sha256 -subj \u0026#34;/CN=UnisonDB CA\u0026#34; -days 365 -out ca.crt # Generate server certificate openssl genrsa -out server.key 4096 openssl req -new -key server.key -out server.csr -subj \u0026#34;/CN=localhost\u0026#34; openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out server.crt -days 365 -sha256 # Generate client certificate openssl genrsa -out client.key 4096 openssl req -new -key client.key -out client.csr -subj \u0026#34;/CN=client\u0026#34; openssl x509 -req -in client.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out client.crt -days 365 -sha256 cd ..2. Create Server Configuration# Create a server.toml configuration file:\n## HTTP API port http_port = 4000 listen_ip = \u0026#34;0.0.0.0\u0026#34; ## gRPC configuration (for replication) [grpc_config] listen_ip = \u0026#34;0.0.0.0\u0026#34; port = 4001 cert_path = \u0026#34;./certs/server.crt\u0026#34; key_path = \u0026#34;./certs/server.key\u0026#34; ca_path = \u0026#34;./certs/ca.crt\u0026#34; # For development only - allows insecure connections allow_insecure = false ## Storage configuration [storage_config] base_dir = \u0026#34;./data/server\u0026#34; namespaces = [\u0026#34;default\u0026#34;, \u0026#34;users\u0026#34;, \u0026#34;products\u0026#34;] bytes_per_sync = \u0026#34;1MB\u0026#34; segment_size = \u0026#34;16MB\u0026#34; arena_size = \u0026#34;4MB\u0026#34; wal_fsync_interval = \u0026#34;1s\u0026#34; ## WAL cleanup (prevents disk exhaustion) [storage_config.wal_cleanup_config] enabled = true interval = \u0026#34;5m\u0026#34; max_age = \u0026#34;1h\u0026#34; min_segments = 5 max_segments = 100 ## Write notification coalescing [write_notify_config] enabled = true max_delay = \u0026#34;20ms\u0026#34; ## ZeroMQ notifications (optional - for local apps) [notifier_config.default] bind_port = 5555 high_water_mark = 1000 linger_time = 1000 [notifier_config.users] bind_port = 5556 high_water_mark = 1000 linger_time = 1000 ## Profiling endpoint [pprof_config] enabled = true port = 6060 ## Logging [log_config] log_level = \u0026#34;info\u0026#34; [log_config.min_level_percents] debug = 100.0 info = 100.0 warn = 100.0 error = 100.03. Start the Server# Server Mode (using replicator command):\n./unisondb --config server.toml replicatorYou can also put the command before flags:\n./unisondb replicator --config server.tomlExpected output:\n__ ___ _ ____ ____ / / / / ___ (_)______ ____ / __ \\/ __ ) / / / / / __ \\/ / ___/ __ \\/ __ \\/ / / / __ | / /_/ / / / / / (__ ) /_/ / / / / /_/ / /_/ / \\____/_/_/ /_/_/____/\\____/_/ /_/_____/_____/ [INFO] service.initialization.completed mode=replicator env=dev config_path=server.toml [INFO] HTTP server listening on :4000 [INFO] gRPC server listening on :4001 [INFO] pprof server listening on :60604. Verify Server is Running# Check HTTP health endpoint:\ncurl http://localhost:4000/healthCheck metrics:\ncurl http://localhost:4000/metricsCheck pprof (for profiling):\n# Open in browser open http://localhost:6060/debug/pprof/ # Or get heap profile curl http://localhost:6060/debug/pprof/heap \u0026gt; heap.prof go tool pprof heap.profDevelopment Mode (Insecure)# For quick local testing without TLS:\nserver-dev.toml:\nhttp_port = 4000 [grpc_config] port = 4001 allow_insecure = true # WARNING: Development only! [storage_config] base_dir = \u0026#34;./data/server\u0026#34; namespaces = [\u0026#34;default\u0026#34;] [log_config] log_level = \u0026#34;debug\u0026#34;./unisondb --config server-dev.toml replicatorRunning in Relayer Mode# Relayer Mode runs UnisonDB as a replica that streams changes from an upstream server.\n1. Create Relayer Configuration# Create a relayer.toml configuration file:\n## HTTP API port (different from server) http_port = 5000 listen_ip = \u0026#34;0.0.0.0\u0026#34; ## gRPC config (can accept downstream relayers) [grpc_config] listen_ip = \u0026#34;0.0.0.0\u0026#34; port = 5001 cert_path = \u0026#34;./certs/server.crt\u0026#34; key_path = \u0026#34;./certs/server.key\u0026#34; ca_path = \u0026#34;./certs/ca.crt\u0026#34; ## Storage configuration [storage_config] base_dir = \u0026#34;./data/relayer\u0026#34; namespaces = [\u0026#34;default\u0026#34;, \u0026#34;users\u0026#34;, \u0026#34;products\u0026#34;] bytes_per_sync = \u0026#34;1MB\u0026#34; # IMPORTANT: segment_size MUST match upstream server! segment_size = \u0026#34;16MB\u0026#34; arena_size = \u0026#34;4MB\u0026#34; ## Relayer configuration - connects to upstream [relayer_config.primary] namespaces = [\u0026#34;default\u0026#34;, \u0026#34;users\u0026#34;, \u0026#34;products\u0026#34;] cert_path = \u0026#34;./certs/client.crt\u0026#34; key_path = \u0026#34;./certs/client.key\u0026#34; ca_path = \u0026#34;./certs/ca.crt\u0026#34; upstream_address = \u0026#34;localhost:4001\u0026#34; segment_lag_threshold = 100 allow_insecure = false ## Optional: Connect to multiple upstreams # [relayer_config.secondary] # namespaces = [\u0026#34;products\u0026#34;] # upstream_address = \u0026#34;other-server:4001\u0026#34; # cert_path = \u0026#34;./certs/client.crt\u0026#34; # key_path = \u0026#34;./certs/client.key\u0026#34; # ca_path = \u0026#34;./certs/ca.crt\u0026#34; ## Global rate limiter for replication [wal_io_global_limiter] enable = true burst = 1000 rate_limit = 10000 # 10K ops/sec max ## ZeroMQ notifications (optional) [notifier_config.default] bind_port = 6555 high_water_mark = 1000 linger_time = 1000 ## Logging [log_config] log_level = \u0026#34;info\u0026#34; [log_config.min_level_percents] debug = 1.0 # Sample 1% of debug logs info = 10.0 # Sample 10% of info logs warn = 100.0 error = 100.0 ## Rate limiter for retries [limiter] interval = \u0026#34;500ms\u0026#34; burst = 202. Start the Relayer# Start relayer:\n./unisondb --config relayer.toml relayerOr with command first:\n./unisondb relayer --config relayer.tomlExpected output:\n__ ___ _ ____ ____ / / / / ___ (_)______ ____ / __ \\/ __ ) / / / / / __ \\/ / ___/ __ \\/ __ \\/ / / / __ | / /_/ / / / / / (__ ) /_/ / / / / /_/ / /_/ / \\____/_/_/ /_/_/____/\\____/_/ /_/_____/_____/ [INFO] service.initialization.completed mode=relayer env=dev config_path=relayer.toml [INFO] HTTP server listening on :5000 [INFO] gRPC server listening on :5001 [INFO] Relayer \u0026#39;primary\u0026#39; connected to upstream localhost:4001 [INFO] Streaming namespace \u0026#39;default\u0026#39; from segment 0 [INFO] Streaming namespace \u0026#39;users\u0026#39; from segment 0 [INFO] Streaming namespace \u0026#39;products\u0026#39; from segment 03. Enable gRPC Server on Relayer (Multi-Hop)# To allow downstream relayers to connect to this relayer:\n./unisondb --config relayer.toml --grpc relayerOr with command first:\n./unisondb relayer --config relayer.toml --grpcThis enables the relayer to act as both a consumer (from upstream) and a producer (to downstream).\nDevelopment Mode (Insecure)# relayer-dev.toml:\nhttp_port = 5000 [grpc_config] port = 5001 [storage_config] base_dir = \u0026#34;./data/relayer\u0026#34; namespaces = [\u0026#34;default\u0026#34;] segment_size = \u0026#34;16MB\u0026#34; # Must match server! [relayer_config.primary] namespaces = [\u0026#34;default\u0026#34;] upstream_address = \u0026#34;localhost:4001\u0026#34; allow_insecure = true # WARNING: Development only! segment_lag_threshold = 100 [log_config] log_level = \u0026#34;debug\u0026#34;./unisondb --config relayer-dev.toml relayerBasic Operations# Writing Data# Key-Value Write (via HTTP):\n# Put a key-value pair curl -X POST http://localhost:4000/api/v1/namespaces/default/kv \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;key\u0026#34;: \u0026#34;user:123\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;eyJuYW1lIjoiQWxpY2UiLCJlbWFpbCI6ImFsaWNlQGV4YW1wbGUuY29tIn0=\u0026#34; }\u0026#39;Batch Write:\ncurl -X POST http://localhost:4000/api/v1/namespaces/default/kv/batch \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;operations\u0026#34;: [ {\u0026#34;key\u0026#34;: \u0026#34;user:1\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;...\u0026#34;}, {\u0026#34;key\u0026#34;: \u0026#34;user:2\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;...\u0026#34;}, {\u0026#34;key\u0026#34;: \u0026#34;user:3\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;...\u0026#34;} ] }\u0026#39;Reading Data# Read from Server:\ncurl http://localhost:4000/api/v1/namespaces/default/kv/user:123Read from Relayer (same API):\ncurl http://localhost:5000/api/v1/namespaces/default/kv/user:123Subscribing to Changes (ZeroMQ)# Python example (install pyzmq first):\nimport zmq context = zmq.Context() socket = context.socket(zmq.SUB) # Subscribe to \u0026#39;default\u0026#39; namespace socket.connect(\u0026#34;tcp://localhost:5555\u0026#34;) socket.setsockopt(zmq.SUBSCRIBE, b\u0026#34;\u0026#34;) # Subscribe to all messages print(\u0026#34;Listening for changes on namespace \u0026#39;default\u0026#39;...\u0026#34;) while True: message = socket.recv() print(f\u0026#34;Change notification: {message}\u0026#34;)Run the subscriber:\npython subscriber.pyNow any writes to the default namespace will trigger notifications!\nMonitoring Replication# Check replication lag (on relayer):\ncurl http://localhost:5000/metrics | grep replication_lagCheck segment offsets:\ncurl http://localhost:5000/metrics | grep segment_offsetCommon Deployment Patterns# 1. Single Server (Development)# # Terminal 1: Start server ./unisondb --config server-dev.toml replicator2. Server + Single Relayer (Read Scaling)# # Terminal 1: Start server ./unisondb --config server.toml replicator # Terminal 2: Start relayer ./unisondb --config relayer.toml relayer3. Server + Multiple Relayers (Edge Computing)# # Terminal 1: Start server ./unisondb --config server.toml replicator # Terminal 2: Start relayer 1 ./unisondb --config relayer1.toml relayer # Terminal 3: Start relayer 2 ./unisondb --config relayer2.toml relayer # Terminal 4: Start relayer 3 ./unisondb --config relayer3.toml relayer4. Multi-Hop (Relayer â†’ Relayer)# # Terminal 1: Primary server ./unisondb --config server.toml replicator # Terminal 2: L1 relayer (with gRPC enabled for downstream) ./unisondb --config relayer-l1.toml --grpc relayer # Terminal 3: L2 relayer (connects to L1) # Update relayer-l2.toml upstream_address to point to L1 (localhost:5001) ./unisondb --config relayer-l2.toml relayerTroubleshooting# CGO Errors# Error: # pkg-config --cflags lmdb: Package lmdb was not found\nSolution: Install LMDB development libraries:\n# Ubuntu/Debian sudo apt-get install liblmdb-dev # macOS brew install lmdbZeroMQ Errors# Error: package zmq: C source files not allowed when not using cgo\nSolution: Ensure CGO is enabled:\nexport CGO_ENABLED=1 go build -o unisondb ./cmd/unisondbIf ZeroMQ is not needed, you can disable notifier config in your TOML.\nCertificate Errors# Error: x509: certificate signed by unknown authority\nSolutions:\nUse allow_insecure = true (development only) Ensure ca_path points to the correct CA certificate Verify certificate paths are correct in config Replication Not Starting# Check:\nServer is running and gRPC port is accessible Namespaces match between server and relayer segment_size matches on both server and relayer Certificates are valid and paths are correct Upstream address is reachable Debug:\n# Check server gRPC port nc -zv localhost 4001 # Check logs with debug level ./unisondb --config relayer.toml --env dev relayerPort Already in Use# Error: bind: address already in use\nSolution: Change ports in config or kill existing process:\n# Find process using port 4000 lsof -i :4000 # Kill process kill -9 \u0026lt;PID\u0026gt;Environment Variables# Configuration values can be overridden with environment variables:\n# Config file path export UNISON_CONFIG=/etc/unisondb/config.toml # Environment export UNISON_ENV=production # Enable gRPC in relayer mode export UNISON_GRPC_ENABLED=true # Start with environment variables ./unisondb relayer # or ./unisondb --grpc relayerNext Steps# Now that you have UnisonDB running, explore:\nConfiguration Guide - Detailed configuration options Architecture Overview - Understanding UnisonDB\u0026rsquo;s design API Reference - HTTP and gRPC API documentation Replication Guide - Advanced replication topologies Operations Guide - Production deployment best practices Performance Tuning - Optimizing for your workload Quick Reference# Server Mode# # Flags before command ./unisondb --config server.toml replicator # Command before flags (alternative) ./unisondb replicator --config server.tomlRelayer Mode# # Flags before command ./unisondb --config relayer.toml relayer # Command before flags (alternative) ./unisondb relayer --config relayer.tomlRelayer with gRPC (Multi-Hop)# # Flags before command ./unisondb --config relayer.toml --grpc relayer # Command before flags (alternative) ./unisondb relayer --config relayer.toml --grpcCommon Config Locations# ./config.toml # Default /etc/unisondb/config.toml ~/unisondb/config.tomlDefault Ports# HTTP API: 4000 (server), 5000 (relayer) gRPC: 4001 (server), 5001 (relayer) PProf: 6060 ZeroMQ: 5555+ (per namespace) Happy building with UnisonDB! ğŸš€\n"},{"id":2,"href":"/docs/api/http-api/","title":"HTTP API Reference","section":"API Reference","content":"HTTP API Reference# Complete reference for UnisonDB\u0026rsquo;s HTTP REST API.\nBase URL# http://localhost:4000/api/v1/{namespace}Data Encoding# All binary values must be base64-encoded:\n# Encode a value echo -n \u0026#34;hello world\u0026#34; | base64 # Output: aGVsbG8gd29ybGQ= # Use in request curl -X PUT http://localhost:4000/api/v1/default/kv/greeting \\ -d \u0026#39;{\u0026#34;value\u0026#34;:\u0026#34;aGVsbG8gd29ybGQ=\u0026#34;}\u0026#39;Key-Value Operations# Put KV# Store a key-value pair.\nRequest:\nPUT /api/v1/{namespace}/kv/{key} Content-Type: application/json { \u0026#34;value\u0026#34;: \u0026#34;base64-encoded-value\u0026#34; }Example:\ncurl -X PUT http://localhost:4000/api/v1/default/kv/user:123 \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;value\u0026#34;: \u0026#34;eyJuYW1lIjoiSm9obiIsImFnZSI6MzB9\u0026#34; }\u0026#39;Response (200 OK):\n{ \u0026#34;success\u0026#34;: true }Errors:\n400 Bad Request: Invalid base64 encoding 404 Not Found: Namespace not found 500 Internal Server Error: Engine error Get KV# Retrieve a value by key.\nRequest:\nGET /api/v1/{namespace}/kv/{key}Example:\ncurl http://localhost:4000/api/v1/default/kv/user:123Response (200 OK):\n{ \u0026#34;value\u0026#34;: \u0026#34;eyJuYW1lIjoiSm9obiIsImFnZSI6MzB9\u0026#34;, \u0026#34;found\u0026#34;: true }Response (404 Not Found):\n{ \u0026#34;value\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;found\u0026#34;: false } Delete KV# Delete a key.\nRequest:\nDELETE /api/v1/{namespace}/kv/{key}Example:\ncurl -X DELETE http://localhost:4000/api/v1/default/kv/user:123Response (200 OK):\n{ \u0026#34;success\u0026#34;: true } Batch KV Operations# Perform multiple operations in one request.\nBatch Put# Request:\nPOST /api/v1/{namespace}/kv/batch Content-Type: application/json { \u0026#34;operation\u0026#34;: \u0026#34;put\u0026#34;, \u0026#34;items\u0026#34;: [ {\u0026#34;key\u0026#34;: \u0026#34;key1\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;dmFsdWUx\u0026#34;}, {\u0026#34;key\u0026#34;: \u0026#34;key2\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;dmFsdWUy\u0026#34;} ] }Example:\ncurl -X POST http://localhost:4000/api/v1/default/kv/batch \\ -d \u0026#39;{ \u0026#34;operation\u0026#34;: \u0026#34;put\u0026#34;, \u0026#34;items\u0026#34;: [ {\u0026#34;key\u0026#34;: \u0026#34;user:1\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;dXNlcjE=\u0026#34;}, {\u0026#34;key\u0026#34;: \u0026#34;user:2\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;dXNlcjI=\u0026#34;} ] }\u0026#39;Response (200 OK):\n{ \u0026#34;success\u0026#34;: true, \u0026#34;processed\u0026#34;: 2 }Batch Get# Request:\nPOST /api/v1/{namespace}/kv/batch Content-Type: application/json { \u0026#34;operation\u0026#34;: \u0026#34;get\u0026#34;, \u0026#34;keys\u0026#34;: [\u0026#34;key1\u0026#34;, \u0026#34;key2\u0026#34;] }Response (200 OK):\n{ \u0026#34;success\u0026#34;: true, \u0026#34;items\u0026#34;: [ {\u0026#34;key\u0026#34;: \u0026#34;key1\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;dmFsdWUx\u0026#34;, \u0026#34;found\u0026#34;: true}, {\u0026#34;key\u0026#34;: \u0026#34;key2\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;found\u0026#34;: false} ] }Batch Delete# Request:\nPOST /api/v1/{namespace}/kv/batch Content-Type: application/json { \u0026#34;operation\u0026#34;: \u0026#34;delete\u0026#34;, \u0026#34;keys\u0026#34;: [\u0026#34;key1\u0026#34;, \u0026#34;key2\u0026#34;] }Response (200 OK):\n{ \u0026#34;success\u0026#34;: true, \u0026#34;processed\u0026#34;: 2 } Wide-Column Operations# Put Row# Store a row with multiple columns.\nRequest:\nPUT /api/v1/{namespace}/row/{rowKey} Content-Type: application/json { \u0026#34;columns\u0026#34;: { \u0026#34;column1\u0026#34;: \u0026#34;base64-value1\u0026#34;, \u0026#34;column2\u0026#34;: \u0026#34;base64-value2\u0026#34; } }Example:\ncurl -X PUT http://localhost:4000/api/v1/default/row/user:john \\ -d \u0026#39;{ \u0026#34;columns\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Sm9obiBEb2U=\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;am9obkBleGFtcGxlLmNvbQ==\u0026#34;, \u0026#34;age\u0026#34;: \u0026#34;MzA=\u0026#34; } }\u0026#39;Response (200 OK):\n{ \u0026#34;success\u0026#34;: true } Get Row# Retrieve all columns for a row.\nRequest:\nGET /api/v1/{namespace}/row/{rowKey}Example:\ncurl http://localhost:4000/api/v1/default/row/user:johnResponse (200 OK):\n{ \u0026#34;rowKey\u0026#34;: \u0026#34;user:john\u0026#34;, \u0026#34;columns\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Sm9obiBEb2U=\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;am9obkBleGFtcGxlLmNvbQ==\u0026#34;, \u0026#34;age\u0026#34;: \u0026#34;MzA=\u0026#34; }, \u0026#34;found\u0026#34;: true } Get Row Columns# Retrieve specific columns only.\nRequest:\nGET /api/v1/{namespace}/row/{rowKey}?columns=col1,col2Example:\ncurl \u0026#34;http://localhost:4000/api/v1/default/row/user:john?columns=name,email\u0026#34;Response (200 OK):\n{ \u0026#34;rowKey\u0026#34;: \u0026#34;user:john\u0026#34;, \u0026#34;columns\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Sm9obiBEb2U=\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;am9obkBleGFtcGxlLmNvbQ==\u0026#34; }, \u0026#34;found\u0026#34;: true } Delete Row# Delete an entire row.\nRequest:\nDELETE /api/v1/{namespace}/row/{rowKey}Example:\ncurl -X DELETE http://localhost:4000/api/v1/default/row/user:johnResponse (200 OK):\n{ \u0026#34;success\u0026#34;: true } Delete Row Columns# Delete specific columns from a row.\nRequest:\nDELETE /api/v1/{namespace}/row/{rowKey}/columns?columns=col1,col2Example:\ncurl -X DELETE \u0026#34;http://localhost:4000/api/v1/default/row/user:john/columns?columns=age,city\u0026#34;Response (200 OK):\n{ \u0026#34;success\u0026#34;: true } Batch Row Operations# Batch Put Rows# Request:\nPOST /api/v1/{namespace}/row/batch Content-Type: application/json { \u0026#34;operation\u0026#34;: \u0026#34;put\u0026#34;, \u0026#34;rows\u0026#34;: [ { \u0026#34;rowKey\u0026#34;: \u0026#34;user:1\u0026#34;, \u0026#34;columns\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;QWxpY2U=\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;YWxpY2VAZXhhbXBsZS5jb20=\u0026#34; } } ] }Response (200 OK):\n{ \u0026#34;success\u0026#34;: true, \u0026#34;processed\u0026#34;: 1 }Batch Get Rows# Request:\nPOST /api/v1/{namespace}/row/batch Content-Type: application/json { \u0026#34;operation\u0026#34;: \u0026#34;get\u0026#34;, \u0026#34;rowKeys\u0026#34;: [\u0026#34;user:1\u0026#34;, \u0026#34;user:2\u0026#34;] }Response (200 OK):\n{ \u0026#34;success\u0026#34;: true, \u0026#34;rows\u0026#34;: [ { \u0026#34;rowKey\u0026#34;: \u0026#34;user:1\u0026#34;, \u0026#34;columns\u0026#34;: {\u0026#34;name\u0026#34;: \u0026#34;QWxpY2U=\u0026#34;}, \u0026#34;found\u0026#34;: true }, { \u0026#34;rowKey\u0026#34;: \u0026#34;user:2\u0026#34;, \u0026#34;columns\u0026#34;: {}, \u0026#34;found\u0026#34;: false } ] } Large Object (LOB) Operations# Put LOB# Upload a large binary object. Check Transaction API.\nGet LOB# Download a large binary object.\nRequest:\nGET /api/v1/{namespace}/lob?key={key}Example:\ncurl \u0026#34;http://localhost:4000/api/v1/default/lob?key=file:doc.pdf\u0026#34; \\ --output document.pdfResponse: Binary data stream\nTransaction Operations# Transactions allow atomic operations across multiple keys.\nTransaction Lifecycle# 1. BEGIN â†’ Get transaction ID 2. APPEND â†’ Add operations (multiple times) 3. COMMIT â†’ Apply atomically OR 3. ABORT â†’ Cancel transactionBegin Transaction# Start a new transaction.\nRequest:\nPOST /api/v1/{namespace}/tx/begin Content-Type: application/json { \u0026#34;operation\u0026#34;: \u0026#34;put\u0026#34;, \u0026#34;entryType\u0026#34;: \u0026#34;kv\u0026#34; }Parameters:\noperation: \u0026quot;put\u0026quot;, \u0026quot;update\u0026quot;, or \u0026quot;delete\u0026quot; entryType: \u0026quot;kv\u0026quot;, \u0026quot;row\u0026quot;, or \u0026quot;lob\u0026quot; Example:\ncurl -X POST http://localhost:4000/api/v1/default/tx/begin \\ -d \u0026#39;{ \u0026#34;operation\u0026#34;: \u0026#34;put\u0026#34;, \u0026#34;entryType\u0026#34;: \u0026#34;kv\u0026#34; }\u0026#39;Response (200 OK):\n{ \u0026#34;txnId\u0026#34;: \u0026#34;2a3b4c5d6e7f8g9h0i1j2k3l4m5n6o7p\u0026#34;, \u0026#34;success\u0026#34;: true }Save the txnId - you\u0026rsquo;ll need it for subsequent requests!\nAppend KV to Transaction# Add a key-value operation to the transaction.\nRequest:\nPOST /api/v1/{namespace}/tx/{txnId}/kv Content-Type: application/json { \u0026#34;key\u0026#34;: \u0026#34;mykey\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;bXl2YWx1ZQ==\u0026#34; }Example:\n# Use the txnId from BEGIN response curl -X POST http://localhost:4000/api/v1/default/tx/2a3b4c.../kv \\ -d \u0026#39;{ \u0026#34;key\u0026#34;: \u0026#34;account:alice\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;MTAwMA==\u0026#34; }\u0026#39;Response (200 OK):\n{ \u0026#34;success\u0026#34;: true }Call this endpoint multiple times to add multiple operations to the same transaction.\nAppend Row to Transaction# Add a row operation to the transaction.\nRequest:\nPOST /api/v1/{namespace}/tx/{txnId}/row Content-Type: application/json { \u0026#34;rowKey\u0026#34;: \u0026#34;user:1\u0026#34;, \u0026#34;columns\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;QWxpY2U=\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;YWN0aXZl\u0026#34; } }Example:\ncurl -X POST http://localhost:4000/api/v1/default/tx/{txnId}/row \\ -d \u0026#39;{ \u0026#34;rowKey\u0026#34;: \u0026#34;user:charlie\u0026#34;, \u0026#34;columns\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Q2hhcmxpZQ==\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;Y2hhcmxpZUBleGFtcGxlLmNvbQ==\u0026#34; } }\u0026#39;Response (200 OK):\n{ \u0026#34;success\u0026#34;: true } Append LOB to Transaction# Add a large object to the transaction.\nRequest:\nPOST /api/v1/{namespace}/tx/{txnId}/lob?key={key} Content-Type: application/octet-stream \u0026lt;binary data\u0026gt;Example:\ncurl -X POST \u0026#34;http://localhost:4000/api/v1/default/tx/{txnId}/lob?key=file:backup.tar.gz\u0026#34; \\ --data-binary @backup.tar.gzResponse (200 OK):\n{ \u0026#34;success\u0026#34;: true } Commit Transaction# Apply all operations atomically.\nRequest:\nPOST /api/v1/{namespace}/tx/{txnId}/commitExample:\ncurl -X POST http://localhost:4000/api/v1/default/tx/2a3b4c.../commitResponse (200 OK):\n{ \u0026#34;success\u0026#34;: true }After commit:\nAll operations are applied atomically Transaction ID is no longer valid Data is durable and replicated Abort Transaction# Cancel the transaction without applying changes.\nRequest:\nPOST /api/v1/{namespace}/tx/{txnId}/abortExample:\ncurl -X POST http://localhost:4000/api/v1/default/tx/2a3b4c.../abortResponse (200 OK):\n{ \u0026#34;success\u0026#34;: true }After abort:\nNo operations are applied Transaction ID is no longer valid All buffered changes are discarded Metadata Operations# Get Current Offset# Get the current WAL position.\nRequest:\nGET /api/v1/{namespace}/offsetExample:\ncurl http://localhost:4000/api/v1/default/offsetResponse (200 OK):\n{ \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;segmentId\u0026#34;: 5, \u0026#34;offset\u0026#34;: 12345 } Get Engine Statistics# Get engine performance statistics.\nRequest:\nGET /api/v1/{namespace}/statsExample:\ncurl http://localhost:4000/api/v1/default/statsResponse (200 OK):\n{ \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;opsReceived\u0026#34;: 15234, \u0026#34;opsFlushed\u0026#34;: 15100, \u0026#34;currentSegment\u0026#34;: 5, \u0026#34;currentOffset\u0026#34;: 12345, \u0026#34;lastFlushTime\u0026#34;: \u0026#34;2024-01-15T10:30:45Z\u0026#34; } Get Checkpoint# Get the last checkpoint position.\nRequest:\nGET /api/v1/{namespace}/checkpointExample:\ncurl http://localhost:4000/api/v1/default/checkpointResponse (200 OK):\n{ \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;recordProcessed\u0026#34;: 15000, \u0026#34;segmentId\u0026#34;: 5, \u0026#34;offset\u0026#34;: 12000 } Error Responses# All errors follow this format:\n{ \u0026#34;error\u0026#34;: \u0026#34;error message description\u0026#34; }HTTP Status Codes# Code Meaning Example 200 Success Operation completed 400 Bad Request Invalid base64, malformed JSON 404 Not Found Namespace not found, key not found, transaction not found 500 Internal Server Error Engine error, disk full, WAL error Common Errors# Namespace not found:\n{ \u0026#34;error\u0026#34;: \u0026#34;namespace not found: invalid-ns\u0026#34; }Status: 404 Not Found\nTransaction not found:\n{ \u0026#34;error\u0026#34;: \u0026#34;transaction not found: 2a3b4c5d...\u0026#34; }Status: 404 Not Found\nInvalid base64:\n{ \u0026#34;error\u0026#34;: \u0026#34;invalid base64 encoding\u0026#34; }Status: 400 Bad Request\nEngine error:\n{ \u0026#34;error\u0026#34;: \u0026#34;failed to write: disk full\u0026#34; }Status: 500 Internal Server Error\nComplete Transaction Example# #!/bin/bash # 1. Begin transaction RESPONSE=$(curl -s -X POST http://localhost:4000/api/v1/default/tx/begin \\ -d \u0026#39;{\u0026#34;operation\u0026#34;:\u0026#34;put\u0026#34;,\u0026#34;entryType\u0026#34;:\u0026#34;kv\u0026#34;}\u0026#39;) TXN_ID=$(echo $RESPONSE | jq -r \u0026#39;.txnId\u0026#39;) echo \u0026#34;Transaction ID: $TXN_ID\u0026#34; # 2. Append multiple operations curl -X POST http://localhost:4000/api/v1/default/tx/$TXN_ID/kv \\ -d \u0026#39;{\u0026#34;key\u0026#34;:\u0026#34;account:alice:balance\u0026#34;,\u0026#34;value\u0026#34;:\u0026#34;MTAwMA==\u0026#34;}\u0026#39; curl -X POST http://localhost:4000/api/v1/default/tx/$TXN_ID/kv \\ -d \u0026#39;{\u0026#34;key\u0026#34;:\u0026#34;account:bob:balance\u0026#34;,\u0026#34;value\u0026#34;:\u0026#34;MjAwMA==\u0026#34;}\u0026#39; curl -X POST http://localhost:4000/api/v1/default/tx/$TXN_ID/kv \\ -d \u0026#39;{\u0026#34;key\u0026#34;:\u0026#34;transfer:log:123\u0026#34;,\u0026#34;value\u0026#34;:\u0026#34;YWxpY2UgLT4gYm9iOiAxMDA=\u0026#34;}\u0026#39; # 3. Commit curl -X POST http://localhost:4000/api/v1/default/tx/$TXN_ID/commit echo \u0026#34;Transaction committed!\u0026#34; # 4. Verify curl http://localhost:4000/api/v1/default/kv/account:alice:balance "},{"id":3,"href":"/docs/architecture/","title":"Architecture","section":"Documentation","content":"Architecture Overview# UnisonDB is a log-native database that replicates like a message bus, combining database semantics with streaming mechanics. It operates in two modes: Server Mode (primary/standalone) and Relayer Mode (replica/streaming).\nOperational Modes# Server Mode# Primary instance that accepts writes and serves reads. This is the source of truth for data.\nCharacteristics:\nAccepts write operations via HTTP APIs Maintains the authoritative Write-Ahead Log (WAL) Streams WAL changes to relayers via gRPC Can publish local change notifications via ZeroMQ Stores data in LMDB B+Tree Relayer Mode# Replica instance that streams changes from one or more upstream servers and serves local reads.\nCharacteristics:\nConnects to upstream server(s) via gRPC Receives and applies WAL segment streams Read-only local access (no direct writes) Can relay to downstream instances Can publish local change notifications via ZeroMQ Provides data locality and read scalability Communication Architecture# UnisonDB uses two distinct communication channels for different purposes:\n1. gRPC - Replication Channel (Node-to-Node)# Purpose: Stream WAL segments between UnisonDB nodes for replication.\nUse Cases:\nServer â†’ Relayer replication (primary to replica) Multi-hop replication (relayer â†’ relayer) Cross-region data distribution Hub-and-spoke topologies Characteristics:\nBidirectional streaming RPC mTLS authentication. Segment-based streaming with flow control Security:\nTLS/mTLS required in production Can run insecure mode (development only) 2. ZeroMQ - Notification Channel (Interprocess Communication)# Purpose: Publish real-time change notifications to local applications on the same machine.\nUse Cases:\nReact to data changes in real-time Trigger local application logic on writes Build event-driven architectures Cache invalidation signals Local microservices coordination Characteristics:\nPUB/SUB pattern (one-to-many) Per-namespace sockets (isolation) Local IPC only (same machine) Fire-and-forget (no acknowledgments) Typical Flow:\nApplication A writes to UnisonDB â†’ UnisonDB writes to WAL â†’ ZeroMQ publishes notification â†’ Applications B, C, D receive notificationHigh-Level Architecture# +--------------------------------------------------------------------------------------+ | UnisonDB Instance | +--------------------------------------------------------------------------------------+ | Client / Integration Layer | | | | +------------------+ +------------------+ +------------------+ | | | HTTP / REST | | Transaction API | | Admin / Stats | | | +------------------+ +------------------+ +------------------+ | | | | | | +--------------+------------------------+-----------------------+----------------------+ | Multi-Model Data Interfaces | | | | +-------------+ +----------------+ +------------------+ | | | Key-Value | | Wide-Column | | LOB (Chunked TXN)| | | +-------------+ +----------------+ +------------------+ | | \\__________ _____________/ | | \\ / | +--------------------------v-----------------------------------------------------------+ | Storage Engine (dbkernel) | | | | Write Path: | | +----------------+ +----------------+ +----------------+ | | | WALFS | -\u0026gt; | MemTable | -\u0026gt; | B+Tree Store | | | | (append-only, | | (in-memory) | | (LMDB / Bolt) | | | | mmap, ordered)| +----------------+ +----------------+ | | +----------------+ * | | * | | Read Path: B+Tree (+MemTable overlay) ---------------------------------------*----\u0026gt;| | | +--------------------------------------------------------------------------------------+ | Replication \u0026amp; Notifications | | | | +-----------------------+ +-----------------------------+ | | | gRPC Replicator | | Change Notifier | | | | - Stream WAL | | - PUB/SUB (ZeroMQ / MQTT) | | | | - Offset catch-up | | - Per-namespace topics | | | | - TLS / mTLS | | - Fire-and-forget delivery | | | +-----------------------+ +-----------------------------+ | | | | | | v v | | +------------------+ +------------------+ | | | Follower Node | | Reactive Clients | | | | (Edge Replica) | | (Dashboards etc.)| | | +------------------+ +------------------+ | +--------------------------------------------------------------------------------------+ Data Flow Summary: 1. Client writes â†’ WALFS (append-only) 2. WALFS â†’ MemTable â†’ B+Tree (commit-ordered) 3. gRPC Replicator streams WAL entries to other nodes 4. Notifier publishes change events to local/remote subscribers 5. Readers query locally from B+Tree + MemTable (no network call)Core Components# [Write-Ahead Log (WAL)]# Transaction logging system that enables streaming replication and crash recovery.\nResponsibilities:\nSequential write log (append-only) Segment-based storage (configurable size) Checksum validation for integrity Source of truth for replication Crash recovery mechanism WAL Lifecycle:\nClient writes â†’ WAL append WAL fsync (durability) MemTable update Background flush to B+Tree Segment cleanup (after checkpoint) [Storage Engine]# Built on B+Trees (LMDB/BOLT-DB).\nResponsibilities:\nMulti-modal data storage (KV, Wide-Column, LOB) Crash recovery via WAL replay [Replication System]# gRPC-based streaming replication with WAL segment fan-out.\nServer Mode (WAL Producer):\nExposes gRPC streaming endpoint Streams WAL segments to connected relayers Monitors replication lag Relayer Mode (WAL Consumer):\nConnects to upstream gRPC endpoints Receives WAL segments in order Applies segments to local storage Can relay to downstream instances Multiple upstream sources supported Replication Guarantees:\nEventual Consistency: All replicas converge to same state Ordered Delivery: Segments applied in correct order At-Least-Once: Retries ensure delivery Gap Detection: Missing segments are requested [Change Notifications]# ZeroMQ-based PUB/SUB system for local interprocess communication.\nArchitecture:\nOne ZeroMQ PUB socket per namespace Local applications subscribe via SUB sockets Notifications triggered on WAL writes Non-blocking, fire-and-forget delivery Notification Flow:\nWrite Request â†’ WAL Append â†’ [optional delay for coalescing] â†’ ZeroMQ Publish â†’ Local SubscribersConfiguration Options:\nbind_port: TCP port for namespace socket high_water_mark: Max queued messages before dropping linger_time: Wait time for pending messages on shutdown Use Case Example:\nNamespace: \u0026#34;inventory\u0026#34; - UnisonDB binds: tcp://*:5555 - Warehouse app subscribes: tcp://localhost:5555 - Web API app subscribes: tcp://localhost:5555 - Both apps receive real-time inventory updatesDesign Principles# 1. Log-Native Design# The Write-Ahead Log is a first-class citizen, not just a recovery mechanism.\nImplications:\nLog is the source of truth: All operations flow through WAL Replication is log streaming: No separate replication protocol needed Recovery is log replay: Database state reconstructed from WAL Time-travel possible: Historic state can be reconstructed from log 2. Multi-Modal Storage# Three storage models share the same underlying B+Tree and WAL.\nKey-Value Storage:\nKey: \u0026#34;user:123\u0026#34; Value: {\u0026#34;name\u0026#34;: \u0026#34;Alice\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;alice@example.com\u0026#34;}Wide-Column Storage (like Cassandra/HBase):\nRowKey: \u0026#34;user:123\u0026#34; Columns: { \u0026#34;profile:name\u0026#34;: \u0026#34;Alice\u0026#34;, \u0026#34;profile:email\u0026#34;: \u0026#34;alice@example.com\u0026#34;, \u0026#34;settings:theme\u0026#34;: \u0026#34;dark\u0026#34; }Large Object (LOB) Storage:\nObjectKey: \u0026#34;video:abc\u0026#34; Chunks: [chunk0, chunk1, chunk2, ...] (streaming support)3. Edge-First Architecture# Designed for edge computing, local-first applications, and distributed topologies.\nCharacteristics:\nHub-and-Spoke: Central primary, many edge replicas Multi-hop: Relayers can relay to other relayers Eventual consistency: Replicas converge over time Offline operation: Edge nodes work independently Local reads: Read from nearby replica (data locality) Example Topology:\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Primary (US) â”‚ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ gRPC replication â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â” â†“ â†“ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Relayer â”‚ â”‚ Relayer â”‚ â”‚ (Europe) â”‚ â”‚ (Asia) â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ ZeroMQ ZeroMQ (local) (local) â†“ â†“ Local Apps Local Apps4. Separation of Concerns# gRPC for Distribution (between machines):\nDurable replication Network-tolerant Authenticated and encrypted Handles intermittent connectivity ZeroMQ for Local Reactivity (within machine):\nFast interprocess communication Event-driven architecture No network overhead Decoupled local services Data Flow# Write Path (Server Mode)# Client Write Request â†“ HTTP/gRPC API Handler â†“ Transaction Context â†“ Storage Engine (dbkernel) â†“ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ 1. Append to WAL â”‚ â† Sequential write (durability) â”‚ 2. fsync (if configured) â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â†“ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ 3. Update MemTable â”‚ â† In-memory write buffer â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â†“ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ 4. Background flush to â”‚ â† Periodic flush to LMDB â”‚ B+Tree when full â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â†“ Response to Client â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â†“ â†“ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ gRPC Stream to â”‚ â”‚ ZeroMQ Publish â”‚ â”‚ Relayers â”‚ â”‚ to Local Apps â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜Read Path# Client Read Request â†“ HTTP/gRPC API Handler â†“ Storage Engine (dbkernel) â†“ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ 1. Check MemTable â”‚ â† Recent writes (hot data) â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ (if not found) â†“ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ 2. Query B+Tree (LMDB) â”‚ â† Persistent storage â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â†“ Response to ClientReplication Flow (gRPC)# Server Mode (Primary) â”‚ â”‚ gRPC Streaming RPC â”‚ (bidirectional) â†“ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ WAL Segment Streaming â”‚ â”‚ - Segment metadata â”‚ â”‚ - Binary WAL data â”‚ â”‚ - Checksum validation â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ Network (TLS/mTLS) â†“ Relayer Mode (Replica) â”‚ â†“ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ 1. Receive WAL segment â”‚ â”‚ 2. Validate checksum â”‚ â”‚ 3. Write to local WAL â”‚ â”‚ 4. Apply to MemTable â”‚ â”‚ 5. Flush to B+Tree â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â†“ â†“ Can relay to Can notify downstream local apps relayers via ZeroMQNotification Flow (ZeroMQ)# WAL Write Event â†“ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Write Notify Coalescer â”‚ â† Optional delay (max_delay) â”‚ (reduces notification spam) â”‚ to batch rapid writes â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â†“ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ ZeroMQ PUB Socket â”‚ â† Per-namespace socket â”‚ tcp://*:{bind_port} â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ Local IPC â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â†“ â†“ â†“ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ App A â”‚ â”‚ App B â”‚ â”‚ App C â”‚ â”‚ (SUB) â”‚ â”‚ (SUB) â”‚ â”‚ (SUB) â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ tcp://localhost:{bind_port}Storage Layout# Directory Structure# data/ â”œâ”€â”€ default/ # Namespace: default â”‚ â”œâ”€â”€ wal/ # Write-Ahead Log directory â”‚ â”‚ â”œâ”€â”€ segment-00000000 # WAL segment 0 (16MB) â”‚ â”‚ â”œâ”€â”€ segment-00000001 # WAL segment 1 â”‚ â”‚ â”œâ”€â”€ segment-00000002 # WAL segment 2 â”‚ â”‚ â””â”€â”€ ... â”‚ â”œâ”€â”€ db/ # LMDB B+Tree database â”‚ â”‚ â”œâ”€â”€ data.mdb # LMDB data file â”‚ â”‚ â””â”€â”€ lock.mdb # LMDB lock file â”‚ â””â”€â”€ checkpoint # Checkpoint metadata â”‚ â””â”€â”€ last_applied # Last applied WAL offset â”œâ”€â”€ users/ # Namespace: users â”‚ â”œâ”€â”€ wal/ â”‚ â”œâ”€â”€ db/ â”‚ â””â”€â”€ checkpoint â””â”€â”€ metrics/ # Namespace: metrics â”œâ”€â”€ wal/ â”œâ”€â”€ db/ â””â”€â”€ checkpointKey Encoding# Internal key structure varies by storage model:\nKey-Value: Internal Key: \u0026lt;user-provided-key\u0026gt; Example: \u0026#34;user:123\u0026#34; Wide-Column (Row): Internal Key: \u0026lt;rowKey\u0026gt;:\u0026lt;columnName\u0026gt; Example: \u0026#34;user:123:profile:name\u0026#34; Large Object (LOB): Internal Key: \u0026lt;objectKey\u0026gt;:chunk:\u0026lt;chunkIndex\u0026gt; Example: \u0026#34;video:abc:chunk:0000000042\u0026#34;WAL Segment Format# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ WAL Segment Header â”‚ â”‚ - Magic Number (validation) â”‚ â”‚ - Segment Number â”‚ â”‚ - Timestamp â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ WAL Entry 1 â”‚ â”‚ - Entry Type (Put/Delete/Txn) â”‚ â”‚ - Namespace â”‚ â”‚ - Key Length â”‚ â”‚ - Value Length â”‚ â”‚ - Key Data â”‚ â”‚ - Value Data â”‚ â”‚ - Checksum (CRC32) â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ WAL Entry 2 â”‚ â”‚ ... â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ WAL Entry N â”‚ â”‚ ... â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜Concurrency Model# Per-Namespace Isolation:\nDifferent namespaces are independent Concurrent access to different namespaces Failure Modes and Recovery# Crash Recovery (Both Modes)# Server Mode Recovery:\nScan WAL: Identify all uncommitted operations Replay WAL: Re-apply operations to MemTable and B+Tree Validate Checkpoint: Ensure consistency with last checkpoint Resume Operations: Begin accepting requests Relayer Mode Recovery:\nScan Local WAL: Determine last applied segment Resume Stream: Reconnect to upstream at last offset Gap Detection: Request missing segments if needed Catch-up: Apply backlog before accepting reads Replication Failure# Segment Gap Detection:\nRelayers track segment sequence numbers Missing segments trigger explicit requests Prevents inconsistent state Deployment Topologies# 1. Single Server# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Primary Server â”‚ â”‚ (Server Mode) â”‚ â”‚ - HTTP API â”‚ â”‚ - Writes OK â”‚ â”‚ - Reads OK â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜Simple deployment for small workloads.\n2. Primary + Replicas (Read Scaling)# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Primary Server â”‚ â”‚ (Server Mode) â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ gRPC replication â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â” â†“ â†“ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Relayer 1 â”‚ â”‚ Relayer 2 â”‚ â”‚ (Read-only) â”‚ â”‚ (Read-only) â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜Primary handles all writes, relayers provide read scalability.\n3. Hub-and-Spoke (Edge Computing)# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Central Hub â”‚ â”‚ (Server Mode) â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ gRPC replication â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â†“ â†“ â†“ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Edge 1 â”‚ â”‚ Edge 2 â”‚ â”‚ Edge 3 â”‚ â”‚Relayer â”‚ â”‚Relayer â”‚ â”‚Relayer â”‚ â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ ZeroMQ ZeroMQ ZeroMQ â†“ â†“ â†“ Local Local Local Apps Apps AppsCentral hub replicates to many edge nodes. Each edge serves local apps via ZeroMQ.\n4. Multi-Hub (Geographic Distribution)# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Hub US-East â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ Hub Europe â”‚ â”‚ (Server Mode)â”‚ gRPC â”‚(Server Mode) â”‚ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ bidirect.â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ gRPCâ”‚ â”‚gRPC â†“ â†“ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ East â”‚ â”‚ Europe â”‚ â”‚Relayersâ”‚ â”‚Relayersâ”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜Multiple hubs for geographic distribution. Each hub can be a server accepting local writes.\n5. Multi-Hop Relay (Deep Edge)# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Primary â”‚ (Server Mode) â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â”‚ gRPC â†“ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Relay L1â”‚ (Relayer Mode) â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â”‚ gRPC â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â” â†“ â†“ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚Relay L2aâ”‚ â”‚Relay L2bâ”‚ (Relayer Mode) â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â”‚ â”‚ ZeroMQ â†“ â†“ Local Local Apps AppsMulti-hop replication for deep edge deployments.\n6. Hybrid: Replication + Local Notifications# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Primary Server â”‚ â”‚ (Server Mode) â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ Storage â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â” â”‚ â”‚ â†“ â†“ â”‚ â”‚ [gRPC] [ZeroMQ] â”‚ â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â””â”€â”€â†’ Local Apps (IPC) â”‚ - Cache service â”‚ - Analytics service â”‚ - Audit logger â†“ Relayers (other machines)Same instance uses gRPC for replication AND ZeroMQ for local notifications.\nSummary# UnisonDB\u0026rsquo;s architecture is built around two key insights:\nDual Communication Channels:\ngRPC for durable, network-based replication between nodes ZeroMQ for fast, local interprocess notifications Dual Operational Modes:\nServer Mode for writable primary instances Relayer Mode for read-scalable replicas This separation allows you to build sophisticated topologies:\nHub-and-spoke for edge computing Multi-region for geographic distribution Local event-driven architectures via ZeroMQ Scalable read replicas via gRPC streaming The log-native design ensures that replication, recovery, and notifications all flow naturally from the Write-Ahead Log, making the system simple to reason about while remaining powerful and flexible.\n"},{"id":4,"href":"/docs/api/","title":"API Reference","section":"Documentation","content":"API Reference# UnisonDB provides API interfaces for client access:\nHTTP REST API# RESTful HTTP API with JSON payloads, supporting:\nKey-Value operations Wide-Column operations Large Object (LOB) operations Stateful transactions Metadata queries Next Steps# HTTP API Reference - Complete HTTP API documentation "},{"id":5,"href":"/docs/getting-started/configurations/","title":"Configuration","section":"Getting Started","content":"Configuration Guide# UnisonDB uses TOML for configuration. This guide covers all available configuration options for both server and relayer modes.\nTable of Contents# Server Mode Relayer Mode Configuration Reference Server Mode# Server mode runs UnisonDB as a primary instance that accepts writes and serves reads. Here\u0026rsquo;s a complete example:\n## Port of the http server http_port = 4000 listen_ip = \u0026#34;0.0.0.0\u0026#34; ## grpc config for replication [grpc_config] listen_ip = \u0026#34;0.0.0.0\u0026#34; port = 4001 # SSL/TLS certificate paths for gRPC server cert_path = \u0026#34;../../certs/server.crt\u0026#34; key_path = \u0026#34;../../certs/server.key\u0026#34; ca_path = \u0026#34;../../certs/ca.crt\u0026#34; # Allow insecure connections (no TLS) - ONLY for development! allow_insecure = false # StorageConfig stores all tunable parameters. [storage_config] base_dir = \u0026#34;/tmp/unisondb/server\u0026#34; # Base directory for storage namespaces = [\u0026#34;default\u0026#34;, \u0026#34;tenant_1\u0026#34;, \u0026#34;tenant_2\u0026#34;, \u0026#34;tenant_3\u0026#34;, \u0026#34;tenant_4\u0026#34;] bytes_per_sync = \u0026#34;1MB\u0026#34; segment_size = \u0026#34;16MB\u0026#34; arena_size = \u0026#34;4MB\u0026#34; wal_fsync_interval = \u0026#34;1s\u0026#34; ## WAL cleanup configuration [storage_config.wal_cleanup_config] enabled = false interval = \u0026#34;5m\u0026#34; max_age = \u0026#34;1h\u0026#34; min_segments = 5 max_segments = 10 ## Write notify config - coalesces notifications from WAL writers to readers [write_notify_config] enabled = true max_delay = \u0026#34;20ms\u0026#34; ## ZeroMQ notifier configuration (per-namespace) ## Publishes change notifications for local application consumption [notifier_config.default] bind_port = 5555 high_water_mark = 1000 linger_time = 1000 [notifier_config.tenant_1] bind_port = 5556 high_water_mark = 1000 linger_time = 1000 [pprof_config] enabled = true port = 6060 [log_config] log_level = \u0026#34;info\u0026#34; disable_timestamp = false ## This is for grpc logging only - controls sampling percentages per level [log_config.min_level_percents] debug = 100.0 info = 50.0 warn = 100.0 error = 100.0 ## Fuzzer configuration (for testing) [fuzz_config] ops_per_namespace = 400 workers_per_namespace = 50 local_relayer_count = 1000 startup_delay = \u0026#34;10s\u0026#34; enable_read_ops = falseRelayer Mode# Relayer mode runs UnisonDB as a replica that streams changes from one or more upstream servers. This provides read scalability and data locality.\n## Port of the http server http_port = 6000 [grpc_config] port = 6001 cert_path = \u0026#34;../../certs/server.crt\u0026#34; key_path = \u0026#34;../../certs/server.key\u0026#34; ca_path = \u0026#34;../../certs/ca.crt\u0026#34; [storage_config] base_dir = \u0026#34;/tmp/unisondb/relayer\u0026#34; namespaces = [\u0026#34;default\u0026#34;, \u0026#34;tenant_1\u0026#34;, \u0026#34;tenant_2\u0026#34;] bytes_per_sync = \u0026#34;1MB\u0026#34; ## IMPORTANT: segment_size must match upstream server! segment_size = \u0026#34;16MB\u0026#34; arena_size = \u0026#34;4MB\u0026#34; ## Relayer configuration - can have multiple upstreams [relayer_config] [relayer_config.relayer1] namespaces = [\u0026#34;default\u0026#34;, \u0026#34;tenant_1\u0026#34;, \u0026#34;tenant_2\u0026#34;] cert_path = \u0026#34;../../certs/client.crt\u0026#34; key_path = \u0026#34;../../certs/client.key\u0026#34; ca_path = \u0026#34;../../certs/ca.crt\u0026#34; upstream_address = \u0026#34;localhost:4001\u0026#34; segment_lag_threshold = 100 allow_insecure = false # Optional: custom gRPC service config JSON grpc_service_config = \u0026#34;\u0026#34; ## Optional: Add more relayers for different upstream sources [relayer_config.relayer2] namespaces = [\u0026#34;tenant_3\u0026#34;] cert_path = \u0026#34;../../certs/client2.crt\u0026#34; key_path = \u0026#34;../../certs/client2.key\u0026#34; ca_path = \u0026#34;../../certs/ca.crt\u0026#34; upstream_address = \u0026#34;remote-server:4001\u0026#34; segment_lag_threshold = 100 [log_config] log_level = \u0026#34;info\u0026#34; [log_config.min_level_percents] debug = 0.01 info = 1.0 warn = 1.0 error = 1.0 Configuration Reference# Server Configuration# HTTP Server# http_port = 4000 listen_ip = \u0026#34;0.0.0.0\u0026#34;http_port# Type: Integer Default: 4000 Description: Port for the HTTP API server listen_ip# Type: String Default: \u0026quot;0.0.0.0\u0026quot; Description: IP address to bind HTTP server to Note: Use \u0026quot;127.0.0.1\u0026quot; for localhost-only access gRPC Configuration# [grpc_config] listen_ip = \u0026#34;0.0.0.0\u0026#34; port = 4001 cert_path = \u0026#34;/path/to/server.crt\u0026#34; key_path = \u0026#34;/path/to/server.key\u0026#34; ca_path = \u0026#34;/path/to/ca.crt\u0026#34; allow_insecure = falselisten_ip# Type: String Default: \u0026quot;0.0.0.0\u0026quot; Description: IP address to bind gRPC server to port# Type: Integer Default: 4001 Description: Port for the gRPC server (used for replication) cert_path# Type: String Default: \u0026quot;\u0026quot; Description: Path to TLS certificate file (PEM format) Required: Yes (unless allow_insecure = true) key_path# Type: String Default: \u0026quot;\u0026quot; Description: Path to TLS private key file (PEM format) Required: Yes (unless allow_insecure = true) ca_path# Type: String Default: \u0026quot;\u0026quot; Description: Path to CA certificate file for mTLS Required: Yes (unless allow_insecure = true) allow_insecure# Type: Boolean Default: false Description: Allow insecure connections without TLS Warning: ONLY use in development! Always enable TLS in production Storage Configuration# [storage_config] base_dir = \u0026#34;./data\u0026#34; namespaces = [\u0026#34;default\u0026#34;, \u0026#34;app\u0026#34;] bytes_per_sync = \u0026#34;1MB\u0026#34; segment_size = \u0026#34;16MB\u0026#34; arena_size = \u0026#34;4MB\u0026#34; wal_fsync_interval = \u0026#34;1s\u0026#34; disable_entry_type_check = falsebase_dir# Type: String Default: \u0026quot;./data\u0026quot; Description: Base directory for all data files (WAL segments, LMDB) Note: Must have write permissions namespaces# Type: Array of Strings Default: [\u0026quot;default\u0026quot;] Description: List of namespaces to create on startup Example: [\u0026quot;default\u0026quot;, \u0026quot;users\u0026quot;, \u0026quot;metrics\u0026quot;, \u0026quot;logs\u0026quot;] Note: Each namespace is isolated with separate WAL and storage bytes_per_sync# Type: String (with unit) Default: \u0026quot;1MB\u0026quot; Valid Units: KB, MB, GB Description: Number of bytes to write before forcing fsync segment_size# Type: String (with unit) Default: \u0026quot;16MB\u0026quot; Valid Units: KB, MB, GB Range: 1MB to 1GB Description: Size of each WAL segment file Important: Must match across server and relayer! arena_size# Type: String (with unit) Default: \u0026quot;4MB\u0026quot; Valid Units: KB, MB, GB Range: 1MB to 64MB Description: Size of the write buffer (memtable) Performance: Larger = fewer flushes, more memory usage wal_fsync_interval# Type: String (duration) Default: \u0026quot;1s\u0026quot; Valid Units: ms, s, m Description: Interval for periodic WAL fsync Trade-off: Lower = better durability, higher = better performance WAL Cleanup Configuration# [storage_config.wal_cleanup_config] enabled = false interval = \u0026#34;5m\u0026#34; max_age = \u0026#34;1h\u0026#34; min_segments = 5 max_segments = 10enabled# Type: Boolean Default: false Description: Enable automatic WAL segment cleanup interval# Type: String (duration) Default: \u0026quot;5m\u0026quot; Description: How often to run cleanup max_age# Type: String (duration) Default: \u0026quot;1h\u0026quot; Description: Maximum age of segments before cleanup min_segments# Type: Integer Default: 5 Description: Minimum number of segments to keep max_segments# Type: Integer Default: 10 Description: Trigger cleanup when this many segments exist Write Notification Configuration# Write notifications coalesce updates from WAL writers to readers, reducing notification overhead.\n[write_notify_config] enabled = true max_delay = \u0026#34;20ms\u0026#34;enabled# Type: Boolean Default: true Description: Enable write notification coalescing max_delay# Type: String (duration) Default: \u0026quot;20ms\u0026quot; Valid Units: ms, s Description: Maximum delay before notifying readers Trade-off: Higher = better batching, higher latency for reads ZeroMQ Notifier Configuration# UnisonDB can publish change notifications via ZeroMQ PUB/SUB sockets. This allows local applications to subscribe to real-time change notifications for specific namespaces.\nUse Case: Applications running on the same machine can subscribe to a namespace\u0026rsquo;s ZeroMQ socket and receive notifications whenever data changes, enabling reactive architectures.\n## Each namespace can have its own ZeroMQ notifier [notifier_config.default] bind_port = 5555 high_water_mark = 1000 linger_time = 1000 [notifier_config.tenant_1] bind_port = 5556 high_water_mark = 2000 linger_time = 500bind_port# Type: Integer Required: Yes Description: Port to bind ZeroMQ PUB socket to Format: Applications subscribe to tcp://localhost:{bind_port} high_water_mark# Type: Integer Default: 1000 Description: Maximum number of queued messages before blocking Note: Higher values use more memory but reduce message loss linger_time# Type: Integer (milliseconds) Default: 1000 Description: How long to wait for pending messages on shutdown Range: 0 to 5000 ms Example Application Subscription:\nimport zmq context = zmq.Context() socket = context.socket(zmq.SUB) socket.connect(\u0026#34;tcp://localhost:5555\u0026#34;) # Connect to default namespace socket.setsockopt(zmq.SUBSCRIBE, b\u0026#34;\u0026#34;) # Subscribe to all messages while True: message = socket.recv() print(f\u0026#34;Received change notification: {message}\u0026#34;) Relayer Configuration# Relayer configuration allows a UnisonDB instance to stream WAL changes from one or more upstream servers. This is useful for:\nRead scaling: Run multiple read replicas Data locality: Keep data close to consumers in different regions Backup: Maintain hot standbys [relayer_config] [relayer_config.relayer1] namespaces = [\u0026#34;default\u0026#34;, \u0026#34;tenant_1\u0026#34;] cert_path = \u0026#34;../../certs/client.crt\u0026#34; key_path = \u0026#34;../../certs/client.key\u0026#34; ca_path = \u0026#34;../../certs/ca.crt\u0026#34; upstream_address = \u0026#34;primary-server:4001\u0026#34; segment_lag_threshold = 100 allow_insecure = false grpc_service_config = \u0026#34;\u0026#34;Map Key (e.g., relayer1)# Type: String Description: Unique identifier for this relayer connection Note: Multiple relayers can be configured with different keys namespaces# Type: Array of Strings Required: Yes Description: List of namespaces to replicate from this upstream Note: Namespaces must exist on both upstream and local instance cert_path# Type: String Description: Path to client TLS certificate for mTLS Required: Yes (unless allow_insecure = true) key_path# Type: String Description: Path to client private key for mTLS Required: Yes (unless allow_insecure = true) ca_path# Type: String Description: Path to CA certificate to verify upstream server Required: Yes (unless allow_insecure = true) upstream_address# Type: String Required: Yes Description: Address of upstream gRPC server Format: host:port (e.g., \u0026quot;localhost:4001\u0026quot;, \u0026quot;10.0.1.5:4001\u0026quot;) segment_lag_threshold# Type: Integer Default: 100 Description: Maximum segment lag before logging warnings Note: Helps monitor replication health allow_insecure# Type: Boolean Default: false Description: Allow insecure connection to upstream (no TLS) Warning: Only for development! grpc_service_config# Type: String (JSON) Default: \u0026quot;\u0026quot; (uses built-in defaults) Description: Custom gRPC service configuration JSON Advanced: See gRPC documentation for format Logging Configuration# [log_config] log_level = \u0026#34;info\u0026#34; disable_timestamp = false [log_config.min_level_percents] debug = 100.0 info = 50.0 warn = 100.0 error = 100.0log_level# Type: String Valid Values: \u0026quot;debug\u0026quot;, \u0026quot;info\u0026quot;, \u0026quot;warn\u0026quot;, \u0026quot;error\u0026quot; Default: \u0026quot;info\u0026quot; Description: Minimum log level to output disable_timestamp# Type: Boolean Default: false Description: Disable timestamps in log output Use Case: When running under systemd/journal (timestamps added automatically) min_level_percents# Type: Map of String to Float Description: Sampling percentages for gRPC logging per level Range: 0.0 to 100.0 Purpose: Reduce log volume in high-traffic scenarios Example: info = 1.0 means sample 1% of info logs Log Levels:\ndebug: 100.0 = log all debug messages info: 50.0 = log 50% of info messages (randomly sampled) warn: 100.0 = log all warnings error: 100.0 = log all errors PProf Configuration# [pprof_config] enabled = true port = 6060enabled# Type: Boolean Default: false Description: Enable pprof HTTP server for profiling port# Type: Integer Default: 6060 Description: Port for pprof HTTP server Access: http://localhost:6060/debug/pprof/ Available Profiles:\n/debug/pprof/heap - Memory allocation /debug/pprof/goroutine - Goroutine stack traces /debug/pprof/profile - CPU profile /debug/pprof/trace - Execution trace Fuzzer Configuration# Built-in fuzzer for testing and stress testing UnisonDB.\n[fuzz_config] ops_per_namespace = 400 workers_per_namespace = 50 local_relayer_count = 1000 startup_delay = \u0026#34;10s\u0026#34; enable_read_ops = falseops_per_namespace# Type: Integer Default: 400 Description: Number of operations to perform per namespace workers_per_namespace# Type: Integer Default: 50 Description: Number of concurrent workers per namespace local_relayer_count# Type: Integer Default: 1000 Description: Number of local relayer goroutines to simulate startup_delay# Type: String (duration) Default: \u0026quot;10s\u0026quot; Description: Delay before starting fuzzer Purpose: Allow infrastructure to fully initialize enable_read_ops# Type: Boolean Default: false Description: Include read operations in fuzzing Note: Generates mixed read/write workload when enabled "}]