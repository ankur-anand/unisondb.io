[{"id":0,"href":"/docs/","title":"UnisonDB Documentation","section":"UnisonDB – Log-Native Database For AI And Edge Computing","content":"UnisonDB Documentation# This guide covers everything you need to install, configure, and operate UnisonDB at scale.\nDocumentation Sections# Getting Started # Step-by-step installation and quick-start setup for UnisonDB, including replicator and relayer modes.\nArchitecture # Dive into UnisonDB’s core concepts — log-native storage, WAL streaming replication, and edge-first design.\nDeployment # Explore deployment topologies and best practices for production, including hub-and-spoke and multi-region setups.\nAPI Reference # Detailed HTTP documentation for integrating UnisonDB with your applications and services.\n"},{"id":1,"href":"/docs/getting-started/","title":"Getting Started","section":"UnisonDB Documentation","content":"Getting Started with UnisonDB# This guide will walk you through installing UnisonDB, configuring it, and running it in both Replicator and Relayer modes.\n┌────────────────┐ │ Replicator │ │ (Primary) │ │ Writes → WAL │ │ Streams gRPC │ └──────┬─────────┘ │ WAL Stream (gRPC) │ ┌─────────────┴──────────────┐ ↓ ↓ ┌───────────┐ ┌───────────┐ │ Relayer 1 │ │ Relayer 2 │ │ (Replica) │ │ (Replica) │ │ Local DB │ │ Local DB │ │ Watch API │ │ Watch API │ └───────────┘ └───────────┘Table of Contents# Prerequisites Installation Running in Server Mode Running in Relayer Mode Prerequisites# System Requirements# Operating System: Linux or macOS Go: Version 1.24 or higher Installation# Building from Source# UnisonDB requires CGO to be enabled for LMDB bindings.\n# Clone the repository git clone https://github.com/ankur-anand/unisondb.git cd unisondb # Enable CGO (required for LMDB) export CGO_ENABLED=1 # Build the binary go build -o unisondb ./cmd/unisondb # Verify installation ./unisondb --helpExpected output:\n_ _ _ ___ ___ | | | | _ _ (_) ___ ___ _ _ | \\ | _ ) | |_| | | \u0026#39; \\ | | (_-\u0026lt; / _ \\ | \u0026#39; \\ | |) | | _ \\ \\___/ |_||_| |_| /__/ \\___/ |_||_| |___/ |___/ Database + Message Bus. Built for Edge. https://unisondb.io NAME: unisondb - Run UnisonDB USAGE: unisondb [global options] command [command options] COMMANDS: replicator Run in replicator mode relayer Run in relayer mode fuzzer This is a testing-only feature (disabled in production builds) help, h Shows a list of commands or help for one command GLOBAL OPTIONS: --config value, -c value Path to TOML config file (default: \u0026#34;./config.toml\u0026#34;) [$UNISON_CONFIG] --env value, -e value Environment: dev, staging, prod (default: \u0026#34;dev\u0026#34;) [$UNISON_ENV] --grpc, -G Enable gRPC server in Relayer Mode (default: false) [$UNISON_GRPC_ENABLED] --help, -h show helpBuilding with Fuzzer Support (Optional)# For testing and development, you can build with fuzzer support:\nCGO_ENABLED=1 go build -tags fuzz -o unisondb ./cmd/unisondbNote: When built with -tags fuzz:\nfuzzer command is available replicator command is disabled for safety To run fuzzer mode:\n./unisondb --config config.toml fuzzerInstallation to System Path (Optional)# # Move to system path sudo mv unisondb /usr/local/bin/ # Verify unisondb --helpRunning in Replicator Mode# Replicator Mode runs UnisonDB as a primary instance that accepts writes and serves reads.\n1. Generate TLS Certificates (Recommended)# For production or multi-node setups, generate TLS certificates for gRPC:\nUsing OpenSSL:\nmkdir -p certs \u0026amp;\u0026amp; cd certs # Generate CA openssl genrsa -out ca.key 4096 openssl req -new -x509 -key ca.key -sha256 -subj \u0026#34;/CN=UnisonDB CA\u0026#34; -days 365 -out ca.crt # Generate server certificate openssl genrsa -out server.key 4096 openssl req -new -key server.key -out server.csr -subj \u0026#34;/CN=localhost\u0026#34; openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out server.crt -days 365 -sha256 # Generate client certificate openssl genrsa -out client.key 4096 openssl req -new -key client.key -out client.csr -subj \u0026#34;/CN=client\u0026#34; openssl x509 -req -in client.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out client.crt -days 365 -sha256 cd ..2. Create Server Configuration# Create a server.toml configuration file:\n## HTTP API port http_port = 4000 listen_ip = \u0026#34;0.0.0.0\u0026#34; ## gRPC configuration (for replication) [grpc_config] listen_ip = \u0026#34;0.0.0.0\u0026#34; port = 4001 cert_path = \u0026#34;./certs/server.crt\u0026#34; key_path = \u0026#34;./certs/server.key\u0026#34; ca_path = \u0026#34;./certs/ca.crt\u0026#34; # For development only - allows insecure connections allow_insecure = false ## Storage configuration [storage_config] base_dir = \u0026#34;./data/server\u0026#34; namespaces = [\u0026#34;default\u0026#34;, \u0026#34;users\u0026#34;, \u0026#34;products\u0026#34;] bytes_per_sync = \u0026#34;1MB\u0026#34; segment_size = \u0026#34;16MB\u0026#34; arena_size = \u0026#34;4MB\u0026#34; wal_fsync_interval = \u0026#34;1s\u0026#34; ## WAL cleanup (prevents disk exhaustion) [storage_config.wal_cleanup_config] enabled = true interval = \u0026#34;5m\u0026#34; max_age = \u0026#34;1h\u0026#34; min_segments = 5 max_segments = 100 ## Write notification coalescing [write_notify_config] enabled = true max_delay = \u0026#34;20ms\u0026#34; ## ZeroMQ notifications (optional - for local apps) [notifier_config.default] bind_port = 5555 high_water_mark = 1000 linger_time = 1000 [notifier_config.users] bind_port = 5556 high_water_mark = 1000 linger_time = 1000 ## Profiling endpoint [pprof_config] enabled = true port = 6060 ## Logging [log_config] log_level = \u0026#34;info\u0026#34; [log_config.min_level_percents] debug = 100.0 info = 100.0 warn = 100.0 error = 100.03. Start the Replicator Server# Replicator Mode (using replicator command):\n./unisondb --config server.toml replicator4. Verify Server is Running# Check HTTP health endpoint:\ncurl http://localhost:4000/healthDevelopment Mode (Insecure)# For quick local testing without TLS:\nserver-dev.toml:\nhttp_port = 4000 [grpc_config] port = 4001 allow_insecure = true # WARNING: Development only! [storage_config] base_dir = \u0026#34;./data/server\u0026#34; namespaces = [\u0026#34;default\u0026#34;] [log_config] log_level = \u0026#34;debug\u0026#34;./unisondb --config server-dev.toml replicatorRunning in Relayer Mode# Relayer Mode runs UnisonDB as a replica that streams changes from an upstream server.\n1. Create Relayer Configuration# Create a relayer.toml configuration file:\n## HTTP API port (different from server) http_port = 5000 listen_ip = \u0026#34;0.0.0.0\u0026#34; ## gRPC config (can accept downstream relayers) [grpc_config] listen_ip = \u0026#34;0.0.0.0\u0026#34; port = 5001 cert_path = \u0026#34;./certs/server.crt\u0026#34; key_path = \u0026#34;./certs/server.key\u0026#34; ca_path = \u0026#34;./certs/ca.crt\u0026#34; ## Storage configuration [storage_config] base_dir = \u0026#34;./data/relayer\u0026#34; namespaces = [\u0026#34;default\u0026#34;, \u0026#34;users\u0026#34;, \u0026#34;products\u0026#34;] bytes_per_sync = \u0026#34;1MB\u0026#34; # IMPORTANT: segment_size MUST match upstream server! segment_size = \u0026#34;16MB\u0026#34; arena_size = \u0026#34;4MB\u0026#34; ## Relayer configuration - connects to upstream [relayer_config.primary] namespaces = [\u0026#34;default\u0026#34;, \u0026#34;users\u0026#34;, \u0026#34;products\u0026#34;] cert_path = \u0026#34;./certs/client.crt\u0026#34; key_path = \u0026#34;./certs/client.key\u0026#34; ca_path = \u0026#34;./certs/ca.crt\u0026#34; upstream_address = \u0026#34;localhost:4001\u0026#34; segment_lag_threshold = 100 allow_insecure = false ## Optional: Connect to multiple upstreams # [relayer_config.secondary] # namespaces = [\u0026#34;products\u0026#34;] # upstream_address = \u0026#34;other-server:4001\u0026#34; # cert_path = \u0026#34;./certs/client.crt\u0026#34; # key_path = \u0026#34;./certs/client.key\u0026#34; # ca_path = \u0026#34;./certs/ca.crt\u0026#34; ## ZeroMQ notifications (optional) [notifier_config.default] bind_port = 6555 high_water_mark = 1000 linger_time = 1000 ## Logging [log_config] log_level = \u0026#34;info\u0026#34; [log_config.min_level_percents] debug = 1.0 # Sample 1% of debug logs info = 10.0 # Sample 10% of info logs warn = 100.0 error = 100.02. Start the Relayer Server# Start relayer:\n./unisondb --config relayer.toml relayer3. Enable gRPC Server on Relayer (Multi-Hop)# To allow downstream relayers to connect to this relayer:\n./unisondb --config relayer.toml --grpc relayerThis enables the relayer to act as both a consumer (from upstream) and a producer (to downstream).\nDevelopment Mode (Insecure)# relayer-dev.toml:\nhttp_port = 5000 [grpc_config] port = 5001 [storage_config] base_dir = \u0026#34;./data/relayer\u0026#34; namespaces = [\u0026#34;default\u0026#34;] segment_size = \u0026#34;16MB\u0026#34; # Must match server! [relayer_config.primary] namespaces = [\u0026#34;default\u0026#34;] upstream_address = \u0026#34;localhost:4001\u0026#34; allow_insecure = true # WARNING: Development only! segment_lag_threshold = 100 [log_config] log_level = \u0026#34;debug\u0026#34;./unisondb --config relayer-dev.toml relayerBasic Operations# Writing Data# Key-Value Write (via HTTP):\n# Put a key-value pair curl -X POST http://localhost:4000/api/v1/namespaces/default/kv \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;key\u0026#34;: \u0026#34;user:123\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;eyJuYW1lIjoiQWxpY2UiLCJlbWFpbCI6ImFsaWNlQGV4YW1wbGUuY29tIn0=\u0026#34; }\u0026#39;Batch Write:\ncurl -X POST http://localhost:4000/api/v1/namespaces/default/kv/batch \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;operations\u0026#34;: [ {\u0026#34;key\u0026#34;: \u0026#34;user:1\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;...\u0026#34;}, {\u0026#34;key\u0026#34;: \u0026#34;user:2\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;...\u0026#34;}, {\u0026#34;key\u0026#34;: \u0026#34;user:3\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;...\u0026#34;} ] }\u0026#39;Reading Data# Read from Server:\ncurl http://localhost:4000/api/v1/namespaces/default/kv/user:123Read from Relayer (same API):\ncurl http://localhost:5000/api/v1/namespaces/default/kv/user:123Subscribing to Changes (ZeroMQ)# Build UnisonDB with ZeroMQ Lib This needs Zero MQ Installed Make Sure You\u0026rsquo;ve have it Installed. Install ZeroMQ dependency # Clone the repository git clone https://github.com/ankur-anand/unisondb.git cd unisondb # Build the binary (CGO required for RocksDB) CGO_ENABLED=1 go build -tags zeromq ./cmd/unisondbPython example (install pyzmq first):\nimport zmq context = zmq.Context() socket = context.socket(zmq.SUB) # Subscribe to \u0026#39;default\u0026#39; namespace socket.connect(\u0026#34;tcp://localhost:5555\u0026#34;) socket.setsockopt(zmq.SUBSCRIBE, b\u0026#34;\u0026#34;) # Subscribe to all messages print(\u0026#34;Listening for changes on namespace \u0026#39;default\u0026#39;...\u0026#34;) while True: message = socket.recv() print(f\u0026#34;Change notification: {message}\u0026#34;)Run the subscriber:\npython subscriber.pyNow any writes to the default namespace will trigger notifications!\nCommon Deployment Patterns# 1. Single Server (Development)# # Terminal 1: Start server ./unisondb --config server-dev.toml replicator2. Server + Single Relayer (Read Scaling)# # Terminal 1: Start server ./unisondb --config server.toml replicator # Terminal 2: Start relayer ./unisondb --config relayer.toml relayer3. Server + Multiple Relayers (Edge Computing)# # Terminal 1: Start server ./unisondb --config server.toml replicator # Terminal 2: Start relayer 1 ./unisondb --config relayer1.toml relayer # Terminal 3: Start relayer 2 ./unisondb --config relayer2.toml relayer # Terminal 4: Start relayer 3 ./unisondb --config relayer3.toml relayer4. Multi-Hop (Relayer → Relayer)# # Terminal 1: Primary server ./unisondb --config server.toml replicator # Terminal 2: L1 relayer (with gRPC enabled for downstream) ./unisondb --config relayer-l1.toml --grpc relayer # Terminal 3: L2 relayer (connects to L1) # Update relayer-l2.toml upstream_address to point to L1 (localhost:5001) ./unisondb --config relayer-l2.toml relayerHappy building with UnisonDB!\n"},{"id":2,"href":"/docs/architecture/","title":"Architecture Overview","section":"UnisonDB Documentation","content":"Architecture Overview# UnisonDB is a log-native, real-time database that replicates like a message bus — merging transactional consistency with streaming replication. It’s purpose-built for Edge AI , Edge Computing , and local-first distributed systems that demand low-latency synchronization across nodes.\nKey Characteristics# Aspect Design Choice Storage Model Multi-modal: Key-Value, Wide-Column, Large Objects (LOB) Storage Engine B+Tree (LMDB/BoltDB) with in-memory MemTable overlay Replication Log streaming (gRPC) with eventual consistency Watch API Best-effort change notifications (per-namespace) Consistency Single-primary writes, eventual consistency for replicas Durability WAL-first with configurable fsync Deployment Modes Replicator (writable primary) \u0026amp; Relayer (read-only replica) Core Concepts# 1. Log-Native Design# The Write-Ahead Log (WAL) is a first-class citizen, not just a recovery mechanism.\nReplication = Log streaming: No separate replication protocol Recovery = Log replay: State reconstructed from WAL Time-travel enabled: Historic snapshots from log Single source of truth: All operations flow through WAL 2. Operational Modes# UnisonDB instances run in one of two modes:\nReplicator Mode (Primary)# Writable instance that maintains the authoritative WAL.\n┌─────────────────────────────────────┐ │ Replicator Mode Instance │ │ │ │ • Accepts writes (HTTP API) │ │ • Maintains authoritative WAL │ │ • Streams to relayers (gRPC) │ │ • Publishes watch events (local) │ │ • Serves reads from local storage │ └─────────────────────────────────────┘Relayer Mode (Replica)# Read-only instance that streams changes from upstream replicators.\n┌─────────────────────────────────────┐ │ Relayer Mode Instance │ │ │ │ • Connects to upstream (gRPC) │ │ • Receives WAL segment streams │ │ • Applies to local storage (RO) │ │ • Can relay to downstream nodes │ │ • Publishes watch events (local) │ │ • Serves reads (data locality) │ └─────────────────────────────────────┘3. Dual Communication Channels# UnisonDB separates distribution from local reactivity:\nChannel Purpose Use Case Scope gRPC Durable replication Node-to-node WAL streaming Network (cross-machine) Watch API Best-effort notifications Local application reactivity IPC (same machine) Design Rationale:\ngRPC Replication: Network-tolerant, authenticated, durable, at-least-once delivery Watch API: Lightweight, fire-and-forget, at-most-once, local-only System Architecture# ┌────────────────────────────────────────────────────────────────────┐ │ UnisonDB Instance │ ├────────────────────────────────────────────────────────────────────┤ │ Client APIs │ │ ┌──────────────┐ ┌──────────────┐ ┌──────────────┐ │ │ │ HTTP REST │ │ Transactions │ │ Admin/Stats │ │ │ └──────┬───────┘ └──────┬───────┘ └──────┬───────┘ │ │ └─────────────────┴─────────────────┘ │ ├────────────────────────────────────────────────────────────────────┤ │ Storage Models │ │ ┌─────────────┐ ┌──────────────┐ ┌──────────────┐ │ │ │ Key-Value │ │ Wide-Column │ │ Large Object │ │ │ └──────┬──────┘ └──────┬───────┘ └──────┬───────┘ │ │ └────────────────┴─────────────────┘ │ ├────────────────────────────────────────────────────────────────────┤ │ Storage Engine (dbkernel) │ │ │ │ Write: WAL (append) → MemTable → B+Tree (LMDB) │ │ Read: MemTable + B+Tree → Response │ ├────────────────────────────────────────────────────────────────────┤ │ Distribution Layer │ │ ┌──────────────────────┐ ┌──────────────────────┐ │ │ │ gRPC Replicator │ │ Watch API │ │ │ │ • WAL streaming │ │ • Change events │ │ │ │ • TLS/mTLS │ │ • Per-namespace │ │ │ │ • At-least-once │ │ • At-most-once │ │ │ └──────────┬───────────┘ └──────────┬───────────┘ │ │ v v │ │ Remote Relayers Local Applications │ └────────────────────────────────────────────────────────────────────┘Core Components# Write-Ahead Log (WAL)# Append-only transaction log that serves as the source of truth.\nProperty Implementation Structure Segmented files (configurable size, default 16MB) Format Binary with CRC32 checksums per entry Lifecycle Write → fsync → MemTable → B+Tree → Segment cleanup Purpose Durability, replication streaming, crash recovery Segment Format:\n┌─────────────────────────────────┐ │ Header (magic, segment#, ts) │ ├─────────────────────────────────┤ │ Entry: flatbuffer-encoded │ │ Entry: flatbuffer-encoded │ │ ... │ └─────────────────────────────────┘Storage Engine# Multi-modal storage built on B+Trees with MemTable overlay.\nComponent Technology Purpose MemTable In-memory skip list Write buffer, recent reads B+Tree LMDB/BoltDB Persistent sorted storage Encoding Model-specific key schemas Key Space isolation Storage Models:\nKey-Value: \u0026lt;key\u0026gt; -\u0026gt; \u0026lt;value\u0026gt; Wide-Column: \u0026lt;rowKey\u0026gt;:\u0026lt;colName\u0026gt; -\u0026gt; \u0026lt;colValue\u0026gt; Large Object: \u0026lt;objectKey\u0026gt;:chunk:\u0026lt;N\u0026gt; -\u0026gt; \u0026lt;chunk_data\u0026gt;Replication System# gRPC-based WAL streaming with bidirectional flow control.\nReplicator Role (Primary):\nStreams WAL segments to connected relayers Relayer Role (Replica):\nConsumes from one or more upstream servers Applies segments in order to local storage Can fan-out to downstream relayers (multi-hop) Guarantees:\nConsistency Model: Eventual (all replicas converge) Delivery: At-least-once with gap detection Ordering: Strict segment sequence enforcement Namespace Watch API# Lightweight, best-effort notification system for real-time change awareness.\nEvent Structure:\n{ \u0026#34;namespace\u0026#34;: \u0026#34;users\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;user:123\u0026#34;, \u0026#34;operation\u0026#34;: \u0026#34;put\u0026#34;, \u0026#34;entry_type\u0026#34;: \u0026#34;kv\u0026#34;, \u0026#34;seq_num\u0026#34;: 42 } Field Description namespace Namespace where change occurred key Exact key that changed operation Operation: put, delete, delete_row entry_type Operation: kv, lob, row seq Monotonic sequence number (per-namespace ordering of events) Characteristics:\nPer-namespace streams: Independent event streams per namespace Ordered delivery: Events delivered in sequence order (if received) Fire-and-forget: No acknowledgments, best-effort delivery Ring buffer: Configurable size-based event retention Backpressure handling: Events dropped if subscribers can\u0026rsquo;t keep up Delivery Guarantees:\nEvents may be dropped if consumer can\u0026rsquo;t keep up. Cannot replay from a specific sequence Subscribers not notified of missed events Use Cases:\nCache invalidation across microservices Trigger event-driven workflows Local application coordination NOT for critical workflows requiring guaranteed delivery (use gRPC replication instead) Example Flow:\nApplication A: PUT user:123 → UnisonDB appends to WAL → Watch stream publishes: {namespace:\u0026#34;users\u0026#34;, key:\u0026#34;user:123\u0026#34;, entry_type:\u0026#34;PUT\u0026#34;, seq:42} → Applications B, C, D receive notification and reactDesign Principles# Multi-Modal Storage Examples# All storage models share the same WAL and B+Tree, differing only in key encoding:\nModel Example Key Encoding Key-Value user:123 → {\u0026quot;name\u0026quot;:\u0026quot;Alice\u0026quot;} \u0026lt;key\u0026gt; Wide-Column user:123 with columns name, email \u0026lt;rowKey\u0026gt;:\u0026lt;colName\u0026gt; Large Object video:abc as streamable chunks \u0026lt;objectKey\u0026gt;:chunk:\u0026lt;N\u0026gt; Edge-First Topology# Designed for hub-and-spoke, multi-hop replication with data locality:\n┌──────────────┐ │ Primary │ (Repliactor Mode - accepts writes) │ (US-East) │ └──────┬───────┘ │ gRPC (durable replication) ┌────────┼────────┐ ↓ ↓ ↓ ┌───────┐┌───────┐┌───────┐ │Europe ││ Asia ││US-West│ (Relayer Mode - read replicas) │Relayer││Relayer││Relayer│ └───┬───┘└───┬───┘└───┬───┘ │ │ │ Watch API (local events) ↓ ↓ ↓ Local Local Local Apps Apps AppsData Flow# Write Path (Server Mode)# Write Request → API Handler → Storage Engine ↓ ┌───────────────┴───────────────┐ ↓ ↓ 1. WAL Append 2. MemTable Update (+ fsync) (in-memory) ↓ ↓ 3. Background Flush ────────────────→ B+Tree (LMDB) ↓ ┌───────┴────────┐ ↓ ↓ gRPC Stream Watch Event (to relayers) (to local apps)Read Path# Read Request → API Handler → Storage Engine ↓ ┌───────┴───────┐ ↓ ↓ MemTable B+Tree (check first) (if not found) └───────┬───────┘ ↓ Merge \u0026amp; ResponseReplication Flow (gRPC)# Replicator (Primary) Relayer (Replica) │ │ │ ─── WAL Segment (gRPC stream) ────→ | │ [metadata + binary + CRC] │ │ ↓ │ 1. Validate checksum │ 2. Append to local WAL │ 3. Apply to MemTable │ 4. Flush to B+Tree │ ↓ │ Can relay downstream │ Can notify local appsWatch Event Flow# WAL Write → Watch Event Builder → Ring Buffer → Transport Layer │ ┌──────────────┼──────────────┐ ↓ ↓ ↓ App A App B App C (subscriber) (subscriber) (subscriber)Event Details:\nTriggered on every WAL append (PUT/DELETE/UPDATE) Buffered in ring buffer (configurable size) Events dropped if buffer full or subscriber slow Storage Layout# data/ ├── \u0026lt;namespace\u0026gt;/ │ ├── wal/ │ │ ├── segment-00000000 # 16MB segments (configurable) │ │ ├── segment-00000001 │ │ └── ... │ ├── db/ │ │ ├── data.mdb # LMDB data file │ │ └── lock.mdb # LMDB lock file │ └── checkpoint/ │ └── last_applied # Recovery pointPer-Namespace Isolation: Each namespace has independent WAL, DB, and checkpoint state.\nSystem Characteristics# Consistency Model# Aspect Guarantee Writes Single primary (Replicator Mode) for linearizability Reads Eventually consistent across relayers Replication At-least-once delivery, ordered segments Isolation Per-namespace (independent namespaces) Durability \u0026amp; Recovery# Crash Recovery:\nScan WAL for uncommitted operations Replay WAL to rebuild MemTable and B+Tree Validate checkpoint consistency Resume operations Replication Recovery (Relayer Mode):\nDetermine last applied segment from checkpoint Reconnect to upstream at last offset Request missing segments (gap detection) Apply backlog before serving reads Data Durability:\nWAL with optional fsync (configurable) CRC32 checksums on all WAL entries Segment-level validation during replication Deployment Topologies# UnisonDB supports various deployment patterns. See the Deployment Guide for detailed configurations and examples.\nExample: Hub-and-Spoke# ┌──────────┐ │ Hub │ (Replicator Mode) └────┬─────┘ │ gRPC (durable replication) ┌──────────┼──────────┐ ↓ ↓ ↓ ┌──────┐ ┌──────┐ ┌──────┐ │Edge 1│ │Edge 2│ │Edge 3│ (Relayers) └──┬───┘ └──┬───┘ └──┬───┘ │ │ │ Watch API (local events) ↓ ↓ ↓ Local Local Local Apps Apps AppsKey characteristics:\nCentral hub accepts all writes Edge relayers provide data locality Local applications subscribe to watch events For detailed configurations, monitoring, and operational guidance, see the Deployment Topologies Guide .\nTradeoffs \u0026amp; Limitations# Design Tradeoffs# Aspect Tradeoff Rationale Write Scalability Single primary per namespace Ensures linearizable writes, simplifies conflict resolution Read Consistency Eventual consistency on relayers Enables high read scalability and data locality Replication Model At-least-once delivery Prioritizes availability over exactly-once semantics Watch Events At-most-once, best-effort Minimizes latency, lightweight notification for non-critical use cases Current Limitations# Write Scaling: Single primary per namespace (no multi-master) Consistency: No strong consistency guarantees for relayer reads Transactions: Limited to single-namespace operations Query Model: No complex queries (no SQL, joins, aggregations) Schema: Schema-less (application-managed structure) When to Use UnisonDB# Good Fit:\nEdge computing with hub-and-spoke topology Read-heavy workloads requiring data locality Event-driven architectures (via Watch API) Applications tolerating eventual consistency Key-value or wide-column access patterns Large object storage with streaming Not a Good Fit:\nStrong consistency requirements across replicas Complex relational queries (joins, aggregations) Multi-region active-active writes Workloads requiring ACID transactions across namespaces Summary# UnisonDB combines database semantics with streaming mechanics through:\nLog-Native Design: WAL as first-class citizen (replication = log streaming) Dual Communication: gRPC for distribution, Watch API for local reactivity Dual Modes: Server (writable primary) and Relayer (read replicas) Multi-Modal Storage: Key-Value, Wide-Column, Large Objects on shared B+Tree Architecture Strengths:\nData locality through edge replicas Event-driven integration via Watch API Simple operational model (log streaming) Flexible deployment topologies (hub-and-spoke, multi-hop) Best For: Edge computing, local-first applications, and read-scalable systems with eventual consistency tolerance.\n"},{"id":3,"href":"/docs/api/","title":"API Reference","section":"UnisonDB Documentation","content":"API Reference# UnisonDB provides API interfaces for client access:\nHTTP REST API # RESTful HTTP API with JSON payloads, supporting:\nKey-Value operations Wide-Column operations Large Object (LOB) operations Stateful transactions Metadata queries Next Steps# HTTP API Reference - Complete HTTP API documentation "},{"id":4,"href":"/docs/getting-started/configurations/","title":"Configuration","section":"Getting Started","content":"Configuration Guide# UnisonDB uses TOML for configuration. This guide covers all available configuration options for both replicator and relayer modes.\nTable of Contents# Replicator Mode Relayer Mode Configuration Reference Replicator Mode# Replicator mode runs UnisonDB as a primary instance that accepts writes and serves reads. Here\u0026rsquo;s a complete example:\n## Port of the http server http_port = 4000 listen_ip = \u0026#34;0.0.0.0\u0026#34; ## grpc config for replication [grpc_config] listen_ip = \u0026#34;0.0.0.0\u0026#34; port = 4001 # SSL/TLS certificate paths for gRPC server cert_path = \u0026#34;../../certs/server.crt\u0026#34; key_path = \u0026#34;../../certs/server.key\u0026#34; ca_path = \u0026#34;../../certs/ca.crt\u0026#34; # Allow insecure connections (no TLS) - ONLY for development! allow_insecure = false # StorageConfig stores all tunable parameters. [storage_config] base_dir = \u0026#34;/tmp/unisondb/server\u0026#34; # Base directory for storage namespaces = [\u0026#34;default\u0026#34;, \u0026#34;tenant_1\u0026#34;, \u0026#34;tenant_2\u0026#34;, \u0026#34;tenant_3\u0026#34;, \u0026#34;tenant_4\u0026#34;] bytes_per_sync = \u0026#34;1MB\u0026#34; segment_size = \u0026#34;16MB\u0026#34; arena_size = \u0026#34;4MB\u0026#34; wal_fsync_interval = \u0026#34;1s\u0026#34; ## WAL cleanup configuration [storage_config.wal_cleanup_config] enabled = false interval = \u0026#34;5m\u0026#34; max_age = \u0026#34;1h\u0026#34; min_segments = 5 max_segments = 10 ## Write notify config - coalesces notifications from WAL writers to readers [write_notify_config] enabled = true max_delay = \u0026#34;20ms\u0026#34; ## ZeroMQ notifier configuration (per-namespace) ## Publishes change notifications for local application consumption [notifier_config.default] bind_port = 5555 high_water_mark = 1000 linger_time = 1000 [notifier_config.tenant_1] bind_port = 5556 high_water_mark = 1000 linger_time = 1000 [pprof_config] enabled = true port = 6060 [log_config] log_level = \u0026#34;info\u0026#34; disable_timestamp = false ## This is for grpc logging only - controls sampling percentages per level [log_config.min_level_percents] debug = 100.0 info = 50.0 warn = 100.0 error = 100.0 ## Fuzzer configuration (for testing) [fuzz_config] ops_per_namespace = 400 workers_per_namespace = 50 local_relayer_count = 1000 startup_delay = \u0026#34;10s\u0026#34; enable_read_ops = falseRelayer Mode# Relayer mode runs UnisonDB as a replica that streams changes from one or more upstream servers. This provides read scalability and data locality.\n## Port of the http server http_port = 6000 [grpc_config] port = 6001 cert_path = \u0026#34;../../certs/server.crt\u0026#34; key_path = \u0026#34;../../certs/server.key\u0026#34; ca_path = \u0026#34;../../certs/ca.crt\u0026#34; [storage_config] base_dir = \u0026#34;/tmp/unisondb/relayer\u0026#34; namespaces = [\u0026#34;default\u0026#34;, \u0026#34;tenant_1\u0026#34;, \u0026#34;tenant_2\u0026#34;] bytes_per_sync = \u0026#34;1MB\u0026#34; ## IMPORTANT: segment_size must match upstream server! segment_size = \u0026#34;16MB\u0026#34; arena_size = \u0026#34;4MB\u0026#34; ## Relayer configuration - can have multiple upstreams [relayer_config] [relayer_config.relayer1] namespaces = [\u0026#34;default\u0026#34;, \u0026#34;tenant_1\u0026#34;, \u0026#34;tenant_2\u0026#34;] cert_path = \u0026#34;../../certs/client.crt\u0026#34; key_path = \u0026#34;../../certs/client.key\u0026#34; ca_path = \u0026#34;../../certs/ca.crt\u0026#34; upstream_address = \u0026#34;localhost:4001\u0026#34; segment_lag_threshold = 100 allow_insecure = false # Optional: custom gRPC service config JSON grpc_service_config = \u0026#34;\u0026#34; ## Optional: Add more relayers for different upstream sources [relayer_config.relayer2] namespaces = [\u0026#34;tenant_3\u0026#34;] cert_path = \u0026#34;../../certs/client2.crt\u0026#34; key_path = \u0026#34;../../certs/client2.key\u0026#34; ca_path = \u0026#34;../../certs/ca.crt\u0026#34; upstream_address = \u0026#34;remote-server:4001\u0026#34; segment_lag_threshold = 100 [log_config] log_level = \u0026#34;info\u0026#34; [log_config.min_level_percents] debug = 0.01 info = 1.0 warn = 1.0 error = 1.0 Configuration Reference# Server Configuration# HTTP Server# http_port = 4000 listen_ip = \u0026#34;0.0.0.0\u0026#34;http_port# Type: Integer Default: 4000 Description: Port for the HTTP API server listen_ip# Type: String Default: \u0026quot;0.0.0.0\u0026quot; Description: IP address to bind HTTP server to Note: Use \u0026quot;127.0.0.1\u0026quot; for localhost-only access gRPC Configuration# [grpc_config] listen_ip = \u0026#34;0.0.0.0\u0026#34; port = 4001 cert_path = \u0026#34;/path/to/server.crt\u0026#34; key_path = \u0026#34;/path/to/server.key\u0026#34; ca_path = \u0026#34;/path/to/ca.crt\u0026#34; allow_insecure = falselisten_ip# Type: String Default: \u0026quot;0.0.0.0\u0026quot; Description: IP address to bind gRPC server to port# Type: Integer Default: 4001 Description: Port for the gRPC server (used for replication) cert_path# Type: String Default: \u0026quot;\u0026quot; Description: Path to TLS certificate file (PEM format) Required: Yes (unless allow_insecure = true) key_path# Type: String Default: \u0026quot;\u0026quot; Description: Path to TLS private key file (PEM format) Required: Yes (unless allow_insecure = true) ca_path# Type: String Default: \u0026quot;\u0026quot; Description: Path to CA certificate file for mTLS Required: Yes (unless allow_insecure = true) allow_insecure# Type: Boolean Default: false Description: Allow insecure connections without TLS Warning: ONLY use in development! Always enable TLS in production Storage Configuration# [storage_config] base_dir = \u0026#34;./data\u0026#34; namespaces = [\u0026#34;default\u0026#34;, \u0026#34;app\u0026#34;] bytes_per_sync = \u0026#34;1MB\u0026#34; segment_size = \u0026#34;16MB\u0026#34; arena_size = \u0026#34;4MB\u0026#34; wal_fsync_interval = \u0026#34;1s\u0026#34; disable_entry_type_check = falsebase_dir# Type: String Default: \u0026quot;./data\u0026quot; Description: Base directory for all data files (WAL segments, LMDB) Note: Must have write permissions namespaces# Type: Array of Strings Default: [\u0026quot;default\u0026quot;] Description: List of namespaces to create on startup Example: [\u0026quot;default\u0026quot;, \u0026quot;users\u0026quot;, \u0026quot;metrics\u0026quot;, \u0026quot;logs\u0026quot;] Note: Each namespace is isolated with separate WAL and storage bytes_per_sync# Type: String (with unit) Default: \u0026quot;1MB\u0026quot; Valid Units: KB, MB, GB Description: Number of bytes to write before forcing fsync segment_size# Type: String (with unit) Default: \u0026quot;16MB\u0026quot; Valid Units: KB, MB, GB Range: 1MB to 1GB Description: Size of each WAL segment file Important: Must match across server and relayer! arena_size# Type: String (with unit) Default: \u0026quot;4MB\u0026quot; Valid Units: KB, MB, GB Range: 1MB to 64MB Description: Size of the write buffer (memtable) Performance: Larger = fewer flushes, more memory usage wal_fsync_interval# Type: String (duration) Default: \u0026quot;1s\u0026quot; Valid Units: ms, s, m Description: Interval for periodic WAL fsync Trade-off: Lower = better durability, higher = better performance WAL Cleanup Configuration# [storage_config.wal_cleanup_config] enabled = false interval = \u0026#34;5m\u0026#34; max_age = \u0026#34;1h\u0026#34; min_segments = 5 max_segments = 10enabled# Type: Boolean Default: false Description: Enable automatic WAL segment cleanup interval# Type: String (duration) Default: \u0026quot;5m\u0026quot; Description: How often to run cleanup max_age# Type: String (duration) Default: \u0026quot;1h\u0026quot; Description: Maximum age of segments before cleanup min_segments# Type: Integer Default: 5 Description: Minimum number of segments to keep max_segments# Type: Integer Default: 10 Description: Trigger cleanup when this many segments exist Write Notification Configuration# Write notifications coalesce updates from WAL writers to readers, reducing notification overhead.\n[write_notify_config] enabled = true max_delay = \u0026#34;20ms\u0026#34;enabled# Type: Boolean Default: true Description: Enable write notification coalescing max_delay# Type: String (duration) Default: \u0026quot;20ms\u0026quot; Valid Units: ms, s Description: Maximum delay before notifying readers Trade-off: Higher = better batching, higher latency for reads ZeroMQ Notifier Configuration# UnisonDB can publish change notifications via ZeroMQ PUB/SUB sockets. This allows local applications to subscribe to real-time change notifications for specific namespaces.\nUse Case: Applications running on the same machine can subscribe to a namespace\u0026rsquo;s ZeroMQ socket and receive notifications whenever data changes, enabling reactive architectures.\n## Each namespace can have its own ZeroMQ notifier [notifier_config.default] bind_port = 5555 high_water_mark = 1000 linger_time = 1000 [notifier_config.tenant_1] bind_port = 5556 high_water_mark = 2000 linger_time = 500bind_port# Type: Integer Required: Yes Description: Port to bind ZeroMQ PUB socket to Format: Applications subscribe to tcp://localhost:{bind_port} high_water_mark# Type: Integer Default: 1000 Description: Maximum number of queued messages before blocking Note: Higher values use more memory but reduce message loss linger_time# Type: Integer (milliseconds) Default: 1000 Description: How long to wait for pending messages on shutdown Range: 0 to 5000 ms Example Application Subscription:\nimport zmq context = zmq.Context() socket = context.socket(zmq.SUB) socket.connect(\u0026#34;tcp://localhost:5555\u0026#34;) # Connect to default namespace socket.setsockopt(zmq.SUBSCRIBE, b\u0026#34;\u0026#34;) # Subscribe to all messages while True: message = socket.recv() print(f\u0026#34;Received change notification: {message}\u0026#34;) Relayer Configuration# Relayer configuration allows a UnisonDB instance to stream WAL changes from one or more upstream servers. This is useful for:\nRead scaling: Run multiple read replicas Data locality: Keep data close to consumers in different regions Backup: Maintain hot standbys [relayer_config] [relayer_config.relayer1] namespaces = [\u0026#34;default\u0026#34;, \u0026#34;tenant_1\u0026#34;] cert_path = \u0026#34;../../certs/client.crt\u0026#34; key_path = \u0026#34;../../certs/client.key\u0026#34; ca_path = \u0026#34;../../certs/ca.crt\u0026#34; upstream_address = \u0026#34;primary-server:4001\u0026#34; segment_lag_threshold = 100 allow_insecure = false grpc_service_config = \u0026#34;\u0026#34;Map Key (e.g., relayer1)# Type: String Description: Unique identifier for this relayer connection Note: Multiple relayers can be configured with different keys namespaces# Type: Array of Strings Required: Yes Description: List of namespaces to replicate from this upstream Note: Namespaces must exist on both upstream and local instance cert_path# Type: String Description: Path to client TLS certificate for mTLS Required: Yes (unless allow_insecure = true) key_path# Type: String Description: Path to client private key for mTLS Required: Yes (unless allow_insecure = true) ca_path# Type: String Description: Path to CA certificate to verify upstream server Required: Yes (unless allow_insecure = true) upstream_address# Type: String Required: Yes Description: Address of upstream gRPC server Format: host:port (e.g., \u0026quot;localhost:4001\u0026quot;, \u0026quot;10.0.1.5:4001\u0026quot;) segment_lag_threshold# Type: Integer Default: 100 Description: Maximum segment lag before logging warnings Note: Helps monitor replication health allow_insecure# Type: Boolean Default: false Description: Allow insecure connection to upstream (no TLS) Warning: Only for development! grpc_service_config# Type: String (JSON) Default: \u0026quot;\u0026quot; (uses built-in defaults) Description: Custom gRPC service configuration JSON Advanced: See gRPC documentation for format Logging Configuration# [log_config] log_level = \u0026#34;info\u0026#34; disable_timestamp = false [log_config.min_level_percents] debug = 100.0 info = 50.0 warn = 100.0 error = 100.0log_level# Type: String Valid Values: \u0026quot;debug\u0026quot;, \u0026quot;info\u0026quot;, \u0026quot;warn\u0026quot;, \u0026quot;error\u0026quot; Default: \u0026quot;info\u0026quot; Description: Minimum log level to output disable_timestamp# Type: Boolean Default: false Description: Disable timestamps in log output Use Case: When running under systemd/journal (timestamps added automatically) min_level_percents# Type: Map of String to Float Description: Sampling percentages for gRPC logging per level Range: 0.0 to 100.0 Purpose: Reduce log volume in high-traffic scenarios Example: info = 1.0 means sample 1% of info logs Log Levels:\ndebug: 100.0 = log all debug messages info: 50.0 = log 50% of info messages (randomly sampled) warn: 100.0 = log all warnings error: 100.0 = log all errors PProf Configuration# [pprof_config] enabled = true port = 6060enabled# Type: Boolean Default: false Description: Enable pprof HTTP server for profiling port# Type: Integer Default: 6060 Description: Port for pprof HTTP server Access: http://localhost:6060/debug/pprof/ Available Profiles:\n/debug/pprof/heap - Memory allocation /debug/pprof/goroutine - Goroutine stack traces /debug/pprof/profile - CPU profile /debug/pprof/trace - Execution trace Fuzzer Configuration# Built-in fuzzer for testing and stress testing UnisonDB.\n[fuzz_config] ops_per_namespace = 400 workers_per_namespace = 50 local_relayer_count = 1000 startup_delay = \u0026#34;10s\u0026#34; enable_read_ops = falseops_per_namespace# Type: Integer Default: 400 Description: Number of operations to perform per namespace workers_per_namespace# Type: Integer Default: 50 Description: Number of concurrent workers per namespace local_relayer_count# Type: Integer Default: 1000 Description: Number of local relayer goroutines to simulate startup_delay# Type: String (duration) Default: \u0026quot;10s\u0026quot; Description: Delay before starting fuzzer Purpose: Allow infrastructure to fully initialize enable_read_ops# Type: Boolean Default: false Description: Include read operations in fuzzing Note: Generates mixed read/write workload when enabled "},{"id":5,"href":"/docs/deployment/","title":"Deployment Topologies","section":"UnisonDB Documentation","content":"Deployment Topologies# Scalable, resilient, and edge-ready — UnisonDB adapts to your deployment architecture.\nDesign your own data mesh with UnisonDB’s log-native architecture. Replicate, stream, and sync data effortlessly across edge and cloud environments.\nOverview# UnisonDB uses a dual-mode architecture — Server (Replicator) for writes and Relayer for reads — allowing flexible, distributed deployments.\nCore Components# Replicator Mode – Primary node responsible for writes and WAL-based streaming Relayer Mode – Read-only node replicating from one or more upstreams Watch API – Real-time notifications for local apps or edge devices gRPC Replication – Durable WAL streaming protocol for replication and recovery 1. Single Server# Simplest deployment for development, testing, or small standalone applications.\n┌──────────────────────────┐ │ UnisonDB Server │ │ (Replicator Mode) │ │ │ │ • HTTP API :8080 │ │ • gRPC API :9090 │ │ • Watch API :5555 │ │ │ │ Capabilities: │ │ * Reads \u0026amp; Writes │ │ * Local watch events │ │ * No replication │ │ * No high availability │ └──────────────────────────┘Configuration# Server config (server.toml):\n[server] mode = \u0026#34;server\u0026#34; data_dir = \u0026#34;./data\u0026#34; http_addr = \u0026#34;0.0.0.0:8080\u0026#34; grpc_addr = \u0026#34;0.0.0.0:9090\u0026#34; [wal] segment_size = \u0026#34;16MB\u0026#34; fsync_enabled = true [watch] enabled = true transport = \u0026#34;zeromq\u0026#34; bind_addr = \u0026#34;tcp://*:5555\u0026#34; buffer_size = 10000Monitoring# # Check server health curl http://localhost:8080/health # View server stats curl http://localhost:8080/stats # Monitor watch subscribers curl http://localhost:8080/stats/watch 2. Replicator + Read Replicas# Replicator handles writes, replicas provide read scalability and geographic distribution.\n┌─────────────────────┐ │ Replicator Server │ (Replicator Mode) │ US-East │ │ • Writes │ │ • Reads │ └──────────┬──────────┘ │ gRPC replication (TLS) ┌────────┼────────┬────────┐ ↓ ↓ ↓ ↓ ┌────────┐┌────────┐┌────────┐┌────────┐ │Relayer ││Relayer ││Relayer ││Relayer │ │US-West ││Europe ││Asia ││Canada │ │ ││ ││ ││ │ │Read- ││Read- ││Read- ││Read- │ │only ││only ││only ││only │ └────────┘└────────┘└────────┘└────────┘Configuration# Replicator server (primary.toml):\n[server] mode = \u0026#34;server\u0026#34; data_dir = \u0026#34;/data/unisondb\u0026#34; http_addr = \u0026#34;0.0.0.0:8080\u0026#34; grpc_addr = \u0026#34;0.0.0.0:9090\u0026#34; [replication] enabled = true tls_cert = \u0026#34;/etc/unisondb/tls/server.crt\u0026#34; tls_key = \u0026#34;/etc/unisondb/tls/server.key\u0026#34; tls_ca = \u0026#34;/etc/unisondb/tls/ca.crt\u0026#34; [watch] enabled = true bind_addr = \u0026#34;tcp://*:5555\u0026#34; buffer_size = 10000Relayer (relayer-us-west.toml):\n[server] mode = \u0026#34;relayer\u0026#34; data_dir = \u0026#34;/data/unisondb\u0026#34; http_addr = \u0026#34;0.0.0.0:8080\u0026#34; [relayer] upstreams = [ \u0026#34;primary.us-east.example.com:9090\u0026#34; ] tls_cert = \u0026#34;/etc/unisondb/tls/client.crt\u0026#34; tls_key = \u0026#34;/etc/unisondb/tls/client.key\u0026#34; tls_ca = \u0026#34;/etc/unisondb/tls/ca.crt\u0026#34; # Relayer can also publish local watch events [watch] enabled = true bind_addr = \u0026#34;tcp://*:5555\u0026#34; buffer_size = 10000 3. Hub-and-Spoke (Edge Computing)# Central hub replicates to many edge nodes, each serving local applications.\n┌──────────────────┐ │ Central Hub │ (Replicator Mode) │ (Cloud/DC) │ │ • All writes │ └────────┬─────────┘ │ gRPC replication ┌────────────────┼────────────────┐ ↓ ↓ ↓ ┌─────────┐ ┌─────────┐ ┌─────────┐ │ Edge 1 │ │ Edge 2 │ │ Edge 3 │ (Relayers) │ Store A │ │ Store B │ │ Store C │ └────┬────┘ └────┬────┘ └────┬────┘ │ Watch API │ Watch API │ Watch API ↓ ↓ ↓ ┌─────────┐ ┌─────────┐ ┌─────────┐ │ POS │ │ POS │ │ POS │ Local apps │ Inv Mgmt│ │ Inv Mgmt│ │ Inv Mgmt│ (subscribers) │ Display │ │ Display │ │ Display │ └─────────┘ └─────────┘ └─────────┘Configuration# Hub (central Replicator server):\n[server] mode = \u0026#34;server\u0026#34; data_dir = \u0026#34;/data/unisondb\u0026#34; http_addr = \u0026#34;0.0.0.0:8080\u0026#34; grpc_addr = \u0026#34;0.0.0.0:9090\u0026#34; [namespaces] # Different namespaces for different data types inventory = { wal_segment_size = \u0026#34;16MB\u0026#34; } orders = { wal_segment_size = \u0026#34;8MB\u0026#34; } analytics = { wal_segment_size = \u0026#34;32MB\u0026#34; } [replication] enabled = true tls_enabled = true max_connections = 1000 # Support many edge nodesEdge relayer (edge-store-001.toml):\n[server] mode = \u0026#34;relayer\u0026#34; data_dir = \u0026#34;/data/unisondb\u0026#34; http_addr = \u0026#34;127.0.0.1:8080\u0026#34; # Local only [relayer] upstreams = [\u0026#34;hub.central.example.com:9090\u0026#34;] tls_enabled = true reconnect_interval = \u0026#34;5s\u0026#34; buffer_size = \u0026#34;100MB\u0026#34; # Handle disconnections # Edge nodes publish local watch events [watch] enabled = true namespaces = [\u0026#34;inventory\u0026#34;, \u0026#34;orders\u0026#34;] # Only needed namespaces bind_addr = \u0026#34;tcp://127.0.0.1:5555\u0026#34; # Local IPC only buffer_size = 5000 4. Multi-Hop Relay (Deep Edge)# Hierarchical replication for deep edge deployments or bandwidth-constrained networks.\n┌──────────────┐ │ Primary │ (Replicator Mode - Cloud) │ (Cloud) │ └──────┬───────┘ │ gRPC ↓ ┌──────────────┐ │ Tier 1 │ (Relayer - Regional DC) │ Regional │ └──────┬───────┘ │ gRPC ┌───────┴────────┐ ↓ ↓ ┌─────────┐ ┌─────────┐ │ Tier 2 │ │ Tier 2 │ (Relayer - Edge Cluster) │ West │ │ East │ └────┬────┘ └────┬────┘ │ │ ┌───┴───┐ ┌───┴───┐ ↓ ↓ ↓ ↓ Tier 3 Tier 3 Tier 3 Tier 3 (Relayer - Leaf Nodes) Store1 Store2 Store3 Store4 ↓ ↓ ↓ ↓ Local Local Local Local Apps Apps Apps AppsConfiguration# Tier 1 (Regional relayer):\n[server] mode = \u0026#34;relayer\u0026#34; data_dir = \u0026#34;/data/unisondb\u0026#34; grpc_addr = \u0026#34;0.0.0.0:9090\u0026#34; # Accept downstream connections [relayer] upstreams = [\u0026#34;primary.cloud.example.com:9090\u0026#34;] # This relayer can also relay to downstream enable_relay = true tls_enabled = trueTier 2 (Edge cluster relayer):\n[server] mode = \u0026#34;relayer\u0026#34; data_dir = \u0026#34;/data/unisondb\u0026#34; grpc_addr = \u0026#34;0.0.0.0:9090\u0026#34; [relayer] upstreams = [\u0026#34;tier1-regional.example.com:9090\u0026#34;] enable_relay = true # Relay to Tier 3 tls_enabled = trueTier 3 (Leaf relayer):\n[server] mode = \u0026#34;relayer\u0026#34; data_dir = \u0026#34;/data/unisondb\u0026#34; http_addr = \u0026#34;127.0.0.1:8080\u0026#34; [relayer] upstreams = [\u0026#34;tier2-west.example.com:9090\u0026#34;] enable_relay = false # Leaf node, no downstream tls_enabled = true [watch] enabled = true bind_addr = \u0026#34;tcp://127.0.0.1:5555\u0026#34; 5. Hybrid: Replication + Local Events# Combines durable replication with local event-driven applications.\n┌───────────────────────────────────┐ │ Replicator Server │ │ (Replicator Mode) │ │ │ │ ┌───────────────────┐ │ │ │ Storage Engine │ │ │ └─────────┬─────────┘ │ │ │ │ │ ┌───────┴────────┐ │ │ ↓ ↓ │ │ [gRPC] [Watch API] │ │ :9090 :5555 │ └────┬──────────────────┬───────────┘ │ │ │ └──────┐ ↓ ↓ ┌─────────┐ ┌─────────────┐ │ Remote │ │ Local Apps │ │Relayers │ │ │ └─────────┘ │ • Cache │ │ • Analytics │ │ • Audit Log │ │ • Dashboard │ └─────────────┘Configuration# Primary with both channels:\n[server] mode = \u0026#34;server\u0026#34; data_dir = \u0026#34;/data/unisondb\u0026#34; http_addr = \u0026#34;0.0.0.0:8080\u0026#34; grpc_addr = \u0026#34;0.0.0.0:9090\u0026#34; # gRPC replication for remote relayers [replication] enabled = true tls_enabled = true # Watch API for local applications [watch] enabled = true transport = \u0026#34;zeromq\u0026#34; namespaces = [\u0026#34;users\u0026#34;, \u0026#34;sessions\u0026#34;, \u0026#34;metrics\u0026#34;] # Per-namespace watch configuration [watch.users] bind_addr = \u0026#34;tcp://*:5555\u0026#34; buffer_size = 10000 [watch.sessions] bind_addr = \u0026#34;tcp://*:5556\u0026#34; buffer_size = 20000 # High-frequency updates [watch.metrics] bind_addr = \u0026#34;tcp://*:5557\u0026#34; buffer_size = 50000 Watch API security:\nBind to 127.0.0.1 for local-only access Next Steps# Configuration Reference - Detailed config options HTTP API - API documentation Architecture Overview - Architecture Overview "},{"id":6,"href":"/docs/operations/","title":"Operations","section":"UnisonDB Documentation","content":"Operations# Guides for operating UnisonDB.\nGuides# Backup and Restore # Complete guide to backing up and restoring UnisonDB using WAL segments and B-Tree snapshots.\n"},{"id":7,"href":"/docs/examples/","title":"Examples","section":"UnisonDB Documentation","content":"UnisonDB Examples# Get hands-on with UnisonDB through practical examples that demonstrate how to deploy, replicate, and synchronize data across distributed systems.\nEach tutorial highlights a real-world use case — from single-node configurations to multi-region CRDT replication and real-time streaming at the edge.\n1. How to Build Conflict-Free Multi-Datacenter Systems with CRDTs and UnisonDB # Learn how to design globally replicated systems that stay consistent without coordination.\nThis example walks you through using Conflict-Free Replicated Data Types (CRDTs) with UnisonDB’s WAL-based replication to achieve eventual consistency across data centers.\n"},{"id":8,"href":"/docs/operations/backup-restore/","title":"Backup and Restore","section":"Operations","content":"Backup and Restore# UnisonDB provides HTTP APIs for creating durable backups of both Write-Ahead Log (WAL) segments and B-Tree snapshots.\nOverview# UnisonDB\u0026rsquo;s backup system is designed around two complementary artifacts:\nComponent Purpose Recovery Capability Storage Size WAL Segments Incremental transaction logs Point-in-time recovery Small (append-only) B-Tree Snapshots Full database state Fast baseline restore Larger (full state) Key Principles:\nNamespace isolation: Each namespace has its own backup root (\u0026lt;dataDir\u0026gt;/backups/{namespace}) Crash-consistent: All backups are atomic and immediately usable BYO tooling: UnisonDB emits raw files; you control compression, encryption, and storage Security: Relative paths only—no directory traversal or cross-namespace access Backup APIs# WAL Segment Backup# Copies sealed WAL segments to a backup directory for incremental archival.\nEndpoint:\nPOST /api/v1/{namespace}/wal/backupRequest Body:\n{ \u0026#34;afterSegmentId\u0026#34;: 42, \u0026#34;backupDir\u0026#34;: \u0026#34;wal/customer-a\u0026#34; }Parameters:\nField Type Required Description afterSegmentId integer No Only copy segments with IDs greater than this value. Omit or set to 0 to copy all sealed segments. backupDir string Yes Relative path within \u0026lt;dataDir\u0026gt;/backups/{namespace}. Absolute paths or .. traversal are rejected. Response:\n{ \u0026#34;backups\u0026#34;: [ { \u0026#34;segmentId\u0026#34;: 43, \u0026#34;path\u0026#34;: \u0026#34;/var/unison/data/backups/users/wal/customer-a/000000043.wal\u0026#34; }, { \u0026#34;segmentId\u0026#34;: 44, \u0026#34;path\u0026#34;: \u0026#34;/var/unison/data/backups/users/wal/customer-a/000000044.wal\u0026#34; } ] }Example:\ncurl -X POST http://localhost:8080/api/v1/users/wal/backup \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;afterSegmentId\u0026#34;: 100, \u0026#34;backupDir\u0026#34;: \u0026#34;wal/daily\u0026#34; }\u0026#39; B-Tree Snapshot Backup# Creates a consistent snapshot of the entire B-Tree store.\nEndpoint:\nPOST /api/v1/{namespace}/btree/backupRequest Body:\n{ \u0026#34;path\u0026#34;: \u0026#34;snapshots/users-20250108.snapshot\u0026#34; }Parameters:\nField Type Required Description path string Yes Relative path within \u0026lt;dataDir\u0026gt;/backups/{namespace}. UnisonDB writes to {path}.tmp, fsyncs, then atomically renames. Response:\n{ \u0026#34;path\u0026#34;: \u0026#34;/var/unison/data/backups/users/snapshots/users-20250108.snapshot\u0026#34;, \u0026#34;bytes\u0026#34;: 73400320 }Example:\ncurl -X POST http://localhost:8080/api/v1/users/btree/backup \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;path\u0026#34;: \u0026#34;snapshots/users-\u0026#39;$(date +%Y%m%d)\u0026#39;.snapshot\u0026#34; }\u0026#39; Backup Strategies# 1. Full Backup (Snapshot Only)# Simple strategy for small databases or infrequent backups.\nWorkflow:\n# Daily full snapshot curl -X POST http://localhost:8080/api/v1/users/btree/backup \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;path\u0026#34;: \u0026#34;daily/snapshot-\u0026#39;$(date +%Y%m%d)\u0026#39;.db\u0026#34;}\u0026#39; 2. Incremental Backup (WAL + Periodic Snapshots)# Workflow:\nDaily baseline:\n# Full B-Tree snapshot curl -X POST http://localhost:8080/api/v1/users/btree/backup \\ -d \u0026#39;{\u0026#34;path\u0026#34;: \u0026#34;weekly/snapshot-\u0026#39;$(date +%Y%m%d)\u0026#39;.db\u0026#34;}\u0026#39;Hourly incremental:\n# Every hour: Copy new WAL segments LAST_SEGMENT=$(cat /var/backups/last_wal_id.txt || echo 0) curl -X POST http://localhost:8080/api/v1/users/wal/backup \\ -d \u0026#34;{\\\u0026#34;afterSegmentId\\\u0026#34;: $LAST_SEGMENT, \\\u0026#34;backupDir\\\u0026#34;: \\\u0026#34;wal/$(date +%Y%m%d-%H)\\\u0026#34;}\u0026#34; \\ | jq -r \u0026#39;.backups[-1].segmentId\u0026#39; \u0026gt; /var/backups/last_wal_id.txt 3. Continuous WAL Archival# Workflow:\nUse a cron job or systemd timer to continuously ship WAL segments:\n#!/bin/bash # /usr/local/bin/unison-wal-archive.sh NAMESPACE=\u0026#34;users\u0026#34; STATE_FILE=\u0026#34;/var/lib/unison/wal-archive-state.json\u0026#34; # Read last archived segment LAST_SEGMENT=$(jq -r \u0026#39;.lastSegmentId // 0\u0026#39; \u0026#34;$STATE_FILE\u0026#34; 2\u0026gt;/dev/null || echo 0) # Backup new segments RESPONSE=$(curl -s -X POST http://localhost:8080/api/v1/$NAMESPACE/wal/backup \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#34;{\\\u0026#34;afterSegmentId\\\u0026#34;: $LAST_SEGMENT, \\\u0026#34;backupDir\\\u0026#34;: \\\u0026#34;wal/archive\\\u0026#34;}\u0026#34;) # Extract latest segment ID NEW_LAST=$(echo \u0026#34;$RESPONSE\u0026#34; | jq -r \u0026#39;.backups[-1].segmentId // 0\u0026#39;) if [ \u0026#34;$NEW_LAST\u0026#34; != \u0026#34;0\u0026#34; ]; then # Update state echo \u0026#34;{\\\u0026#34;lastSegmentId\\\u0026#34;: $NEW_LAST, \\\u0026#34;timestamp\\\u0026#34;: \\\u0026#34;$(date -Iseconds)\\\u0026#34;}\u0026#34; \u0026gt; \u0026#34;$STATE_FILE\u0026#34; # Compress and upload to S3 find /var/unison/data/backups/$NAMESPACE/wal/archive -name \u0026#34;*.wal\u0026#34; \\ -mmin -15 -exec gzip {} \\; \\ -exec aws s3 cp {}.gz s3://backups/unison/$NAMESPACE/wal/ \\; fiCron schedule:\n*/15 * * * * /usr/local/bin/unison-wal-archive.sh Backup Automation# Systemd Timer Example# Service unit (/etc/systemd/system/unison-backup.service):\n[Unit] Description=UnisonDB Backup Service After=network.target [Service] Type=oneshot User=unison ExecStart=/usr/local/bin/unison-backup.sh StandardOutput=journal StandardError=journalTimer unit (/etc/systemd/system/unison-backup.timer):\n[Unit] Description=UnisonDB Backup Timer [Timer] OnCalendar=daily OnCalendar=*:0/6 # Every 6 hours Persistent=true [Install] WantedBy=timers.targetBackup script (/usr/local/bin/unison-backup.sh):\n#!/bin/bash set -euo pipefail NAMESPACE=\u0026#34;users\u0026#34; BACKUP_ROOT=\u0026#34;/var/unison/data/backups/$NAMESPACE\u0026#34; DATE=$(date +%Y%m%d-%H%M) # 1. B-Tree snapshot (daily at midnight) if [ \u0026#34;$(date +%H%M)\u0026#34; = \u0026#34;0000\u0026#34; ]; then echo \u0026#34;Creating B-Tree snapshot...\u0026#34; curl -X POST http://localhost:8080/api/v1/$NAMESPACE/btree/backup \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#34;{\\\u0026#34;path\\\u0026#34;: \\\u0026#34;snapshots/btree-$DATE.db\\\u0026#34;}\u0026#34; # Compress snapshot zstd -q --rm \u0026#34;$BACKUP_ROOT/snapshots/btree-$DATE.db\u0026#34; # Upload to S3 aws s3 cp \u0026#34;$BACKUP_ROOT/snapshots/btree-$DATE.db.zst\u0026#34; \\ s3://backups/unison/$NAMESPACE/snapshots/ fi # 2. WAL incremental (every run) echo \u0026#34;Backing up WAL segments...\u0026#34; STATE_FILE=\u0026#34;/var/lib/unison/wal-state-$NAMESPACE.json\u0026#34; LAST_SEGMENT=$(jq -r \u0026#39;.lastSegmentId // 0\u0026#39; \u0026#34;$STATE_FILE\u0026#34; 2\u0026gt;/dev/null || echo 0) RESPONSE=$(curl -s -X POST http://localhost:8080/api/v1/$NAMESPACE/wal/backup \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#34;{\\\u0026#34;afterSegmentId\\\u0026#34;: $LAST_SEGMENT, \\\u0026#34;backupDir\\\u0026#34;: \\\u0026#34;wal/$DATE\\\u0026#34;}\u0026#34;) # Update state NEW_LAST=$(echo \u0026#34;$RESPONSE\u0026#34; | jq -r \u0026#39;.backups[-1].segmentId // 0\u0026#39;) if [ \u0026#34;$NEW_LAST\u0026#34; != \u0026#34;0\u0026#34; ]; then echo \u0026#34;{\\\u0026#34;lastSegmentId\\\u0026#34;: $NEW_LAST, \\\u0026#34;timestamp\\\u0026#34;: \\\u0026#34;$(date -Iseconds)\\\u0026#34;}\u0026#34; \u0026gt; \u0026#34;$STATE_FILE\u0026#34; # Compress and upload WAL segments tar -czf \u0026#34;$BACKUP_ROOT/wal-$DATE.tar.gz\u0026#34; -C \u0026#34;$BACKUP_ROOT\u0026#34; \u0026#34;wal/$DATE\u0026#34; aws s3 cp \u0026#34;$BACKUP_ROOT/wal-$DATE.tar.gz\u0026#34; s3://backups/unison/$NAMESPACE/wal/ # Cleanup local WAL backups older than 7 days find \u0026#34;$BACKUP_ROOT/wal\u0026#34; -type d -mtime +7 -exec rm -rf {} + fi echo \u0026#34;Backup completed successfully\u0026#34;Enable the timer:\nsudo systemctl daemon-reload sudo systemctl enable --now unison-backup.timer sudo systemctl status unison-backup.timer Storage Integration Examples# Amazon S3# #!/bin/bash # Backup to S3 with encryption NAMESPACE=\u0026#34;users\u0026#34; BACKUP_DIR=\u0026#34;/var/unison/data/backups/$NAMESPACE\u0026#34; S3_BUCKET=\u0026#34;s3://company-backups/unison/$NAMESPACE\u0026#34; DATE=$(date +%Y%m%d) # 1. Create B-Tree snapshot curl -X POST http://localhost:8080/api/v1/$NAMESPACE/btree/backup \\ -d \u0026#34;{\\\u0026#34;path\\\u0026#34;: \\\u0026#34;snapshots/btree-$DATE.db\\\u0026#34;}\u0026#34; # 2. Compress and encrypt zstd -q \u0026#34;$BACKUP_DIR/snapshots/btree-$DATE.db\u0026#34; gpg --encrypt --recipient backup@company.com \\ \u0026#34;$BACKUP_DIR/snapshots/btree-$DATE.db.zst\u0026#34; # 3. Upload to S3 with server-side encryption aws s3 cp \u0026#34;$BACKUP_DIR/snapshots/btree-$DATE.db.zst.gpg\u0026#34; \\ \u0026#34;$S3_BUCKET/snapshots/\u0026#34; \\ --storage-class STANDARD_IA \\ --server-side-encryption AES256 # 4. Cleanup local files rm -f \u0026#34;$BACKUP_DIR/snapshots/btree-$DATE.db\u0026#34;* Restore Procedures# Full Restore from B-Tree Snapshot# Scenario: Complete data loss, restore from latest snapshot.\n# 1. Stop UnisonDB sudo systemctl stop unisondb # 2. Download snapshot from S3 aws s3 cp s3://backups/unison/users/snapshots/btree-20250108.db.zst \\ /tmp/restore.db.zst # 3. Decompress zstd -d /tmp/restore.db.zst -o /tmp/restore.db # 4. Replace existing database NAMESPACE=\u0026#34;users\u0026#34; DATA_DIR=\u0026#34;/var/unison/data/$NAMESPACE\u0026#34; rm -rf \u0026#34;$DATA_DIR/db\u0026#34;/* mkdir -p \u0026#34;$DATA_DIR/db\u0026#34; # Copy snapshot to data directory (engine-specific) # For LMDB: cp /tmp/restore.db \u0026#34;$DATA_DIR/db/data.mdb\u0026#34; # 5. Restart UnisonDB sudo systemctl start unisondb # 6. Verify curl http://localhost:8080/api/v1/users/kv/test-key Point-in-Time Recovery (PITR)# Scenario: Restore to a specific point in time using snapshot + WAL replay.\n#!/bin/bash # Restore to 2025-01-08 14:30:00 UTC TARGET_TIME=\u0026#34;2025-01-08T14:30:00Z\u0026#34; NAMESPACE=\u0026#34;users\u0026#34; # 1. Find baseline snapshot before target time SNAPSHOT=$(aws s3 ls s3://backups/unison/$NAMESPACE/snapshots/ | \\ awk \u0026#39;{print $4}\u0026#39; | \\ grep -E \u0026#39;btree-[0-9]{8}\u0026#39; | \\ sort | \\ awk -v target=\u0026#34;$(date -d \u0026#34;$TARGET_TIME\u0026#34; +%Y%m%d)\u0026#34; \u0026#39;$0 \u0026lt;= target\u0026#39; | \\ tail -1) echo \u0026#34;Using baseline snapshot: $SNAPSHOT\u0026#34; # 2. Download and restore snapshot aws s3 cp \u0026#34;s3://backups/unison/$NAMESPACE/snapshots/$SNAPSHOT\u0026#34; /tmp/ zstd -d \u0026#34;/tmp/$SNAPSHOT\u0026#34; -o /tmp/restore.db # 3. Download WAL segments after snapshot SNAPSHOT_DATE=$(echo $SNAPSHOT | grep -oE \u0026#39;[0-9]{8}\u0026#39;) mkdir -p /tmp/wal-restore aws s3 sync \u0026#34;s3://backups/unison/$NAMESPACE/wal/\u0026#34; /tmp/wal-restore/ \\ --exclude \u0026#34;*\u0026#34; \\ --include \u0026#34;wal-${SNAPSHOT_DATE}*.tar.gz\u0026#34; # 4. Extract WAL segments for archive in /tmp/wal-restore/*.tar.gz; do tar -xzf \u0026#34;$archive\u0026#34; -C /tmp/wal-restore/ done # 5. Stop UnisonDB and restore sudo systemctl stop unisondb DATA_DIR=\u0026#34;/var/unison/data/$NAMESPACE\u0026#34; rm -rf \u0026#34;$DATA_DIR/db\u0026#34;/* \u0026#34;$DATA_DIR/wal\u0026#34;/* cp /tmp/restore.db \u0026#34;$DATA_DIR/db/data.mdb\u0026#34; # 6. Copy WAL segments up to target time # (UnisonDB will replay WAL on startup) find /tmp/wal-restore -name \u0026#34;*.wal\u0026#34; | sort | while read wal; do # Check WAL timestamp (implementation-specific) # Copy only if before target time cp \u0026#34;$wal\u0026#34; \u0026#34;$DATA_DIR/wal/\u0026#34; done # 7. Restart and verify sudo systemctl start unisondb Best Practices# Backup Schedule# Frequency Component Retention Hourly WAL segments 7 days local, 30 days remote Daily B-Tree snapshot 7 days local, 90 days remote Weekly Full backup (WAL + snapshot) 1 year Monthly Compliance archive 7 years (if required) Security# 1. Encrypt backups:\n# GPG encryption before upload gpg --encrypt --recipient backup-key@company.com backup.db2. Access control:\n# Restrict backup directory permissions chmod 700 /var/unison/data/backups chown unison:unison /var/unison/data/backups3. Audit logging:\n# Log all backup API calls curl -X POST http://localhost:8080/api/v1/users/wal/backup \\ -d \u0026#39;{\u0026#34;backupDir\u0026#34;: \u0026#34;wal/archive\u0026#34;}\u0026#39; \\ | tee -a /var/log/unison-backups.logTesting Restores# Monthly restore drill:\n#!/bin/bash # Test restore in isolated environment NAMESPACE=\u0026#34;users\u0026#34; TEST_DIR=\u0026#34;/tmp/restore-test-$(date +%s)\u0026#34; # 1. Create test environment mkdir -p \u0026#34;$TEST_DIR\u0026#34;/{db,wal} # 2. Download latest snapshot aws s3 cp s3://backups/unison/$NAMESPACE/snapshots/latest.db.zst \u0026#34;$TEST_DIR/\u0026#34; zstd -d \u0026#34;$TEST_DIR/latest.db.zst\u0026#34; -o \u0026#34;$TEST_DIR/db/data.mdb\u0026#34; # 3. Start UnisonDB in test mode unisondb --data-dir \u0026#34;$TEST_DIR\u0026#34; --http-addr localhost:9999 \u0026amp; PID=$! # 4. Verify data integrity sleep 5 curl http://localhost:9999/health curl http://localhost:9999/api/v1/$NAMESPACE/kv/test-key # 5. Cleanup kill $PID rm -rf \u0026#34;$TEST_DIR\u0026#34; echo \u0026#34;Restore test completed successfully\u0026#34;Monitoring# Key metrics to track:\n# Backup success rate curl http://localhost:8080/metrics | grep backup_success_total # Last backup timestamp curl http://localhost:8080/metrics | grep backup_last_timestamp_seconds # Backup size curl http://localhost:8080/metrics | grep backup_bytes_totalAlert on:\nBackup failures (2+ consecutive failures) Backup age \u0026gt; 24 hours Backup size anomalies (\u0026gt;50% change) WAL segment gap detection Troubleshooting# Backup Fails with Permission Denied# Symptom:\n{\u0026#34;error\u0026#34;: \u0026#34;permission denied: /var/unison/data/backups/users/wal\u0026#34;}Solution:\n# Ensure backup directory is writable sudo chown -R unison:unison /var/unison/data/backups sudo chmod -R 755 /var/unison/data/backups Backup Directory Full# Symptom:\n{\u0026#34;error\u0026#34;: \u0026#34;no space left on device\u0026#34;}Solution:\n# Clean up old local backups find /var/unison/data/backups -type f -mtime +7 -delete # Or mount separate volume sudo mount /dev/sdb1 /var/unison/data/backups Restore Fails with Corrupted Snapshot# Symptom:\nERROR: database file is corruptedSolution:\n# 1. Verify checksum (if available) sha256sum backup.db # Compare with original checksum # 2. Try previous snapshot aws s3 ls s3://backups/unison/users/snapshots/ | sort | tail -2 Next Steps# Deployment Topologies - High availability with relayers Monitoring - Tracking backup health HTTP API Reference - Complete API documentation "},{"id":9,"href":"/docs/examples/multi-dc-crdt/","title":"Multi-DC CRDT Replication","section":"Examples","content":" Introduction: The Challenge of Distributed State Management# Imagine you\u0026rsquo;re building a globally distributed application where users across different continents need to see consistent data think user presence status, live dashboards, or real-time collaboration features. Traditional databases force you to choose between consistency and availability, but what if there was a better way?\nConflict-free Replicated Data Types (CRDTs) offer a mathematical approach to distributed state management where conflicts are resolved automatically through well-defined merge operations. When combined with edge notifications, you get a powerful pattern: write anywhere, replicate everywhere, and get notified of changes in real-time.\nIn this post, we\u0026rsquo;ll build a multi-datacenter system using UnisonDB that demonstrates:\nConcurrent writes to multiple datacenters Automatic conflict resolution using CRDTs Real-time change notifications via ZeroMQ Eventual consistency across all nodes Architecture Overview# Our demo system consists of three UnisonDB nodes:\n+---------------------------------------------------------------+ | Multi-DC CRDT Architecture | +---------------------------------------------------------------+ Writes Writes | | v v +----------------+ +----------------+ | Datacenter 1 | | Datacenter 2 | | (Primary) | | (Primary) | | | | | | HTTP: 8001 | | HTTP: 8002 | | gRPC: 4001 | | gRPC: 4002 | +--------+-------+ +--------+-------+ | | | gRPC Replication | +---------------------+-------------------+ | v +---------------------+ | Relayer | | (Read-Only) | | | | HTTP: 8003 | | ZMQ dc1: 5555 ---\u0026gt; |----+ | ZMQ dc2: 5556 ---\u0026gt; |----+ Watch API +---------------------+ | Notifications | v +--------------------+ | CRDT Client | | (Go / Node.js) | | | | Converged State | +--------------------+Component Roles# Component Role Namespace HTTP Port gRPC Port ZMQ Ports DC1 Primary (accepts writes) ad-campaign-dc1 8001 4001 - DC2 Primary (accepts writes) ad-campaign-dc2 8002 4002 - Relayer Read-only replica ad-campaign-dc1, ad-campaign-dc2 8003 - 5555, 5556 Building and Running UnisonDB# Prerequisites# # Ensure you have Go 1.21+ and CGO enabled go version # go version go1.21.0 or higherStep 1: Build UnisonDB# This needs Zero MQ Installed Make Sure You\u0026rsquo;ve have it Installed. Install ZeroMQ dependency # Clone the repository git clone https://github.com/ankur-anand/unisondb.git cd unisondb # Build the binary (CGO required for RocksDB) CGO_ENABLED=1 go build -tags zeromq ./cmd/unisondbStep 2: Start the Multi-DC Cluster# Open three separate terminal windows and run:\nTerminal 1: Start Datacenter 1\n./unisondb -config ./cmd/examples/crdt-multi-dc/configs/dc1.toml replicatorTerminal 2: Start Datacenter 2\n./unisondb -config ./cmd/examples/crdt-multi-dc/configs/dc2.toml replicatorTerminal 3: Start Relayer\n./unisondb -config ./cmd/examples/crdt-multi-dc/configs/relayer.toml relayerYou should see output indicating each node is ready:\nINFO: HTTP server listening on :8001 INFO: gRPC server listening on :4001 INFO: Namespace \u0026#39;ad-campaign-dc1\u0026#39; initializedStep 3: Start the CRDT Client# Open a fourth terminal to run the client that will observe CRDT state:\ncd cmd/examples/golang-crdt-client go run main.goExpected output:\nWaiting for change notifications... Connecting to ZeroMQ ad-campaign-dc1: tcp://localhost:5555 Connecting to ZeroMQ ad-campaign-dc2: tcp://localhost:5556 ZeroMQ listener started for namespace: ad-campaign-dc1 ZeroMQ listener started for namespace: ad-campaign-dc2Your system is now ready!\nUnderstanding CRDTs: Two Types in Action# 1. LWW-Register (Last-Write-Wins Register)# Use Cases: User profiles, configuration settings, feature flags\nHow it works:\nEach write includes a timestamp and replica ID Conflicts are resolved by choosing the write with the latest timestamp If timestamps are equal, the lexicographically higher replica ID wins Data Format:\n{ \u0026#34;value\u0026#34;: \u0026#34;actual data\u0026#34;, \u0026#34;timestamp\u0026#34;: 1698765432000, \u0026#34;replica\u0026#34;: \u0026#34;ad-campaign-dc1\u0026#34; }2. G-Counter (Grow-Only Counter)# Use Cases: Page views, API calls, distributed metrics (monotonically increasing)\nHow it works:\nEach replica maintains its own counter Merging takes the maximum count per replica Total value is the sum of all replica counters Can only increase (never decrease) Data Format:\n{ \u0026#34;replica\u0026#34;: \u0026#34;ad-campaign-dc1\u0026#34;, \u0026#34;count\u0026#34;: 5 }Demo Scenarios with curl Examples# Scenario 1: Basic LWW-Register Update# Let\u0026rsquo;s update a user\u0026rsquo;s status across two datacenters:\nWrite \u0026ldquo;online\u0026rdquo; to DC1 (timestamp: 1698765432000)\ncurl -X PUT \u0026#34;http://localhost:8001/api/v1/ad-campaign-dc1/kv/lww:user-status\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#39;{\u0026#34;value\u0026#34;:\u0026#34;online\u0026#34;,\u0026#34;timestamp\u0026#34;:1698765432000,\u0026#34;replica\u0026#34;:\u0026#34;ad-campaign-dc1\u0026#34;}\u0026#39; | base64)\u0026#39;\u0026#34;}\u0026#39;Client Output:\nChange notification received Topic: ad-campaign-dc1.kv Key: lww:user-status Operation: put Processing update: lww:user-status LWW-Register updated: lww:user-status Value: online Timestamp: 1698765432000 Replica: ad-campaign-dc1 CURRENT CRDT STATE LWW-Registers: lww:user-status: Value: online Timestamp: 1698765432000 Replica: ad-campaign-dc1Now write \u0026ldquo;away\u0026rdquo; to DC2 with a newer timestamp:\nWrite \u0026ldquo;away\u0026rdquo; to DC2 (timestamp: 1698765433000)\ncurl -X PUT \u0026#34;http://localhost:8002/api/v1/ad-campaign-dc2/kv/lww:user-status\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#39;{\u0026#34;value\u0026#34;:\u0026#34;away\u0026#34;,\u0026#34;timestamp\u0026#34;:1698765433000,\u0026#34;replica\u0026#34;:\u0026#34;ad-campaign-dc2\u0026#34;}\u0026#39; | base64)\u0026#39;\u0026#34;}\u0026#39;Client Output:\nChange notification received Topic: ad-campaign-dc2.kv Key: lww:user-status Operation: put Processing update: lww:user-status LWW-Register updated: lww:user-status Value: away Timestamp: 1698765433000 Replica: ad-campaign-dc2 CURRENT CRDT STATE LWW-Registers: lww:user-status: Value: away Timestamp: 1698765433000 Replica: ad-campaign-dc2What happened? The client automatically resolved the conflict! DC2\u0026rsquo;s write won because it had a newer timestamp (1698765433000 \u0026gt; 1698765432000).\nScenario 2: Concurrent Writes with Same Timestamp# What happens when two datacenters write at the exact same millisecond?\nWrite to DC1:\nTIMESTAMP=$(date +%s)000 curl -X PUT \u0026#34;http://localhost:8001/api/v1/ad-campaign-dc1/kv/lww:config\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#34;{\\\u0026#34;value\\\u0026#34;:\\\u0026#34;DC1 wins?\\\u0026#34;,\\\u0026#34;timestamp\\\u0026#34;:$TIMESTAMP,\\\u0026#34;replica\\\u0026#34;:\\\u0026#34;ad-campaign-dc1\\\u0026#34;}\u0026#34; | base64)\u0026#39;\u0026#34;}\u0026#39;Write to DC2 (same timestamp):\ncurl -X PUT \u0026#34;http://localhost:8002/api/v1/ad-campaign-dc2/kv/lww:config\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#34;{\\\u0026#34;value\\\u0026#34;:\\\u0026#34;DC2 wins!\\\u0026#34;,\\\u0026#34;timestamp\\\u0026#34;:$TIMESTAMP,\\\u0026#34;replica\\\u0026#34;:\\\u0026#34;ad-campaign-dc2\\\u0026#34;}\u0026#34; | base64)\u0026#39;\u0026#34;}\u0026#39;Result: ad-campaign-dc2 wins because lexicographically \u0026quot;ad-campaign-dc2\u0026quot; \u0026gt; \u0026quot;ad-campaign-dc1\u0026quot;. This ensures deterministic conflict resolution across all replicas.\nScenario 3: Distributed Counter (G-Counter)# Let\u0026rsquo;s track page views across two datacenters:\nDC1 serves 5 requests:\ncurl -X PUT \u0026#34;http://localhost:8001/api/v1/ad-campaign-dc1/kv/counter:page-views\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#39;{\u0026#34;replica\u0026#34;:\u0026#34;ad-campaign-dc1\u0026#34;,\u0026#34;count\u0026#34;:5}\u0026#39; | base64)\u0026#39;\u0026#34;}\u0026#39;DC2 serves 3 requests:\ncurl -X PUT \u0026#34;http://localhost:8002/api/v1/ad-campaign-dc2/kv/counter:page-views\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#39;{\u0026#34;replica\u0026#34;:\u0026#34;ad-campaign-dc2\u0026#34;,\u0026#34;count\u0026#34;:3}\u0026#39; | base64)\u0026#39;\u0026#34;}\u0026#39;Client Output:\nCURRENT CRDT STATE G-Counters: counter:page-views: Replica Counts: {\u0026#34;ad-campaign-dc1\u0026#34;:5,\u0026#34;ad-campaign-dc2\u0026#34;:3} Total: 8Result: Total = 8 (5 from DC1 + 3 from DC2). The counters from both datacenters are automatically merged!\nScenario 4: Out-of-Order Delivery (Stale Write)# What if network delays cause an old write to arrive after a newer one?\nWrite NEW value to DC1:\ncurl -X PUT \u0026#34;http://localhost:8001/api/v1/ad-campaign-dc1/kv/lww:feature-flag\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#39;{\u0026#34;value\u0026#34;:true,\u0026#34;timestamp\u0026#34;:2000,\u0026#34;replica\u0026#34;:\u0026#34;ad-campaign-dc1\u0026#34;}\u0026#39; | base64)\u0026#39;\u0026#34;}\u0026#39;Write OLD value to DC2 (stale):\ncurl -X PUT \u0026#34;http://localhost:8002/api/v1/ad-campaign-dc2/kv/lww:feature-flag\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#39;{\u0026#34;value\u0026#34;:false,\u0026#34;timestamp\u0026#34;:1000,\u0026#34;replica\u0026#34;:\u0026#34;ad-campaign-dc2\u0026#34;}\u0026#39; | base64)\u0026#39;\u0026#34;}\u0026#39;Client Output:\nProcessing update: lww:feature-flag LWW-Register ignored (stale): lww:feature-flag Incoming timestamp: 1000 Current timestamp: 2000Result: The stale write is automatically ignored. The CRDT logic ensures we never regress to an older state!\nScenario 5: Multiple Counters Operating Independently# # Track different metrics curl -X PUT \u0026#34;http://localhost:8001/api/v1/ad-campaign-dc1/kv/counter:api-calls\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#39;{\u0026#34;replica\u0026#34;:\u0026#34;ad-campaign-dc1\u0026#34;,\u0026#34;count\u0026#34;:100}\u0026#39; | base64)\u0026#39;\u0026#34;}\u0026#39; curl -X PUT \u0026#34;http://localhost:8002/api/v1/ad-campaign-dc2/kv/counter:api-calls\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#39;{\u0026#34;replica\u0026#34;:\u0026#34;ad-campaign-dc2\u0026#34;,\u0026#34;count\u0026#34;:75}\u0026#39; | base64)\u0026#39;\u0026#34;}\u0026#39; curl -X PUT \u0026#34;http://localhost:8001/api/v1/ad-campaign-dc1/kv/counter:db-queries\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#39;{\u0026#34;replica\u0026#34;:\u0026#34;ad-campaign-dc1\u0026#34;,\u0026#34;count\u0026#34;:250}\u0026#39; | base64)\u0026#39;\u0026#34;}\u0026#39;Client Output:\nCURRENT CRDT STATE G-Counters: counter:api-calls: Replica Counts: {\u0026#34;ad-campaign-dc1\u0026#34;:100,\u0026#34;ad-campaign-dc2\u0026#34;:75} Total: 175 counter:db-queries: Replica Counts: {\u0026#34;ad-campaign-dc1\u0026#34;:250} Total: 250Each counter operates independently with its own convergence!\nReading Data from the Relayer# The relayer provides read-only access to both datacenter namespaces:\nRead from DC1 namespace:\ncurl \u0026#34;http://localhost:8003/api/v1/ad-campaign-dc1/kv/lww:user-status\u0026#34; | jqResponse:\n{ \u0026#34;value\u0026#34;: \u0026#34;eyJ2YWx1ZSI6ImF3YXkiLCJ0aW1lc3RhbXAiOjE2OTg3NjU0MzMwMDAsInJlcGxpY2EiOiJhZC1jYW1wYWlnbi1kYzIifQ==\u0026#34;, \u0026#34;found\u0026#34;: true }Decode the base64 value:\necho \u0026#34;eyJ2YWx1ZSI6ImF3YXkiLCJ0aW1lc3RhbXAiOjE2OTg3NjU0MzMwMDAsInJlcGxpY2EiOiJhZC1jYW1wYWlnbi1kYzIifQ==\u0026#34; | base64 -d | jqOutput:\n{ \u0026#34;value\u0026#34;: \u0026#34;away\u0026#34;, \u0026#34;timestamp\u0026#34;: 1698765433000, \u0026#34;replica\u0026#34;: \u0026#34;ad-campaign-dc2\u0026#34; }How Conflict Resolution Works Under the Hood# LWW-Register Algorithm# The conflict resolution logic in lww_register.go:30-39:\nfunc (r *LWWRegister) Update(value interface{}, timestamp int64, replica string) bool { // Rule 1: Accept if timestamp is newer if timestamp \u0026gt; r.Timestamp { r.Value = value r.Timestamp = timestamp r.Replica = replica return true } // Rule 2: If timestamps equal, use replica ID as tiebreaker if timestamp == r.Timestamp \u0026amp;\u0026amp; replica \u0026gt; r.Replica { r.Value = value r.Replica = replica return true } // Rule 3: Reject stale updates return false }Key Properties:\nCommutative: Order of updates doesn\u0026rsquo;t matter Associative: Grouping of updates doesn\u0026rsquo;t matter Idempotent: Applying the same update multiple times is safe Deterministic: All replicas converge to the same value G-Counter Merge Algorithm# The merge logic in g_counter.go:\nfunc (c *GCounter) Merge(replica string, count int64) bool { current := c.Counts[replica] // Only accept higher counts (monotonic) if count \u0026gt; current { c.Counts[replica] = count return true } return false } func (c *GCounter) GetValue() int64 { total := int64(0) for _, count := range c.Counts { total += count } return total }Key Properties:\nMonotonic: Values only increase Convergent: All replicas reach the same total Partition-tolerant: Works across network splits Real-World Use Cases# 1. User Presence System# # User goes online in US datacenter curl -X PUT \u0026#34;http://localhost:8001/api/v1/ad-campaign-dc1/kv/lww:user:alice:status\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#34;{\\\u0026#34;value\\\u0026#34;:\\\u0026#34;online\\\u0026#34;,\\\u0026#34;timestamp\\\u0026#34;:$(date +%s)000,\\\u0026#34;replica\\\u0026#34;:\\\u0026#34;us-east-1\\\u0026#34;}\u0026#34; | base64)\u0026#39;\u0026#34;}\u0026#39; # User goes away in EU datacenter (newer timestamp wins) curl -X PUT \u0026#34;http://localhost:8002/api/v1/ad-campaign-dc2/kv/lww:user:alice:status\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#34;{\\\u0026#34;value\\\u0026#34;:\\\u0026#34;away\\\u0026#34;,\\\u0026#34;timestamp\\\u0026#34;:$(($(date +%s)+5))000,\\\u0026#34;replica\\\u0026#34;:\\\u0026#34;eu-west-1\\\u0026#34;}\u0026#34; | base64)\u0026#39;\u0026#34;}\u0026#39;All clients worldwide see the latest status in real-time!\n2. Distributed Analytics# # Track impressions across regions curl -X PUT \u0026#34;http://localhost:8001/api/v1/ad-campaign-dc1/kv/counter:campaign-123:impressions\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#39;{\u0026#34;replica\u0026#34;:\u0026#34;us-east-1\u0026#34;,\u0026#34;count\u0026#34;:1500}\u0026#39; | base64)\u0026#39;\u0026#34;}\u0026#39; curl -X PUT \u0026#34;http://localhost:8002/api/v1/ad-campaign-dc2/kv/counter:campaign-123:impressions\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#39;{\u0026#34;replica\u0026#34;:\u0026#34;eu-west-1\u0026#34;,\u0026#34;count\u0026#34;:2300}\u0026#39; | base64)\u0026#39;\u0026#34;}\u0026#39; # Global total: 3800 impressions3. Feature Flags# # Enable feature in production curl -X PUT \u0026#34;http://localhost:8001/api/v1/ad-campaign-dc1/kv/lww:feature:new-ui\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#34;{\\\u0026#34;value\\\u0026#34;:true,\\\u0026#34;timestamp\\\u0026#34;:$(date +%s)000,\\\u0026#34;replica\\\u0026#34;:\\\u0026#34;control-plane\\\u0026#34;}\u0026#34; | base64)\u0026#39;\u0026#34;}\u0026#39;Feature flag changes propagate globally within milliseconds!\nTry It Yourself# # Clone and run the example git clone https://github.com/ankur-anand/unisondb.git cd unisondb CGO_ENABLED=1 go build -tags zeromq -o unisondb ./cmd/unisondb # Start the demo cd cmd/examples/crdt-multi-dcWatch the magic happen as conflicts resolve themselves and state converges across datacenters!\nAdditional Resources# UnisonDB GitHub Repository CRDT Research Papers ZeroMQ Guide Have questions or want to contribute? Open an issue on GitHub or join our community discussions!\n"},{"id":10,"href":"/blog/building-corruption-proof-write-ahead-log-in-go/","title":"Building Corruption Proof Write Ahead Log in Go","section":"Blog Posts","content":"title: \u0026ldquo;Building a Corruption-Proof Write-Ahead Log in Go\u0026rdquo; date: 2025-12-14 description: \u0026ldquo;Learn how to build a crash-safe Write-Ahead Log (WAL) in Go. We dive into 12 layers of durability engineering—from CRC32 and mmap to segment rotation—used in UnisonDB to prevent data corruption.\u0026rdquo; keywords: [\u0026ldquo;Go\u0026rdquo;, \u0026ldquo;Golang\u0026rdquo;, \u0026ldquo;Write-Ahead Log\u0026rdquo;, \u0026ldquo;WAL\u0026rdquo;, \u0026ldquo;Database Engineering\u0026rdquo;, \u0026ldquo;Data Reliability\u0026rdquo;, \u0026ldquo;System Programming\u0026rdquo;, \u0026ldquo;UnisonDB\u0026rdquo;, \u0026ldquo;fsync\u0026rdquo;, \u0026ldquo;mmap\u0026rdquo;]# The Problem: WALs That Lie# Every database promises durability. Write your data, get an acknowledgment, sleep well. But what happens between the write() syscall and the bits actually reaching the disk platter?\nA lot can go wrong:\nPower failure mid-write - Half your record makes it to disk Bit flips - Cosmic rays, dying SSDs, firmware bugs Kernel bugs - The page cache lies about what\u0026rsquo;s persisted Filesystem corruption - Journaling doesn\u0026rsquo;t save you from everything Torn writes - Your 4KB write spans two sectors, only one commits Most WAL implementations handle the easy cases. We wanted to handle them all.\nThe Stakes: Streaming Replication# UnisonDB isn\u0026rsquo;t a typical database. Our WAL isn\u0026rsquo;t just a recovery mechanism—it\u0026rsquo;s the primary source for replication. Followers continuously read from the leader\u0026rsquo;s WAL segments, applying records as they arrive.\n┌─────────┐ WAL Segments ┌──────────┐ │ Leader │ ──────────────────▶ │ Follower │ │ │ (streaming read) │ │ └─────────┘ └──────────┘This means:\nRead performance matters as much as write performance Corruption propagates - A bad record on the leader poisons followers Recovery must be fast - We can\u0026rsquo;t scan terabytes on every restart The WAL is our single source of truth. It had better be correct.\nLayer 1: The Record Format# Every record in our WAL has this structure:\n┌─────────────────────────────────────────────────────────────────┐ │ WAL Record │ ├──────────┬──────────┬─────────────────────┬─────────────────────┤ │ CRC32 │ Length │ Data │ Trailer │ │ 4 bytes │ 4 bytes │ N bytes │ 8 bytes │ ├──────────┴──────────┴─────────────────────┴─────────────────────┤ │ Padded to 8-byte boundary │ └─────────────────────────────────────────────────────────────────┘Three layers of protection in every record:\nCRC32 (Castagnoli)# // CRC covers everything except itself crc := crc32.Checksum(buf[4:], crc32.MakeTable(crc32.Castagnoli)) binary.LittleEndian.PutUint32(buf[0:4], crc)We use Castagnoli (CRC32-C) instead of the older IEEE polynomial. Why?\nHardware acceleration on modern CPUs (SSE 4.2) Better error detection properties ~3x faster than IEEE CRC32 CRC catches:\nRandom bit flips Sector read errors Truncated data CRC doesn\u0026rsquo;t catch:\nIncomplete writes - If we crash mid-write, the CRC might be valid for the partial data Layer 2: The Trailer Canary# This is where 0xDEADBEEFFEEDFACE enters the story.\nconst recordTrailer uint64 = 0xDEADBEEFFEEDFACEEvery record ends with this 8-byte magic value. During recovery, we verify it:\nfunc (s *Segment) scanRecords() (uint32, error) { offset := uint32(segmentHeaderSize) for offset \u0026lt; s.writeOffset { // Read header crc := binary.LittleEndian.Uint32(s.mmapData[offset:]) length := binary.LittleEndian.Uint32(s.mmapData[offset+4:]) entrySize := alignedSize(length) trailerOffset := offset + recordHeaderSize + length // Verify trailer FIRST trailer := binary.LittleEndian.Uint64(s.mmapData[trailerOffset:]) if trailer != recordTrailer { // Incomplete write detected - stop here return offset, nil } // Then verify CRC if !verifyCRC(s.mmapData[offset:offset+entrySize], crc) { return offset, ErrCorruption } offset += entrySize } return offset, nil }Why the Trailer?# Consider this crash scenario:\nWrite sequence: 1. Write CRC (4 bytes) ✓ persisted 2. Write Length (4 bytes) ✓ persisted 3. Write Data (N bytes) ✗ CRASH - only 50% written 4. Write Trailer ✗ never reachedWithout the trailer, recovery sees:\nValid CRC (for partial data? unlikely but possible) Valid length field Garbage data With the trailer, recovery sees:\nTrailer missing or wrong → incomplete write, ignore this record This pattern was inspired by a real etcd bug (#6191 ) where torn writes corrupted the WAL. The trailer acts as a \u0026ldquo;commit marker\u0026rdquo; for each record.\nWhy This Specific Value?# 0xDEADBEEFFEEDFACE is:\nUnlikely to appear in real data (it\u0026rsquo;s a known debug pattern) Easy to spot in hex dumps Memorable for debugging sessions at 3 AM Layer 3: 8-Byte Alignment# Every record is padded to an 8-byte boundary:\nfunc alignedSize(dataLen uint32) uint32 { total := recordHeaderSize + dataLen + recordTrailerSize return (total + 7) \u0026amp;^ 7 // Round up to 8 }Why 8 bytes? Modern storage has interesting atomicity properties:\nHDDs: 512-byte sectors are atomic SSDs: 4KB-16KB pages, but internal writes may not be atomic NVMe: Some drives guarantee 8-byte atomic writes By aligning to 8 bytes, we reduce (but don\u0026rsquo;t eliminate) the chance of the trailer being split across a page boundary. Combined with the CRC, we\u0026rsquo;re covered either way.\nLayer 4: The Segment Header# Each 16MB segment file starts with a 64-byte header:\ntype segmentHeader struct { Magic uint32 // \u0026#34;UWAL\u0026#34; = 0x5557414C Version uint32 // Format version CreatedAt int64 // Nanosecond timestamp LastModified int64 // Updated on every write WriteOffset uint32 // Current write position EntryCount uint32 // Records in segment Flags uint32 // Active/Sealed state FirstLogIndex uint64 // For Raft: first log index Reserved [8]byte // Future use CRC32 uint32 // Header checksum }The header has its own CRC32. On recovery:\nfunc (s *Segment) verifyHeader() error { headerBytes := s.mmapData[:segmentHeaderSize-4] // Exclude CRC field expectedCRC := crc32.Checksum(headerBytes, crcTable) actualCRC := binary.LittleEndian.Uint32(s.mmapData[segmentHeaderSize-4:]) if expectedCRC != actualCRC { return ErrCorruptHeader } return nil }The WriteOffset Trick# Notice WriteOffset in the header. After a crash, we know exactly where valid data ends—no need to scan the entire segment:\n// Fast recovery path if segment.IsSealed() { // Sealed segments are immutable, trust the header return segment.header.WriteOffset, nil } // Active segment: verify from WriteOffset backward if needed return segment.scanRecords()For a 16MB segment with 10,000 records, this saves significant recovery time.\nLayer 5: The Sync Dance (mmap + fsync + dirsync)# Here\u0026rsquo;s where it gets interesting. We use memory-mapped I/O:\nmmapData, err := mmap.Map(fd, mmap.RDWR, 0)mmap is fast—writes go to the page cache, reads are zero-copy. But mmap has a dirty secret: msync() is advisory on most systems.\n// This doesn\u0026#39;t guarantee data is on disk! mmapData.Flush() // Calls msync(MS_ASYNC) internallySo we belt-and-suspenders it:\nfunc (s *Segment) sync() error { // Step 1: Flush mmap pages to page cache if err := s.mmapData.Flush(); err != nil { return err } // Step 2: Force page cache to disk if err := s.fd.Sync(); err != nil { // fsync() return err } // Step 3: Sync the directory (for metadata/renames) if err := s.dirSyncer.Sync(); err != nil { return err } return nil }Why Directory Sync?# On some filesystems (especially ext4 with certain mount options), a file can be written and fsync\u0026rsquo;d, but if you crash before the directory entry is updated, the file doesn\u0026rsquo;t exist on recovery.\ntype DirectorySyncer struct { dirFd *os.File } func (d *DirectorySyncer) Sync() error { return d.dirFd.Sync() // fsync on directory fd }This is especially important during segment rotation when we create new files.\nLayer 6: Conservative Recovery# Our recovery philosophy: when in doubt, stop.\nfunc (w *WALog) recoverSegments() error { segments := listSegmentFiles(w.dir) sort.Sort(segments) // By segment ID for _, seg := range segments { if err := seg.recover(); err != nil { // Don\u0026#39;t try to be clever - stop at first corruption w.logger.Error(\u0026#34;segment corrupt, truncating\u0026#34;, \u0026#34;segment\u0026#34;, seg.ID, \u0026#34;error\u0026#34;, err) return seg.truncateAtLastGoodRecord() } } return nil }We don\u0026rsquo;t try to skip corrupted records and continue. Why?\nOrdering matters - A gap in the log might hide a critical transaction Corruption often spreads - If one record is bad, neighbors might be too Raft requires contiguity - Log indices must be sequential After truncating, we zero-fill ahead to prevent stale data from being misread:\nfunc (s *Segment) truncateAt(offset uint32) error { // Zero 1KB ahead of truncation point zeroFill := make([]byte, 1024) copy(s.mmapData[offset:], zeroFill) // Update header s.header.WriteOffset = offset s.writeHeaderCRC() return s.sync() } The Full Write Path# Putting it all together:\nfunc (s *Segment) Write(data []byte) (RecordPosition, error) { s.writeMu.Lock() defer s.writeMu.Unlock() // 1. Calculate sizes entrySize := alignedSize(uint32(len(data))) offset := s.writeOffset // 2. Check segment capacity if offset + entrySize \u0026gt; s.maxSize { return RecordPosition{}, ErrSegmentFull } // 3. Build record in mmap buf := s.mmapData[offset : offset+entrySize] // Length first (so CRC covers it) binary.LittleEndian.PutUint32(buf[4:8], uint32(len(data))) // Copy data copy(buf[recordHeaderSize:], data) // Write trailer trailerOffset := recordHeaderSize + uint32(len(data)) binary.LittleEndian.PutUint64(buf[trailerOffset:], recordTrailer) // Zero padding for i := trailerOffset + 8; i \u0026lt; entrySize; i++ { buf[i] = 0 } // 4. Calculate and write CRC (LAST - commits the record) crc := crc32.Checksum(buf[4:entrySize], crcTable) binary.LittleEndian.PutUint32(buf[0:4], crc) // 5. Update header s.writeOffset = offset + entrySize s.entryCount++ s.header.LastModified = time.Now().UnixNano() s.writeHeaderCRC() // 6. Sync if configured if s.syncEveryWrite { if err := s.sync(); err != nil { return RecordPosition{}, err } } return RecordPosition{SegmentID: s.id, Offset: offset}, nil }Note the order: CRC is written last. This ensures that if we crash mid-write, the CRC will be invalid (or the trailer will be missing), and recovery will reject the partial record.\nLayer 7: Segment Lifecycle and Rotation# A single 16MB segment would fill up quickly. We manage multiple segments:\ndata/ ├── 000000001.wal (sealed, immutable) ├── 000000001.wal.idx (index file) ├── 000000002.wal (sealed, immutable) ├── 000000002.wal.idx ├── 000000003.wal (active, accepting writes) └── 000000003.wal.idxSegment States# create seal cleanup ┌────┐ ────────▶ ┌────────┐ ────────▶ ┌────────┐ ────────▶ deleted │ NEW│ │ ACTIVE │ │ SEALED │ └────┘ └────────┘ └────────┘ │ │ │ │ write here read-only (mutable) (immutable)const ( FlagActive = 1 \u0026lt;\u0026lt; 0 // Segment is open for writes FlagSealed = 1 \u0026lt;\u0026lt; 1 // Segment is closed, immutable ) func (s *Segment) IsSealed() bool { return s.header.Flags\u0026amp;FlagSealed != 0 }The Rotation Dance# When a segment fills up, we rotate atomically:\nfunc (w *WALog) rotateSegment() error { w.writeMu.Lock() defer w.writeMu.Unlock() current := w.currentSegment // 1. Seal the current segment current.header.Flags \u0026amp;^= FlagActive current.header.Flags |= FlagSealed current.writeHeaderCRC() // 2. Sync sealed segment (critical!) if err := current.sync(); err != nil { return fmt.Errorf(\u0026#34;sync sealed segment: %w\u0026#34;, err) } // 3. Build and persist index file if err := current.writeIndexFile(); err != nil { return fmt.Errorf(\u0026#34;write index: %w\u0026#34;, err) } // 4. Create new segment newID := current.id + 1 newSeg, err := createSegment(w.dir, newID, w.maxSegmentSize) if err != nil { return fmt.Errorf(\u0026#34;create segment: %w\u0026#34;, err) } // 5. Sync directory (makes new file visible) if err := w.dirSyncer.Sync(); err != nil { return fmt.Errorf(\u0026#34;sync dir: %w\u0026#34;, err) } // 6. Atomic pointer swap for readers w.updateSegmentSnapshot(newSeg) // 7. Notify listeners (for replication, monitoring) if w.onRotation != nil { w.onRotation(current.id, newSeg.id) } w.currentSegment = newSeg return nil }Why Directory Sync After Segment Creation?# Consider this crash scenario without directory sync:\n1. Create new segment file ✓ file exists in memory 2. Write segment header ✓ data in page cache 3. fsync segment file ✓ data on disk 4. [CRASH] 5. Recovery: \u0026#34;where\u0026#39;s segment 3?\u0026#34; - directory entry never persisted!With directory sync:\n1. Create new segment file ✓ 2. Write segment header ✓ 3. fsync segment file ✓ 4. fsync directory ✓ directory entry on disk 5. [CRASH] 6. Recovery: segment 3 exists and is valid Layer 8: Index Files for O(1) Lookups# Each sealed segment has a companion .idx file:\ntype IndexEntry struct { Offset uint32 // Record offset within segment Length uint32 // Record length Reserved uint64 // Future use (checksum, flags, etc.) } // 16 bytes per entryFor a segment with 10,000 records: 160KB index file.\nIndex File Format# ┌─────────────────────────────────────────────────┐ │ Index Header (64 bytes) │ │ ┌──────────┬──────────┬──────────┬───────────┐ │ │ │ Magic │ Version │ Count │ CRC32 │ │ │ │ \u0026#34;UIDX\u0026#34; │ 1 │ entries │ │ │ │ └──────────┴──────────┴──────────┴───────────┘ │ ├─────────────────────────────────────────────────┤ │ Entry 0: [Offset: 64 | Length: 128 | ...] │ ├─────────────────────────────────────────────────┤ │ Entry 1: [Offset: 200 | Length: 256 | ...] │ ├─────────────────────────────────────────────────┤ │ Entry 2: [Offset: 464 | Length: 64 | ...] │ ├─────────────────────────────────────────────────┤ │ ... │ └─────────────────────────────────────────────────┘O(1) Lookup# Given a Raft log index, finding the record is O(1):\nfunc (s *Segment) ReadAt(logIndex uint64) ([]byte, error) { // Log index → slot in this segment slot := logIndex - s.header.FirstLogIndex if slot \u0026gt;= uint64(s.entryCount) { return nil, ErrNotFound } // Read from index (mmap\u0026#39;d or loaded) entry := s.index[slot] // Read record directly offset := entry.Offset length := entry.Length return s.mmapData[offset+recordHeaderSize : offset+recordHeaderSize+length], nil }No binary search. No scanning. Direct offset lookup.\nLazy Index Loading# For memory efficiency, we don\u0026rsquo;t load all indexes at startup:\ntype Segment struct { // ... indexLoaded atomic.Bool indexMu sync.RWMutex index []IndexEntry } func (s *Segment) ensureIndexLoaded() error { if s.indexLoaded.Load() { return nil } s.indexMu.Lock() defer s.indexMu.Unlock() // Double-check after lock if s.indexLoaded.Load() { return nil } // Load from .idx file idx, err := loadIndexFile(s.indexPath) if err != nil { // Fallback: rebuild from segment idx, err = s.rebuildIndex() if err != nil { return err } } s.index = idx s.indexLoaded.Store(true) return nil }Old segments that haven\u0026rsquo;t been accessed don\u0026rsquo;t consume memory for their indexes.\nLayer 9: Concurrent Readers Without Blocking Writers# Replication means many followers reading simultaneously while the leader writes. We can\u0026rsquo;t have readers blocking writers.\nThe Snapshot Pattern# type WALog struct { // Writers use this (protected by writeMu) currentSegment *Segment segments map[SegmentID]*Segment writeMu sync.Mutex // Readers use this (lock-free via atomic) segmentSnapshot atomic.Pointer[[]*Segment] }Writers update the snapshot atomically during rotation:\nfunc (w *WALog) updateSegmentSnapshot(newSeg *Segment) { // Build new slice segs := make([]*Segment, 0, len(w.segments)+1) for _, s := range w.segments { segs = append(segs, s) } segs = append(segs, newSeg) sort.Slice(segs, func(i, j int) bool { return segs[i].id \u0026lt; segs[j].id }) // Atomic swap w.segmentSnapshot.Store(\u0026amp;segs) }Readers get a consistent view without locks:\nfunc (w *WALog) Segments() []*Segment { return *w.segmentSnapshot.Load() }Sharded Reader Tracking# But we need to track active readers for safe deletion. A global lock would kill performance:\n// Bad: global lock contention type readerTracker struct { mu sync.Mutex readers map[uint64]struct{} }We shard by reader ID:\n// Good: sharded locks type readerTracker struct { shards [32]readerShard } type readerShard struct { mu sync.Mutex readers map[uint64]struct{} } func (rt *readerTracker) Add(id uint64) { shard := \u0026amp;rt.shards[id%32] shard.mu.Lock() shard.readers[id] = struct{}{} shard.mu.Unlock() } func (rt *readerTracker) Remove(id uint64) { shard := \u0026amp;rt.shards[id%32] shard.mu.Lock() delete(shard.readers, id) shard.mu.Unlock() } func (rt *readerTracker) Count() int { total := 0 for i := range rt.shards { rt.shards[i].mu.Lock() total += len(rt.shards[i].readers) rt.shards[i].mu.Unlock() } return total }Benchmark: Sharded vs sync.Map# BenchmarkReaderTracker/SyncMap-8 5,200,000 ops/sec BenchmarkReaderTracker/Sharded32-8 13,600,000 ops/sec (2.6x faster)sync.Map is optimized for read-heavy, write-rare workloads. Reader tracking is write-heavy (add/remove on every request), so sharding wins.\nLayer 10: Zero-Copy Reads for Replication# When a follower requests records, we want to avoid copying data:\n// Reader for streaming replication type WALReader struct { wal *WALog segmentID SegmentID offset uint32 readerID uint64 } func (r *WALReader) Next() ([]byte, RecordPosition, error) { seg := r.wal.getSegment(r.segmentID) if seg == nil { return nil, RecordPosition{}, ErrSegmentDeleted } // Zero-copy: return slice of mmap\u0026#39;d data data := seg.mmapData[r.offset+recordHeaderSize : r.offset+recordHeaderSize+length] pos := RecordPosition{ SegmentID: r.segmentID, Offset: r.offset, } r.offset += alignedSize(length) // NOTE: Caller must copy if they need data beyond this call return data, pos, nil }The Lifetime Contract# The returned slice is valid only until:\nThe reader advances past this segment The segment is deleted The WAL is closed For network transmission, this is perfect—we can write directly to a socket buffer:\nfunc (s *ReplicationServer) StreamRecords(w io.Writer) error { reader := s.wal.NewReader(startPos) defer reader.Close() for { data, pos, err := reader.Next() if err == io.EOF { return nil } if err != nil { return err } // Write directly from mmap to network // No intermediate copies! if err := writeFrame(w, pos, data); err != nil { return err } } } Layer 11: Safe Segment Deletion# Deleting segments while readers are active is dangerous. We use reference counting:\ntype Segment struct { // ... refCount atomic.Int32 markedForDeletion atomic.Bool } func (s *Segment) Acquire() bool { for { count := s.refCount.Load() if count \u0026lt; 0 { // Already being deleted return false } if s.refCount.CompareAndSwap(count, count+1) { return true } } } func (s *Segment) Release() { if s.refCount.Add(-1) == 0 \u0026amp;\u0026amp; s.markedForDeletion.Load() { // Last reader, trigger deletion s.deleteCh \u0026lt;- struct{}{} } }Deletion Predicate for Raft# Raft has specific rules about when log entries can be deleted (only after snapshotting):\nfunc (l *LogStore) DeletionPredicate(getCompactedIndex func() uint64) DeletionPredicate { return func(segID SegmentID) bool { seg := l.wal.getSegment(segID) if seg == nil { return false } // Can\u0026#39;t delete active segment if seg.id == l.wal.CurrentSegment().id { return false } // Segment\u0026#39;s last log index must be \u0026lt;= compacted index lastLogIndex := seg.header.FirstLogIndex + uint64(seg.entryCount) - 1 compactedIndex := getCompactedIndex() return lastLogIndex \u0026lt;= compactedIndex } }Usage:\n// In Raft snapshot callback predicate := logStore.DeletionPredicate(func() uint64 { return raft.LastSnapshotIndex() }) for _, seg := range logStore.WAL().Segments() { if predicate(seg.ID()) { seg.MarkForDeletion() } } Layer 12: Raft LogStore Integration# The base walfs package is generic. raftwalfs adapts it for HashiCorp Raft:\ntype LogStore struct { wal *walfs.WALog codec Codec index *ShardedIndex // logIndex → position firstIndex atomic.Uint64 lastIndex atomic.Uint64 committedIndex atomic.Uint64 } // Implement raft.LogStore interface var _ raft.LogStore = (*LogStore)(nil) var _ raft.MonotonicLogStore = (*LogStore)(nil)Handling Raft\u0026rsquo;s Truncation# Raft requires truncation on leader changes. If a new leader has different logs, followers must truncate:\nfunc (l *LogStore) DeleteRange(min, max uint64) error { l.mu.Lock() defer l.mu.Unlock() first := l.firstIndex.Load() last := l.lastIndex.Load() // SUFFIX deletion (log truncation on leader change) if max \u0026gt;= last \u0026amp;\u0026amp; min \u0026gt; first { l.lastIndex.Store(min - 1) // Truncate WAL at this point if err := l.wal.Truncate(min - 1); err != nil { return fmt.Errorf(\u0026#34;truncate wal: %w\u0026#34;, err) } // Remove from in-memory index l.index.DeleteRange(min, max) return nil } // PREFIX deletion (log compaction after snapshot) if min == first { l.index.DeleteRange(min, max) // Update first index newFirst, _, ok := l.index.GetFirstLast() if ok { l.firstIndex.Store(newFirst) } else { l.firstIndex.Store(0) l.lastIndex.Store(0) } } return nil }The Monotonic Guarantee# We implement raft.MonotonicLogStore:\nfunc (l *LogStore) IsMonotonic() bool { return true }This tells Raft our log indices only increase (within a term). Raft can skip certain consistency checks, improving performance.\nLayer 13: The Binary Codec# Raft log entries need efficient serialization:\ntype Codec interface { Encode(log *raft.Log) ([]byte, error) Decode(data []byte) (raft.Log, error) }Our binary codec:\n┌────────────────────────────────────────────────────────────┐ │ Term │ Index │ Type │ AppendedAt │ DataLen │ Data │ │ 8 bytes │ 8 bytes │ 1 b │ 8 bytes │ varint │ N bytes │ ├────────────────────────────────────────────────────────────┤ │ Extensions (optional) │ │ ExtLen │ ExtData │ │ varint │ M bytes │ └────────────────────────────────────────────────────────────┘Varint for Space Efficiency# Data length uses varint encoding:\nValues 0-127: 1 byte Values 128-16383: 2 bytes And so on\u0026hellip; Most records are small, so this saves significant space:\nfunc encodeVarint(buf []byte, v uint64) int { i := 0 for v \u0026gt;= 0x80 { buf[i] = byte(v) | 0x80 v \u0026gt;\u0026gt;= 7 i++ } buf[i] = byte(v) return i + 1 }LSN Injection# UnisonDB needs an LSN (Log Sequence Number) in the data for replication. We use a codec mutator:\ntype DataMutator func(data []byte, lsn uint64) []byte codec := BinaryCodecV1{ Mutator: func(data []byte, lsn uint64) []byte { // Inject LSN at known offset in FlatBuffer binary.LittleEndian.PutUint64(data[lsnOffset:], lsn) return data }, }This avoids deserialize → modify → reserialize.\nPerformance: Batching for Throughput# Single-record syncs are slow (~10K ops/sec with fsync). For throughput, we batch:\nfunc (w *WALog) WriteBatch(records [][]byte) ([]RecordPosition, error) { w.writeMu.Lock() defer w.writeMu.Unlock() positions := make([]RecordPosition, len(records)) for i, record := range records { pos, err := w.currentSegment.writeNoSync(record) if err != nil { // Rollback partial batch return nil, err } positions[i] = pos } // Single sync for entire batch if err := w.currentSegment.sync(); err != nil { return nil, err } return positions, nil }Batching amortizes the sync cost. With 100 records per batch, we can sustain 500K+ records/sec while maintaining full durability.\nWar Stories: Failures We\u0026rsquo;ve Seen# 1. The Silent Corruption# Symptom: Followers diverged from leader after a few days.\nRoot Cause: A follower had a bad RAM stick. Bit flips during replication corrupted records. Without CRC, the bad data was accepted and persisted.\nFix: CRC verification on every read (not just recovery). We now detect corruption immediately, not days later.\n2. The Phantom Segment# Symptom: After crash recovery, segment N+1 existed but segment N was empty.\nRoot Cause: Segment rotation created N+1, but crashed before N was sealed. On recovery, N had FlagActive but no valid records. We tried to continue writing to N, corrupting N+1\u0026rsquo;s log sequence.\nFix: On recovery, re-seal any segment that isn\u0026rsquo;t the latest. Only the newest segment can be active.\nfunc (w *WALog) recoverSegments() error { segs := listSegments(w.dir) sort.Sort(segs) for i, seg := range segs { isLast := i == len(segs)-1 if !isLast \u0026amp;\u0026amp; seg.IsActive() { // Non-latest segment still marked active = crash during rotation seg.Seal() seg.Sync() } } return nil }3. The Slow fsync# Symptom: Write latency spikes every few seconds.\nRoot Cause: ext4 journaling was batching our fsyncs with other I/O. A background process doing large writes caused our small fsyncs to wait.\nFix: Use sync_file_range() for data + fsync() for metadata, or mount with data=ordered instead of data=journal.\n4. The mmap Trap# Symptom: Writes succeeded but data was lost on crash.\nRoot Cause: We called mmap.Flush() thinking it persisted data. On Linux, msync(MS_ASYNC) only marks pages dirty—it doesn\u0026rsquo;t wait for writeback.\nFix: Always follow mmap.Flush() with fd.Sync():\n// WRONG: data may not be on disk mmapData.Flush() // RIGHT: data is definitely on disk mmapData.Flush() // Mark dirty pages fd.Sync() // Force writeback5. The Index Corruption# Symptom: O(1) lookups returned wrong records after recovery.\nRoot Cause: Index file was written but not synced before crash. The index had stale offsets pointing to wrong locations in the segment.\nFix: Sync index file before updating any pointers to it:\nfunc (s *Segment) writeIndexFile() error { f, _ := os.Create(s.indexPath) // Write index entries for _, entry := range s.index { binary.Write(f, binary.LittleEndian, entry) } // CRITICAL: sync before anyone uses this file if err := f.Sync(); err != nil { return err } // NOW sync directory to make file visible return s.dirSyncer.Sync() } Testing Corruption Scenarios# We don\u0026rsquo;t just hope our corruption handling works—we test it:\nfunc TestTornWriteRecovery(t *testing.T) { wal := createTestWAL(t) // Write some records for i := 0; i \u0026lt; 100; i++ { wal.Write([]byte(fmt.Sprintf(\u0026#34;record-%d\u0026#34;, i))) } // Simulate torn write: corrupt the last record\u0026#39;s trailer seg := wal.CurrentSegment() lastOffset := seg.WriteOffset() - 24 // Back up to trailer copy(seg.mmapData[lastOffset:], []byte{0, 0, 0, 0, 0, 0, 0, 0}) // Close without sync (simulating crash) wal.CloseNoSync() // Reopen and recover wal2, err := OpenWAL(wal.Dir()) require.NoError(t, err) // Should have 99 records, not 100 count := 0 wal2.Iterate(func(pos RecordPosition, data []byte) bool { count++ return true }) assert.Equal(t, 99, count) }We also use dm-flakey (Linux device-mapper target) to simulate:\nPower failures at arbitrary points Sector write failures Bit flips in specific regions Benchmarks# Single-Record Writes (fsync per record)# BenchmarkWrite/NoSync-8 2,100,000 ops/sec 476 ns/op BenchmarkWrite/MsyncOnly-8 180,000 ops/sec 5.5 μs/op BenchmarkWrite/FullSync-8 12,000 ops/sec 83 μs/opBatch Writes (fsync per batch)# BenchmarkWriteBatch/10records-8 120,000 batches/sec 8.3 μs/op BenchmarkWriteBatch/100records-8 45,000 batches/sec 22 μs/op BenchmarkWriteBatch/1000records-8 12,000 batches/sec 83 μs/opEffective throughput with 100-record batches: 4.5M records/sec with full durability.\nRead Performance# BenchmarkRead/Sequential-8 8,500,000 ops/sec 117 ns/op BenchmarkRead/Random-8 6,200,000 ops/sec 161 ns/op BenchmarkRead/Concurrent16-8 42,000,000 ops/sec 24 ns/opConcurrent reads scale linearly thanks to the snapshot pattern.\nRecovery Time# Segment Size Records Recovery Time 16 MB 10,000 12 ms 16 MB 50,000 45 ms 16 MB 100,000 89 msRecovery is I/O-bound, scanning at ~180 MB/sec.\nComparison with Other WALs# Feature UnisonDB WAL etcd WAL RocksDB WAL BadgerDB Storage mmap pwrite pwrite mmap Torn write detection CRC + Trailer CRC only CRC only CRC + Magic Index Per-segment file None None In-memory Concurrent readers Snapshot pattern Lock Lock MVCC Zero-copy reads Yes No No Yes Segment size Configurable 64 MB fixed Configurable 128 MB Directory sync Yes Yes Optional Yes Configuration Recommendations# High Durability (Financial, Healthcare)# wal, _ := walfs.Open(dir, walfs.WithMSyncEveryWrite(true), walfs.WithMaxSegmentSize(16*1024*1024), )Latency: ~100 μs/write. No data loss on power failure.\nHigh Throughput (Analytics, Logs)# wal, _ := walfs.Open(dir, walfs.WithBytesPerSync(1024*1024), // Sync every 1 MB walfs.WithMaxSegmentSize(64*1024*1024), )Latency: ~1 μs/write. May lose up to 1 MB on crash.\nBalanced (Most Applications)# wal, _ := walfs.Open(dir, walfs.WithBytesPerSync(64*1024), // Sync every 64 KB walfs.WithMaxSegmentSize(16*1024*1024), )Latency: ~10 μs/write. May lose up to 64 KB on crash.\nLessons Learned# CRC alone isn\u0026rsquo;t enough - You need to detect incomplete writes too fsync isn\u0026rsquo;t enough - You need directory sync for metadata mmap is tricky - msync semantics vary by OS; always fsync the fd Alignment matters - 8-byte alignment reduces torn write risk Be conservative in recovery - Stop at first corruption, don\u0026rsquo;t guess Test failure modes - If you haven\u0026rsquo;t tested it, it doesn\u0026rsquo;t work Future Work# Checksummed Pages: Per-4KB checksums for finer-grained corruption detection Compression: LZ4/Zstd per-record or per-segment Encryption: AES-GCM with per-segment keys Remote Storage: S3-compatible segment archival Tiered Storage: Hot segments on NVMe, cold on HDD Conclusion# A WAL is deceptively simple: append bytes, sync, done. The complexity hides in the failure modes:\nWhat if the write is torn? What if the sync lies? What if a bit flips? What if the directory entry isn\u0026rsquo;t persisted? Each layer of our design—CRC, trailer, alignment, header checksum, directory sync, conservative recovery—addresses a specific failure we\u0026rsquo;ve either experienced or studied in others\u0026rsquo; post-mortems.\nThe 0xDEADBEEFFEEDFACE trailer is a perfect metaphor: it looks like a joke, but it\u0026rsquo;s deadly serious. In distributed systems, the boundary between working and broken is measured in these small details.\nBuild your WAL like you don\u0026rsquo;t trust anything—because you shouldn\u0026rsquo;t.\nThe UnisonDB WAL is open source. Star us on GitHub if this saved you from learning these lessons the hard way.\n"},{"id":11,"href":"/docs/api/http-api/","title":"HTTP API Reference","section":"API Reference","content":"HTTP API Reference# Complete reference for UnisonDB\u0026rsquo;s HTTP REST API.\nBase URL# http://localhost:4000/api/v1/{namespace}Data Encoding# All binary values must be base64-encoded :\n# Encode a value echo -n \u0026#34;hello world\u0026#34; | base64 # Output: aGVsbG8gd29ybGQ= # Use in request curl -X PUT http://localhost:4000/api/v1/default/kv/greeting \\ -d \u0026#39;{\u0026#34;value\u0026#34;:\u0026#34;aGVsbG8gd29ybGQ=\u0026#34;}\u0026#39;Request Size Limits Restrictions# Maximum Request Size# To keep UnisonDB\u0026rsquo;s HTTP layer efficient, safe, and predictable, each HTTP request is limited to a maximum size of 1 MB.\nThis limit applies to the entire request body, including:\nJSON payload Base64-encoded values Any metadata sent in the body If your workload needs to store data larger than 1 MB, you should not push it as a single HTTP write. Instead, use UnisonDB\u0026rsquo;s transactional (Txn) write path , which is designed to handle larger logical operations safely and atomically.\nKey-Value Operations# Put KV# Store a key-value pair.\nRequest:\nPUT /api/v1/{namespace}/kv/{key} Content-Type: application/json { \u0026#34;value\u0026#34;: \u0026#34;base64-encoded-value\u0026#34; }Example:\ncurl -X PUT http://localhost:4000/api/v1/default/kv/user:123 \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;value\u0026#34;: \u0026#34;eyJuYW1lIjoiSm9obiIsImFnZSI6MzB9\u0026#34; }\u0026#39;Response (200 OK):\n{ \u0026#34;success\u0026#34;: true }Errors:\n400 Bad Request: Invalid base64 encoding 404 Not Found: Namespace not found 500 Internal Server Error: Engine error Get KV# Retrieve a value by key.\nRequest:\nGET /api/v1/{namespace}/kv/{key}Example:\ncurl http://localhost:4000/api/v1/default/kv/user:123Response (200 OK):\n{ \u0026#34;value\u0026#34;: \u0026#34;eyJuYW1lIjoiSm9obiIsImFnZSI6MzB9\u0026#34;, \u0026#34;found\u0026#34;: true }Response (404 Not Found):\n{ \u0026#34;value\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;found\u0026#34;: false } Delete KV# Delete a key.\nRequest:\nDELETE /api/v1/{namespace}/kv/{key}Example:\ncurl -X DELETE http://localhost:4000/api/v1/default/kv/user:123Response (200 OK):\n{ \u0026#34;success\u0026#34;: true } Batch KV Operations# Perform multiple operations in one request.\nBatch Put# Request:\nPOST /api/v1/{namespace}/kv/batch Content-Type: application/json { \u0026#34;operation\u0026#34;: \u0026#34;put\u0026#34;, \u0026#34;items\u0026#34;: [ {\u0026#34;key\u0026#34;: \u0026#34;key1\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;dmFsdWUx\u0026#34;}, {\u0026#34;key\u0026#34;: \u0026#34;key2\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;dmFsdWUy\u0026#34;} ] }Example:\ncurl -X POST http://localhost:4000/api/v1/default/kv/batch \\ -d \u0026#39;{ \u0026#34;operation\u0026#34;: \u0026#34;put\u0026#34;, \u0026#34;items\u0026#34;: [ {\u0026#34;key\u0026#34;: \u0026#34;user:1\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;dXNlcjE=\u0026#34;}, {\u0026#34;key\u0026#34;: \u0026#34;user:2\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;dXNlcjI=\u0026#34;} ] }\u0026#39;Response (200 OK):\n{ \u0026#34;success\u0026#34;: true, \u0026#34;processed\u0026#34;: 2 }Batch Delete# Request:\nPOST /api/v1/{namespace}/kv/batch Content-Type: application/json { \u0026#34;operation\u0026#34;: \u0026#34;delete\u0026#34;, \u0026#34;keys\u0026#34;: [\u0026#34;key1\u0026#34;, \u0026#34;key2\u0026#34;] }Response (200 OK):\n{ \u0026#34;success\u0026#34;: true, \u0026#34;processed\u0026#34;: 2 } Wide-Column Operations# Put Row# Store a row with multiple columns.\nRequest:\nPUT /api/v1/{namespace}/row/{rowKey} Content-Type: application/json { \u0026#34;columns\u0026#34;: { \u0026#34;column1\u0026#34;: \u0026#34;base64-value1\u0026#34;, \u0026#34;column2\u0026#34;: \u0026#34;base64-value2\u0026#34; } }Example:\ncurl -X PUT http://localhost:4000/api/v1/default/row/user:john \\ -d \u0026#39;{ \u0026#34;columns\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Sm9obiBEb2U=\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;am9obkBleGFtcGxlLmNvbQ==\u0026#34;, \u0026#34;age\u0026#34;: \u0026#34;MzA=\u0026#34; } }\u0026#39;Response (200 OK):\n{ \u0026#34;success\u0026#34;: true } Get Row# Retrieve all columns for a row.\nRequest:\nGET /api/v1/{namespace}/row/{rowKey}Example:\ncurl http://localhost:4000/api/v1/default/row/user:johnResponse (200 OK):\n{ \u0026#34;rowKey\u0026#34;: \u0026#34;user:john\u0026#34;, \u0026#34;columns\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Sm9obiBEb2U=\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;am9obkBleGFtcGxlLmNvbQ==\u0026#34;, \u0026#34;age\u0026#34;: \u0026#34;MzA=\u0026#34; }, \u0026#34;found\u0026#34;: true } Get Row Columns# Retrieve specific columns only.\nRequest:\nGET /api/v1/{namespace}/row/{rowKey}?columns=col1,col2Example:\ncurl \u0026#34;http://localhost:4000/api/v1/default/row/user:john?columns=name,email\u0026#34;Response (200 OK):\n{ \u0026#34;rowKey\u0026#34;: \u0026#34;user:john\u0026#34;, \u0026#34;columns\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Sm9obiBEb2U=\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;am9obkBleGFtcGxlLmNvbQ==\u0026#34; }, \u0026#34;found\u0026#34;: true } Delete Row# Delete an entire row.\nRequest:\nDELETE /api/v1/{namespace}/row/{rowKey}Example:\ncurl -X DELETE http://localhost:4000/api/v1/default/row/user:johnResponse (200 OK):\n{ \u0026#34;success\u0026#34;: true } Delete Row Columns# Delete specific columns from a row.\nRequest:\nDELETE /api/v1/{namespace}/row/{rowKey}/columns?columns=col1,col2Example:\ncurl -X DELETE \u0026#34;http://localhost:4000/api/v1/default/row/user:john/columns?columns=age,city\u0026#34;Response (200 OK):\n{ \u0026#34;success\u0026#34;: true } Batch Row Operations# Batch Put Rows# Request:\nPOST /api/v1/{namespace}/row/batch Content-Type: application/json { \u0026#34;operation\u0026#34;: \u0026#34;put\u0026#34;, \u0026#34;rows\u0026#34;: [ { \u0026#34;rowKey\u0026#34;: \u0026#34;user:1\u0026#34;, \u0026#34;columns\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;QWxpY2U=\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;YWxpY2VAZXhhbXBsZS5jb20=\u0026#34; } } ] }Response (200 OK):\n{ \u0026#34;success\u0026#34;: true, \u0026#34;processed\u0026#34;: 1 } Large Object (LOB) Operations# Put LOB# Upload a large binary object. Check Transaction API.\nGet LOB# Download a large binary object.\nRequest:\nGET /api/v1/{namespace}/lob?key={key}Example:\ncurl \u0026#34;http://localhost:4000/api/v1/default/lob?key=file:doc.pdf\u0026#34; \\ --output document.pdfResponse: Binary data stream\nTransaction Operations# Transactions allow atomic operations across multiple keys.\nTransaction Lifecycle# 1. BEGIN → Get transaction ID 2. APPEND → Add operations (multiple times) 3. COMMIT → Apply atomically OR 3. ABORT → Cancel transactionTransaction Type Restrictions# Transactions are bound to the entryType specified during BEGIN. Once opened, a transaction can only accept operations matching its type:\nentryType Allowed Operations Endpoint kv Key-Value only POST /tx/{txnId}/kv row Wide-Column only POST /tx/{txnId}/row lob Large Objects only POST /tx/{txnId}/lob Begin Transaction# Start a new transaction.\nRequest:\nPOST /api/v1/{namespace}/tx/begin Content-Type: application/json { \u0026#34;operation\u0026#34;: \u0026#34;put\u0026#34;, \u0026#34;entryType\u0026#34;: \u0026#34;kv\u0026#34; }Parameters:\noperation: \u0026quot;put\u0026quot;, \u0026quot;update\u0026quot;, or \u0026quot;delete\u0026quot; entryType: \u0026quot;kv\u0026quot;, \u0026quot;row\u0026quot;, or \u0026quot;lob\u0026quot; Example:\ncurl -X POST http://localhost:4000/api/v1/default/tx/begin \\ -d \u0026#39;{ \u0026#34;operation\u0026#34;: \u0026#34;put\u0026#34;, \u0026#34;entryType\u0026#34;: \u0026#34;kv\u0026#34; }\u0026#39;Response (200 OK):\n{ \u0026#34;txnId\u0026#34;: \u0026#34;2a3b4c5d6e7f8g9h0i1j2k3l4m5n6o7p\u0026#34;, \u0026#34;success\u0026#34;: true }Save the txnId - you\u0026rsquo;ll need it for subsequent requests!\nAppend KV to Transaction# Add a key-value operation to the transaction.\nRequest:\nPOST /api/v1/{namespace}/tx/{txnId}/kv Content-Type: application/json { \u0026#34;key\u0026#34;: \u0026#34;mykey\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;bXl2YWx1ZQ==\u0026#34; }Example:\n# Use the txnId from BEGIN response curl -X POST http://localhost:4000/api/v1/default/tx/2a3b4c.../kv \\ -d \u0026#39;{ \u0026#34;key\u0026#34;: \u0026#34;account:alice\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;MTAwMA==\u0026#34; }\u0026#39;Response (200 OK):\n{ \u0026#34;success\u0026#34;: true }Call this endpoint multiple times to add multiple operations to the same transaction.\nAppend Row to Transaction# Add a row operation to the transaction.\nRequest:\nPOST /api/v1/{namespace}/tx/{txnId}/row Content-Type: application/json { \u0026#34;rowKey\u0026#34;: \u0026#34;user:1\u0026#34;, \u0026#34;columns\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;QWxpY2U=\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;YWN0aXZl\u0026#34; } }Example:\ncurl -X POST http://localhost:4000/api/v1/default/tx/{txnId}/row \\ -d \u0026#39;{ \u0026#34;rowKey\u0026#34;: \u0026#34;user:charlie\u0026#34;, \u0026#34;columns\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Q2hhcmxpZQ==\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;Y2hhcmxpZUBleGFtcGxlLmNvbQ==\u0026#34; } }\u0026#39;Response (200 OK):\n{ \u0026#34;success\u0026#34;: true } Append LOB to Transaction# Add a large object to the transaction.\nRequest:\nPOST /api/v1/{namespace}/tx/{txnId}/lob?key={key} Content-Type: application/octet-stream \u0026lt;binary data\u0026gt;Example:\ncurl -X POST \u0026#34;http://localhost:4000/api/v1/default/tx/{txnId}/lob?key=file:backup.tar.gz\u0026#34; \\ --data-binary @backup.tar.gzResponse (200 OK):\n{ \u0026#34;success\u0026#34;: true }Handling Large Objects (LOB \u0026gt; 1MB)# For files or data larger than 1MB, use LOB transactions with chunking:\nExample: Upload a 5MB File# #!/bin/bash FILE=\u0026#34;large-file.bin\u0026#34; KEY=\u0026#34;files:backup-20250108.tar.gz\u0026#34; CHUNK_SIZE=1048576 # 1MB chunks # 1. Begin LOB transaction RESPONSE=$(curl -s -X POST http://localhost:4000/api/v1/default/tx/begin \\ -d \u0026#39;{\u0026#34;operation\u0026#34;:\u0026#34;put\u0026#34;,\u0026#34;entryType\u0026#34;:\u0026#34;lob\u0026#34;}\u0026#39;) TXN_ID=$(echo $RESPONSE | jq -r \u0026#39;.txnId\u0026#39;) # 2. Split file into 1MB chunks and upload split -b $CHUNK_SIZE \u0026#34;$FILE\u0026#34; /tmp/chunk_ for CHUNK in /tmp/chunk_*; do echo \u0026#34;Uploading $CHUNK...\u0026#34; curl -X POST \u0026#34;http://localhost:4000/api/v1/default/tx/$TXN_ID/lob?key=$KEY\u0026#34; \\ --data-binary @\u0026#34;$CHUNK\u0026#34; done # 3. Commit transaction curl -X POST http://localhost:4000/api/v1/default/tx/$TXN_ID/commit # 4. Cleanup rm /tmp/chunk_* echo \u0026#34;Large file uploaded successfully!\u0026#34; Commit Transaction# Apply all operations atomically.\nRequest:\nPOST /api/v1/{namespace}/tx/{txnId}/commitExample:\ncurl -X POST http://localhost:4000/api/v1/default/tx/2a3b4c.../commitResponse (200 OK):\n{ \u0026#34;success\u0026#34;: true }After commit:\nAll operations are applied atomically Transaction ID is no longer valid Data is durable and replicated Abort Transaction# Cancel the transaction without applying changes.\nRequest:\nPOST /api/v1/{namespace}/tx/{txnId}/abortExample:\ncurl -X POST http://localhost:4000/api/v1/default/tx/2a3b4c.../abortResponse (200 OK):\n{ \u0026#34;success\u0026#34;: true }After abort:\nNo operations are applied Transaction ID is no longer valid Metadata Operations# Get Current Offset# Get the current WAL position.\nRequest:\nGET /api/v1/{namespace}/offsetExample:\ncurl http://localhost:4000/api/v1/default/offsetResponse (200 OK):\n{ \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;segmentId\u0026#34;: 5, \u0026#34;offset\u0026#34;: 12345 } Get Engine Statistics# Get engine performance statistics.\nRequest:\nGET /api/v1/{namespace}/statsExample:\ncurl http://localhost:4000/api/v1/default/statsResponse (200 OK):\n{ \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;opsReceived\u0026#34;: 15234, \u0026#34;opsFlushed\u0026#34;: 15100, \u0026#34;currentSegment\u0026#34;: 5, \u0026#34;currentOffset\u0026#34;: 12345, \u0026#34;lastFlushTime\u0026#34;: \u0026#34;2024-01-15T10:30:45Z\u0026#34; } Get Checkpoint# Get the last checkpoint position.\nRequest:\nGET /api/v1/{namespace}/checkpointExample:\ncurl http://localhost:4000/api/v1/default/checkpointResponse (200 OK):\n{ \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;recordProcessed\u0026#34;: 15000, \u0026#34;segmentId\u0026#34;: 5, \u0026#34;offset\u0026#34;: 12000 } Backup Operations# UnisonDB provides APIs for creating durable backups of both WAL segments and B-Tree snapshots. All backup paths must be relative to the server\u0026rsquo;s backup root (\u0026lt;dataDir\u0026gt;/backups/{namespace}).\nWAL Segment Backup# Create an incremental backup by copying sealed WAL segments.\nRequest:\nPOST /api/v1/{namespace}/wal/backup Content-Type: application/json { \u0026#34;afterSegmentId\u0026#34;: 42, \u0026#34;backupDir\u0026#34;: \u0026#34;wal/customer-a\u0026#34; }Parameters:\nafterSegmentId (optional): Only copy segments with IDs greater than this value. Omit or set to 0 to copy all sealed segments. backupDir (required): Relative path within the backup root. Absolute paths or .. traversal are rejected. Example:\n# Backup all segments after segment 100 curl -X POST http://localhost:4000/api/v1/default/wal/backup \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;afterSegmentId\u0026#34;: 100, \u0026#34;backupDir\u0026#34;: \u0026#34;wal/daily\u0026#34; }\u0026#39;Response (200 OK):\n{ \u0026#34;backups\u0026#34;: [ { \u0026#34;segmentId\u0026#34;: 101, \u0026#34;path\u0026#34;: \u0026#34;/var/unison/data/backups/default/wal/daily/000000101.wal\u0026#34; }, { \u0026#34;segmentId\u0026#34;: 102, \u0026#34;path\u0026#34;: \u0026#34;/var/unison/data/backups/default/wal/daily/000000102.wal\u0026#34; } ] }Use Cases:\nIncremental backups for point-in-time recovery Compliance archival of transaction logs Shipping WAL segments to remote storage Errors:\n400 Bad Request: Invalid path (absolute or contains ..) 404 Not Found: Namespace not found 500 Internal Server Error: Filesystem error, permission denied B-Tree Snapshot Backup# Create a full snapshot of the B-Tree store.\nRequest:\nPOST /api/v1/{namespace}/btree/backup Content-Type: application/json { \u0026#34;path\u0026#34;: \u0026#34;snapshots/users-20250108.snapshot\u0026#34; }Parameters:\npath (required): Relative path within the backup root. The server writes to {path}.tmp, fsyncs, then atomically renames. Example:\n# Create a snapshot with today\u0026#39;s date curl -X POST http://localhost:4000/api/v1/default/btree/backup \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;path\u0026#34;: \u0026#34;snapshots/backup-\u0026#39;$(date +%Y%m%d)\u0026#39;.db\u0026#34; }\u0026#39;Response (200 OK):\n{ \u0026#34;path\u0026#34;: \u0026#34;/var/unison/data/backups/default/snapshots/backup-20250108.db\u0026#34;, \u0026#34;bytes\u0026#34;: 73400320 }Use Cases:\nFull database backups for disaster recovery Creating read-only replicas for analytics Migrating data to new servers Errors:\n400 Bad Request: Invalid path (absolute or contains ..) 404 Not Found: Namespace not found 500 Internal Server Error: Filesystem error, disk full Backup Workflow Example# Complete backup automation script:\n#!/bin/bash # Daily backup script NAMESPACE=\u0026#34;users\u0026#34; DATE=$(date +%Y%m%d) STATE_FILE=\u0026#34;/var/lib/unison/wal-state.json\u0026#34; # 1. B-Tree snapshot (daily) echo \u0026#34;Creating B-Tree snapshot...\u0026#34; curl -X POST http://localhost:4000/api/v1/$NAMESPACE/btree/backup \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#34;{\\\u0026#34;path\\\u0026#34;: \\\u0026#34;snapshots/btree-$DATE.db\\\u0026#34;}\u0026#34; # 2. WAL incremental echo \u0026#34;Backing up WAL segments...\u0026#34; LAST_SEGMENT=$(jq -r \u0026#39;.lastSegmentId // 0\u0026#39; \u0026#34;$STATE_FILE\u0026#34; 2\u0026gt;/dev/null || echo 0) RESPONSE=$(curl -s -X POST http://localhost:4000/api/v1/$NAMESPACE/wal/backup \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#34;{\\\u0026#34;afterSegmentId\\\u0026#34;: $LAST_SEGMENT, \\\u0026#34;backupDir\\\u0026#34;: \\\u0026#34;wal/$DATE\\\u0026#34;}\u0026#34;) # 3. Update state NEW_LAST=$(echo \u0026#34;$RESPONSE\u0026#34; | jq -r \u0026#39;.backups[-1].segmentId // 0\u0026#39;) if [ \u0026#34;$NEW_LAST\u0026#34; != \u0026#34;0\u0026#34; ]; then echo \u0026#34;{\\\u0026#34;lastSegmentId\\\u0026#34;: $NEW_LAST, \\\u0026#34;timestamp\\\u0026#34;: \\\u0026#34;$(date -Iseconds)\\\u0026#34;}\u0026#34; \u0026gt; \u0026#34;$STATE_FILE\u0026#34; fi # 4. Compress and upload to S3 (optional) BACKUP_ROOT=\u0026#34;/var/unison/data/backups/$NAMESPACE\u0026#34; tar -czf \u0026#34;/tmp/backup-$DATE.tar.gz\u0026#34; -C \u0026#34;$BACKUP_ROOT\u0026#34; . aws s3 cp \u0026#34;/tmp/backup-$DATE.tar.gz\u0026#34; \u0026#34;s3://backups/unison/$NAMESPACE/\u0026#34; echo \u0026#34;Backup completed successfully\u0026#34;For detailed backup strategies, automation, and restore procedures, see the Backup and Restore Guide .\nError Responses# All errors follow this format:\n{ \u0026#34;error\u0026#34;: \u0026#34;error message description\u0026#34; }HTTP Status Codes# Code Meaning Example 200 Success Operation completed 400 Bad Request Invalid base64, malformed JSON 404 Not Found Namespace not found, key not found, transaction not found 500 Internal Server Error Engine error, disk full, WAL error Common Errors# Namespace not found:\n{ \u0026#34;error\u0026#34;: \u0026#34;namespace not found: invalid-ns\u0026#34; }Status: 404 Not Found\nTransaction not found:\n{ \u0026#34;error\u0026#34;: \u0026#34;transaction not found: 2a3b4c5d...\u0026#34; }Status: 404 Not Found\nInvalid base64:\n{ \u0026#34;error\u0026#34;: \u0026#34;invalid base64 encoding\u0026#34; }Status: 400 Bad Request\nEngine error:\n{ \u0026#34;error\u0026#34;: \u0026#34;failed to write: disk full\u0026#34; }Status: 500 Internal Server Error\nComplete Transaction Example# #!/bin/bash # 1. Begin transaction RESPONSE=$(curl -s -X POST http://localhost:4000/api/v1/default/tx/begin \\ -d \u0026#39;{\u0026#34;operation\u0026#34;:\u0026#34;put\u0026#34;,\u0026#34;entryType\u0026#34;:\u0026#34;kv\u0026#34;}\u0026#39;) TXN_ID=$(echo $RESPONSE | jq -r \u0026#39;.txnId\u0026#39;) echo \u0026#34;Transaction ID: $TXN_ID\u0026#34; # 2. Append multiple operations curl -X POST http://localhost:4000/api/v1/default/tx/$TXN_ID/kv \\ -d \u0026#39;{\u0026#34;key\u0026#34;:\u0026#34;account:alice:balance\u0026#34;,\u0026#34;value\u0026#34;:\u0026#34;MTAwMA==\u0026#34;}\u0026#39; curl -X POST http://localhost:4000/api/v1/default/tx/$TXN_ID/kv \\ -d \u0026#39;{\u0026#34;key\u0026#34;:\u0026#34;account:bob:balance\u0026#34;,\u0026#34;value\u0026#34;:\u0026#34;MjAwMA==\u0026#34;}\u0026#39; curl -X POST http://localhost:4000/api/v1/default/tx/$TXN_ID/kv \\ -d \u0026#39;{\u0026#34;key\u0026#34;:\u0026#34;transfer:log:123\u0026#34;,\u0026#34;value\u0026#34;:\u0026#34;YWxpY2UgLT4gYm9iOiAxMDA=\u0026#34;}\u0026#39; # 3. Commit curl -X POST http://localhost:4000/api/v1/default/tx/$TXN_ID/commit echo \u0026#34;Transaction committed!\u0026#34; # 4. Verify curl http://localhost:4000/api/v1/default/kv/account:alice:balance "}]