[{"id":0,"href":"/docs/","title":"Documentation","section":"UnisonDB – Log-Native Database For AI And Edge Computing","content":"UnisonDB Documentation# Welcome to the UnisonDB documentation! This guide will help you understand, deploy, and use UnisonDB effectively.\nDocumentation Sections# Getting Started # Learn how to install, configure, and start using UnisonDB.\nArchitecture # Understand the internal design and components of UnisonDB.\nDeployment # Deployment topologies, configurations, and operational best practices.\nAPI Reference # Complete reference for HTTP and gRPC APIs.\n"},{"id":1,"href":"/docs/getting-started/","title":"Getting Started","section":"Documentation","content":"Getting Started with UnisonDB# This guide will walk you through installing UnisonDB, configuring it, and running it in both Server and Relayer modes.\nTable of Contents# Prerequisites Installation Running in Server Mode Running in Relayer Mode Prerequisites# System Requirements# Operating System: Linux or macOS Go: Version 1.24 or higher Installation# Building from Source# UnisonDB requires CGO to be enabled for LMDB bindings.\n# Clone the repository git clone https://github.com/ankur-anand/unisondb.git cd unisondb # Enable CGO (required for LMDB) export CGO_ENABLED=1 # Build the binary go build -o unisondb ./cmd/unisondb # Verify installation ./unisondb --helpExpected output:\nNAME: unisondb - Run UnisonDB USAGE: unisondb [global options] command [command options] [arguments...] COMMANDS: replicator Run in replicator (server) mode relayer Run in relayer (replica) mode fuzzer Run fuzzer for testing (if built with -tags fuzz) help, h Shows a list of commands or help for one command GLOBAL OPTIONS: --config value, -c value Path to TOML config file (default: \u0026#34;./config.toml\u0026#34;) [$UNISON_CONFIG] --env value, -e value Environment: dev, staging, prod (default: \u0026#34;dev\u0026#34;) [$UNISON_ENV] --grpc, -G Enable gRPC server in Relayer Mode (default: false) [$UNISON_GRPC_ENABLED] --help, -h show helpBuilding with Fuzzer Support (Optional)# For testing and development, you can build with fuzzer support:\nCGO_ENABLED=1 go build -tags fuzz -o unisondb ./cmd/unisondbNote: When built with -tags fuzz:\nfuzzer command is available replicator command is disabled for safety To run fuzzer mode:\n./unisondb --config config.toml fuzzerInstallation to System Path (Optional)# # Move to system path sudo mv unisondb /usr/local/bin/ # Verify unisondb --helpRunning in Server Mode# Server Mode runs UnisonDB as a primary instance that accepts writes and serves reads.\n1. Generate TLS Certificates (Recommended)# For production or multi-node setups, generate TLS certificates for gRPC:\nUsing OpenSSL:\nmkdir -p certs \u0026amp;\u0026amp; cd certs # Generate CA openssl genrsa -out ca.key 4096 openssl req -new -x509 -key ca.key -sha256 -subj \u0026#34;/CN=UnisonDB CA\u0026#34; -days 365 -out ca.crt # Generate server certificate openssl genrsa -out server.key 4096 openssl req -new -key server.key -out server.csr -subj \u0026#34;/CN=localhost\u0026#34; openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out server.crt -days 365 -sha256 # Generate client certificate openssl genrsa -out client.key 4096 openssl req -new -key client.key -out client.csr -subj \u0026#34;/CN=client\u0026#34; openssl x509 -req -in client.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out client.crt -days 365 -sha256 cd ..2. Create Server Configuration# Create a server.toml configuration file:\n## HTTP API port http_port = 4000 listen_ip = \u0026#34;0.0.0.0\u0026#34; ## gRPC configuration (for replication) [grpc_config] listen_ip = \u0026#34;0.0.0.0\u0026#34; port = 4001 cert_path = \u0026#34;./certs/server.crt\u0026#34; key_path = \u0026#34;./certs/server.key\u0026#34; ca_path = \u0026#34;./certs/ca.crt\u0026#34; # For development only - allows insecure connections allow_insecure = false ## Storage configuration [storage_config] base_dir = \u0026#34;./data/server\u0026#34; namespaces = [\u0026#34;default\u0026#34;, \u0026#34;users\u0026#34;, \u0026#34;products\u0026#34;] bytes_per_sync = \u0026#34;1MB\u0026#34; segment_size = \u0026#34;16MB\u0026#34; arena_size = \u0026#34;4MB\u0026#34; wal_fsync_interval = \u0026#34;1s\u0026#34; ## WAL cleanup (prevents disk exhaustion) [storage_config.wal_cleanup_config] enabled = true interval = \u0026#34;5m\u0026#34; max_age = \u0026#34;1h\u0026#34; min_segments = 5 max_segments = 100 ## Write notification coalescing [write_notify_config] enabled = true max_delay = \u0026#34;20ms\u0026#34; ## ZeroMQ notifications (optional - for local apps) [notifier_config.default] bind_port = 5555 high_water_mark = 1000 linger_time = 1000 [notifier_config.users] bind_port = 5556 high_water_mark = 1000 linger_time = 1000 ## Profiling endpoint [pprof_config] enabled = true port = 6060 ## Logging [log_config] log_level = \u0026#34;info\u0026#34; [log_config.min_level_percents] debug = 100.0 info = 100.0 warn = 100.0 error = 100.03. Start the Server# Server Mode (using replicator command):\n./unisondb --config server.toml replicatorYou can also put the command before flags:\n./unisondb replicator --config server.toml4. Verify Server is Running# Check HTTP health endpoint:\ncurl http://localhost:4000/healthDevelopment Mode (Insecure)# For quick local testing without TLS:\nserver-dev.toml:\nhttp_port = 4000 [grpc_config] port = 4001 allow_insecure = true # WARNING: Development only! [storage_config] base_dir = \u0026#34;./data/server\u0026#34; namespaces = [\u0026#34;default\u0026#34;] [log_config] log_level = \u0026#34;debug\u0026#34;./unisondb --config server-dev.toml replicatorRunning in Relayer Mode# Relayer Mode runs UnisonDB as a replica that streams changes from an upstream server.\n1. Create Relayer Configuration# Create a relayer.toml configuration file:\n## HTTP API port (different from server) http_port = 5000 listen_ip = \u0026#34;0.0.0.0\u0026#34; ## gRPC config (can accept downstream relayers) [grpc_config] listen_ip = \u0026#34;0.0.0.0\u0026#34; port = 5001 cert_path = \u0026#34;./certs/server.crt\u0026#34; key_path = \u0026#34;./certs/server.key\u0026#34; ca_path = \u0026#34;./certs/ca.crt\u0026#34; ## Storage configuration [storage_config] base_dir = \u0026#34;./data/relayer\u0026#34; namespaces = [\u0026#34;default\u0026#34;, \u0026#34;users\u0026#34;, \u0026#34;products\u0026#34;] bytes_per_sync = \u0026#34;1MB\u0026#34; # IMPORTANT: segment_size MUST match upstream server! segment_size = \u0026#34;16MB\u0026#34; arena_size = \u0026#34;4MB\u0026#34; ## Relayer configuration - connects to upstream [relayer_config.primary] namespaces = [\u0026#34;default\u0026#34;, \u0026#34;users\u0026#34;, \u0026#34;products\u0026#34;] cert_path = \u0026#34;./certs/client.crt\u0026#34; key_path = \u0026#34;./certs/client.key\u0026#34; ca_path = \u0026#34;./certs/ca.crt\u0026#34; upstream_address = \u0026#34;localhost:4001\u0026#34; segment_lag_threshold = 100 allow_insecure = false ## Optional: Connect to multiple upstreams # [relayer_config.secondary] # namespaces = [\u0026#34;products\u0026#34;] # upstream_address = \u0026#34;other-server:4001\u0026#34; # cert_path = \u0026#34;./certs/client.crt\u0026#34; # key_path = \u0026#34;./certs/client.key\u0026#34; # ca_path = \u0026#34;./certs/ca.crt\u0026#34; ## ZeroMQ notifications (optional) [notifier_config.default] bind_port = 6555 high_water_mark = 1000 linger_time = 1000 ## Logging [log_config] log_level = \u0026#34;info\u0026#34; [log_config.min_level_percents] debug = 1.0 # Sample 1% of debug logs info = 10.0 # Sample 10% of info logs warn = 100.0 error = 100.02. Start the Relayer# Start relayer:\n./unisondb --config relayer.toml relayer3. Enable gRPC Server on Relayer (Multi-Hop)# To allow downstream relayers to connect to this relayer:\n./unisondb --config relayer.toml --grpc relayerThis enables the relayer to act as both a consumer (from upstream) and a producer (to downstream).\nDevelopment Mode (Insecure)# relayer-dev.toml:\nhttp_port = 5000 [grpc_config] port = 5001 [storage_config] base_dir = \u0026#34;./data/relayer\u0026#34; namespaces = [\u0026#34;default\u0026#34;] segment_size = \u0026#34;16MB\u0026#34; # Must match server! [relayer_config.primary] namespaces = [\u0026#34;default\u0026#34;] upstream_address = \u0026#34;localhost:4001\u0026#34; allow_insecure = true # WARNING: Development only! segment_lag_threshold = 100 [log_config] log_level = \u0026#34;debug\u0026#34;./unisondb --config relayer-dev.toml relayerBasic Operations# Writing Data# Key-Value Write (via HTTP):\n# Put a key-value pair curl -X POST http://localhost:4000/api/v1/namespaces/default/kv \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;key\u0026#34;: \u0026#34;user:123\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;eyJuYW1lIjoiQWxpY2UiLCJlbWFpbCI6ImFsaWNlQGV4YW1wbGUuY29tIn0=\u0026#34; }\u0026#39;Batch Write:\ncurl -X POST http://localhost:4000/api/v1/namespaces/default/kv/batch \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;operations\u0026#34;: [ {\u0026#34;key\u0026#34;: \u0026#34;user:1\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;...\u0026#34;}, {\u0026#34;key\u0026#34;: \u0026#34;user:2\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;...\u0026#34;}, {\u0026#34;key\u0026#34;: \u0026#34;user:3\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;...\u0026#34;} ] }\u0026#39;Reading Data# Read from Server:\ncurl http://localhost:4000/api/v1/namespaces/default/kv/user:123Read from Relayer (same API):\ncurl http://localhost:5000/api/v1/namespaces/default/kv/user:123Subscribing to Changes (ZeroMQ)# Python example (install pyzmq first):\nimport zmq context = zmq.Context() socket = context.socket(zmq.SUB) # Subscribe to \u0026#39;default\u0026#39; namespace socket.connect(\u0026#34;tcp://localhost:5555\u0026#34;) socket.setsockopt(zmq.SUBSCRIBE, b\u0026#34;\u0026#34;) # Subscribe to all messages print(\u0026#34;Listening for changes on namespace \u0026#39;default\u0026#39;...\u0026#34;) while True: message = socket.recv() print(f\u0026#34;Change notification: {message}\u0026#34;)Run the subscriber:\npython subscriber.pyNow any writes to the default namespace will trigger notifications!\nCommon Deployment Patterns# 1. Single Server (Development)# # Terminal 1: Start server ./unisondb --config server-dev.toml replicator2. Server + Single Relayer (Read Scaling)# # Terminal 1: Start server ./unisondb --config server.toml replicator # Terminal 2: Start relayer ./unisondb --config relayer.toml relayer3. Server + Multiple Relayers (Edge Computing)# # Terminal 1: Start server ./unisondb --config server.toml replicator # Terminal 2: Start relayer 1 ./unisondb --config relayer1.toml relayer # Terminal 3: Start relayer 2 ./unisondb --config relayer2.toml relayer # Terminal 4: Start relayer 3 ./unisondb --config relayer3.toml relayer4. Multi-Hop (Relayer → Relayer)# # Terminal 1: Primary server ./unisondb --config server.toml replicator # Terminal 2: L1 relayer (with gRPC enabled for downstream) ./unisondb --config relayer-l1.toml --grpc relayer # Terminal 3: L2 relayer (connects to L1) # Update relayer-l2.toml upstream_address to point to L1 (localhost:5001) ./unisondb --config relayer-l2.toml relayerHappy building with UnisonDB!\n"},{"id":2,"href":"/docs/api/http-api/","title":"HTTP API Reference","section":"API Reference","content":"HTTP API Reference# Complete reference for UnisonDB\u0026rsquo;s HTTP REST API.\nBase URL# http://localhost:4000/api/v1/{namespace}Data Encoding# All binary values must be base64-encoded:\n# Encode a value echo -n \u0026#34;hello world\u0026#34; | base64 # Output: aGVsbG8gd29ybGQ= # Use in request curl -X PUT http://localhost:4000/api/v1/default/kv/greeting \\ -d \u0026#39;{\u0026#34;value\u0026#34;:\u0026#34;aGVsbG8gd29ybGQ=\u0026#34;}\u0026#39;Key-Value Operations# Put KV# Store a key-value pair.\nRequest:\nPUT /api/v1/{namespace}/kv/{key} Content-Type: application/json { \u0026#34;value\u0026#34;: \u0026#34;base64-encoded-value\u0026#34; }Example:\ncurl -X PUT http://localhost:4000/api/v1/default/kv/user:123 \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;value\u0026#34;: \u0026#34;eyJuYW1lIjoiSm9obiIsImFnZSI6MzB9\u0026#34; }\u0026#39;Response (200 OK):\n{ \u0026#34;success\u0026#34;: true }Errors:\n400 Bad Request: Invalid base64 encoding 404 Not Found: Namespace not found 500 Internal Server Error: Engine error Get KV# Retrieve a value by key.\nRequest:\nGET /api/v1/{namespace}/kv/{key}Example:\ncurl http://localhost:4000/api/v1/default/kv/user:123Response (200 OK):\n{ \u0026#34;value\u0026#34;: \u0026#34;eyJuYW1lIjoiSm9obiIsImFnZSI6MzB9\u0026#34;, \u0026#34;found\u0026#34;: true }Response (404 Not Found):\n{ \u0026#34;value\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;found\u0026#34;: false } Delete KV# Delete a key.\nRequest:\nDELETE /api/v1/{namespace}/kv/{key}Example:\ncurl -X DELETE http://localhost:4000/api/v1/default/kv/user:123Response (200 OK):\n{ \u0026#34;success\u0026#34;: true } Batch KV Operations# Perform multiple operations in one request.\nBatch Put# Request:\nPOST /api/v1/{namespace}/kv/batch Content-Type: application/json { \u0026#34;operation\u0026#34;: \u0026#34;put\u0026#34;, \u0026#34;items\u0026#34;: [ {\u0026#34;key\u0026#34;: \u0026#34;key1\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;dmFsdWUx\u0026#34;}, {\u0026#34;key\u0026#34;: \u0026#34;key2\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;dmFsdWUy\u0026#34;} ] }Example:\ncurl -X POST http://localhost:4000/api/v1/default/kv/batch \\ -d \u0026#39;{ \u0026#34;operation\u0026#34;: \u0026#34;put\u0026#34;, \u0026#34;items\u0026#34;: [ {\u0026#34;key\u0026#34;: \u0026#34;user:1\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;dXNlcjE=\u0026#34;}, {\u0026#34;key\u0026#34;: \u0026#34;user:2\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;dXNlcjI=\u0026#34;} ] }\u0026#39;Response (200 OK):\n{ \u0026#34;success\u0026#34;: true, \u0026#34;processed\u0026#34;: 2 }Batch Get# Request:\nPOST /api/v1/{namespace}/kv/batch Content-Type: application/json { \u0026#34;operation\u0026#34;: \u0026#34;get\u0026#34;, \u0026#34;keys\u0026#34;: [\u0026#34;key1\u0026#34;, \u0026#34;key2\u0026#34;] }Response (200 OK):\n{ \u0026#34;success\u0026#34;: true, \u0026#34;items\u0026#34;: [ {\u0026#34;key\u0026#34;: \u0026#34;key1\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;dmFsdWUx\u0026#34;, \u0026#34;found\u0026#34;: true}, {\u0026#34;key\u0026#34;: \u0026#34;key2\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;found\u0026#34;: false} ] }Batch Delete# Request:\nPOST /api/v1/{namespace}/kv/batch Content-Type: application/json { \u0026#34;operation\u0026#34;: \u0026#34;delete\u0026#34;, \u0026#34;keys\u0026#34;: [\u0026#34;key1\u0026#34;, \u0026#34;key2\u0026#34;] }Response (200 OK):\n{ \u0026#34;success\u0026#34;: true, \u0026#34;processed\u0026#34;: 2 } Wide-Column Operations# Put Row# Store a row with multiple columns.\nRequest:\nPUT /api/v1/{namespace}/row/{rowKey} Content-Type: application/json { \u0026#34;columns\u0026#34;: { \u0026#34;column1\u0026#34;: \u0026#34;base64-value1\u0026#34;, \u0026#34;column2\u0026#34;: \u0026#34;base64-value2\u0026#34; } }Example:\ncurl -X PUT http://localhost:4000/api/v1/default/row/user:john \\ -d \u0026#39;{ \u0026#34;columns\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Sm9obiBEb2U=\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;am9obkBleGFtcGxlLmNvbQ==\u0026#34;, \u0026#34;age\u0026#34;: \u0026#34;MzA=\u0026#34; } }\u0026#39;Response (200 OK):\n{ \u0026#34;success\u0026#34;: true } Get Row# Retrieve all columns for a row.\nRequest:\nGET /api/v1/{namespace}/row/{rowKey}Example:\ncurl http://localhost:4000/api/v1/default/row/user:johnResponse (200 OK):\n{ \u0026#34;rowKey\u0026#34;: \u0026#34;user:john\u0026#34;, \u0026#34;columns\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Sm9obiBEb2U=\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;am9obkBleGFtcGxlLmNvbQ==\u0026#34;, \u0026#34;age\u0026#34;: \u0026#34;MzA=\u0026#34; }, \u0026#34;found\u0026#34;: true } Get Row Columns# Retrieve specific columns only.\nRequest:\nGET /api/v1/{namespace}/row/{rowKey}?columns=col1,col2Example:\ncurl \u0026#34;http://localhost:4000/api/v1/default/row/user:john?columns=name,email\u0026#34;Response (200 OK):\n{ \u0026#34;rowKey\u0026#34;: \u0026#34;user:john\u0026#34;, \u0026#34;columns\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Sm9obiBEb2U=\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;am9obkBleGFtcGxlLmNvbQ==\u0026#34; }, \u0026#34;found\u0026#34;: true } Delete Row# Delete an entire row.\nRequest:\nDELETE /api/v1/{namespace}/row/{rowKey}Example:\ncurl -X DELETE http://localhost:4000/api/v1/default/row/user:johnResponse (200 OK):\n{ \u0026#34;success\u0026#34;: true } Delete Row Columns# Delete specific columns from a row.\nRequest:\nDELETE /api/v1/{namespace}/row/{rowKey}/columns?columns=col1,col2Example:\ncurl -X DELETE \u0026#34;http://localhost:4000/api/v1/default/row/user:john/columns?columns=age,city\u0026#34;Response (200 OK):\n{ \u0026#34;success\u0026#34;: true } Batch Row Operations# Batch Put Rows# Request:\nPOST /api/v1/{namespace}/row/batch Content-Type: application/json { \u0026#34;operation\u0026#34;: \u0026#34;put\u0026#34;, \u0026#34;rows\u0026#34;: [ { \u0026#34;rowKey\u0026#34;: \u0026#34;user:1\u0026#34;, \u0026#34;columns\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;QWxpY2U=\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;YWxpY2VAZXhhbXBsZS5jb20=\u0026#34; } } ] }Response (200 OK):\n{ \u0026#34;success\u0026#34;: true, \u0026#34;processed\u0026#34;: 1 }Batch Get Rows# Request:\nPOST /api/v1/{namespace}/row/batch Content-Type: application/json { \u0026#34;operation\u0026#34;: \u0026#34;get\u0026#34;, \u0026#34;rowKeys\u0026#34;: [\u0026#34;user:1\u0026#34;, \u0026#34;user:2\u0026#34;] }Response (200 OK):\n{ \u0026#34;success\u0026#34;: true, \u0026#34;rows\u0026#34;: [ { \u0026#34;rowKey\u0026#34;: \u0026#34;user:1\u0026#34;, \u0026#34;columns\u0026#34;: {\u0026#34;name\u0026#34;: \u0026#34;QWxpY2U=\u0026#34;}, \u0026#34;found\u0026#34;: true }, { \u0026#34;rowKey\u0026#34;: \u0026#34;user:2\u0026#34;, \u0026#34;columns\u0026#34;: {}, \u0026#34;found\u0026#34;: false } ] } Large Object (LOB) Operations# Put LOB# Upload a large binary object. Check Transaction API.\nGet LOB# Download a large binary object.\nRequest:\nGET /api/v1/{namespace}/lob?key={key}Example:\ncurl \u0026#34;http://localhost:4000/api/v1/default/lob?key=file:doc.pdf\u0026#34; \\ --output document.pdfResponse: Binary data stream\nTransaction Operations# Transactions allow atomic operations across multiple keys.\nTransaction Lifecycle# 1. BEGIN → Get transaction ID 2. APPEND → Add operations (multiple times) 3. COMMIT → Apply atomically OR 3. ABORT → Cancel transactionBegin Transaction# Start a new transaction.\nRequest:\nPOST /api/v1/{namespace}/tx/begin Content-Type: application/json { \u0026#34;operation\u0026#34;: \u0026#34;put\u0026#34;, \u0026#34;entryType\u0026#34;: \u0026#34;kv\u0026#34; }Parameters:\noperation: \u0026quot;put\u0026quot;, \u0026quot;update\u0026quot;, or \u0026quot;delete\u0026quot; entryType: \u0026quot;kv\u0026quot;, \u0026quot;row\u0026quot;, or \u0026quot;lob\u0026quot; Example:\ncurl -X POST http://localhost:4000/api/v1/default/tx/begin \\ -d \u0026#39;{ \u0026#34;operation\u0026#34;: \u0026#34;put\u0026#34;, \u0026#34;entryType\u0026#34;: \u0026#34;kv\u0026#34; }\u0026#39;Response (200 OK):\n{ \u0026#34;txnId\u0026#34;: \u0026#34;2a3b4c5d6e7f8g9h0i1j2k3l4m5n6o7p\u0026#34;, \u0026#34;success\u0026#34;: true }Save the txnId - you\u0026rsquo;ll need it for subsequent requests!\nAppend KV to Transaction# Add a key-value operation to the transaction.\nRequest:\nPOST /api/v1/{namespace}/tx/{txnId}/kv Content-Type: application/json { \u0026#34;key\u0026#34;: \u0026#34;mykey\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;bXl2YWx1ZQ==\u0026#34; }Example:\n# Use the txnId from BEGIN response curl -X POST http://localhost:4000/api/v1/default/tx/2a3b4c.../kv \\ -d \u0026#39;{ \u0026#34;key\u0026#34;: \u0026#34;account:alice\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;MTAwMA==\u0026#34; }\u0026#39;Response (200 OK):\n{ \u0026#34;success\u0026#34;: true }Call this endpoint multiple times to add multiple operations to the same transaction.\nAppend Row to Transaction# Add a row operation to the transaction.\nRequest:\nPOST /api/v1/{namespace}/tx/{txnId}/row Content-Type: application/json { \u0026#34;rowKey\u0026#34;: \u0026#34;user:1\u0026#34;, \u0026#34;columns\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;QWxpY2U=\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;YWN0aXZl\u0026#34; } }Example:\ncurl -X POST http://localhost:4000/api/v1/default/tx/{txnId}/row \\ -d \u0026#39;{ \u0026#34;rowKey\u0026#34;: \u0026#34;user:charlie\u0026#34;, \u0026#34;columns\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Q2hhcmxpZQ==\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;Y2hhcmxpZUBleGFtcGxlLmNvbQ==\u0026#34; } }\u0026#39;Response (200 OK):\n{ \u0026#34;success\u0026#34;: true } Append LOB to Transaction# Add a large object to the transaction.\nRequest:\nPOST /api/v1/{namespace}/tx/{txnId}/lob?key={key} Content-Type: application/octet-stream \u0026lt;binary data\u0026gt;Example:\ncurl -X POST \u0026#34;http://localhost:4000/api/v1/default/tx/{txnId}/lob?key=file:backup.tar.gz\u0026#34; \\ --data-binary @backup.tar.gzResponse (200 OK):\n{ \u0026#34;success\u0026#34;: true } Commit Transaction# Apply all operations atomically.\nRequest:\nPOST /api/v1/{namespace}/tx/{txnId}/commitExample:\ncurl -X POST http://localhost:4000/api/v1/default/tx/2a3b4c.../commitResponse (200 OK):\n{ \u0026#34;success\u0026#34;: true }After commit:\nAll operations are applied atomically Transaction ID is no longer valid Data is durable and replicated Abort Transaction# Cancel the transaction without applying changes.\nRequest:\nPOST /api/v1/{namespace}/tx/{txnId}/abortExample:\ncurl -X POST http://localhost:4000/api/v1/default/tx/2a3b4c.../abortResponse (200 OK):\n{ \u0026#34;success\u0026#34;: true }After abort:\nNo operations are applied Transaction ID is no longer valid All buffered changes are discarded Metadata Operations# Get Current Offset# Get the current WAL position.\nRequest:\nGET /api/v1/{namespace}/offsetExample:\ncurl http://localhost:4000/api/v1/default/offsetResponse (200 OK):\n{ \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;segmentId\u0026#34;: 5, \u0026#34;offset\u0026#34;: 12345 } Get Engine Statistics# Get engine performance statistics.\nRequest:\nGET /api/v1/{namespace}/statsExample:\ncurl http://localhost:4000/api/v1/default/statsResponse (200 OK):\n{ \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;opsReceived\u0026#34;: 15234, \u0026#34;opsFlushed\u0026#34;: 15100, \u0026#34;currentSegment\u0026#34;: 5, \u0026#34;currentOffset\u0026#34;: 12345, \u0026#34;lastFlushTime\u0026#34;: \u0026#34;2024-01-15T10:30:45Z\u0026#34; } Get Checkpoint# Get the last checkpoint position.\nRequest:\nGET /api/v1/{namespace}/checkpointExample:\ncurl http://localhost:4000/api/v1/default/checkpointResponse (200 OK):\n{ \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;recordProcessed\u0026#34;: 15000, \u0026#34;segmentId\u0026#34;: 5, \u0026#34;offset\u0026#34;: 12000 } Error Responses# All errors follow this format:\n{ \u0026#34;error\u0026#34;: \u0026#34;error message description\u0026#34; }HTTP Status Codes# Code Meaning Example 200 Success Operation completed 400 Bad Request Invalid base64, malformed JSON 404 Not Found Namespace not found, key not found, transaction not found 500 Internal Server Error Engine error, disk full, WAL error Common Errors# Namespace not found:\n{ \u0026#34;error\u0026#34;: \u0026#34;namespace not found: invalid-ns\u0026#34; }Status: 404 Not Found\nTransaction not found:\n{ \u0026#34;error\u0026#34;: \u0026#34;transaction not found: 2a3b4c5d...\u0026#34; }Status: 404 Not Found\nInvalid base64:\n{ \u0026#34;error\u0026#34;: \u0026#34;invalid base64 encoding\u0026#34; }Status: 400 Bad Request\nEngine error:\n{ \u0026#34;error\u0026#34;: \u0026#34;failed to write: disk full\u0026#34; }Status: 500 Internal Server Error\nComplete Transaction Example# #!/bin/bash # 1. Begin transaction RESPONSE=$(curl -s -X POST http://localhost:4000/api/v1/default/tx/begin \\ -d \u0026#39;{\u0026#34;operation\u0026#34;:\u0026#34;put\u0026#34;,\u0026#34;entryType\u0026#34;:\u0026#34;kv\u0026#34;}\u0026#39;) TXN_ID=$(echo $RESPONSE | jq -r \u0026#39;.txnId\u0026#39;) echo \u0026#34;Transaction ID: $TXN_ID\u0026#34; # 2. Append multiple operations curl -X POST http://localhost:4000/api/v1/default/tx/$TXN_ID/kv \\ -d \u0026#39;{\u0026#34;key\u0026#34;:\u0026#34;account:alice:balance\u0026#34;,\u0026#34;value\u0026#34;:\u0026#34;MTAwMA==\u0026#34;}\u0026#39; curl -X POST http://localhost:4000/api/v1/default/tx/$TXN_ID/kv \\ -d \u0026#39;{\u0026#34;key\u0026#34;:\u0026#34;account:bob:balance\u0026#34;,\u0026#34;value\u0026#34;:\u0026#34;MjAwMA==\u0026#34;}\u0026#39; curl -X POST http://localhost:4000/api/v1/default/tx/$TXN_ID/kv \\ -d \u0026#39;{\u0026#34;key\u0026#34;:\u0026#34;transfer:log:123\u0026#34;,\u0026#34;value\u0026#34;:\u0026#34;YWxpY2UgLT4gYm9iOiAxMDA=\u0026#34;}\u0026#39; # 3. Commit curl -X POST http://localhost:4000/api/v1/default/tx/$TXN_ID/commit echo \u0026#34;Transaction committed!\u0026#34; # 4. Verify curl http://localhost:4000/api/v1/default/kv/account:alice:balance "},{"id":3,"href":"/docs/architecture/","title":"Architecture Overview – Log-Native Database for Edge and AI Systems","section":"Documentation","content":"Architecture Overview# UnisonDB is a log-native database that replicates like a message bus. It combines transactional database semantics with streaming replication mechanics, designed for edge computing, local-first applications, and distributed topologies.\nKey Characteristics# Aspect Design Choice Storage Model Multi-modal: Key-Value, Wide-Column, Large Objects (LOB) Storage Engine B+Tree (LMDB/BoltDB) with in-memory MemTable overlay Replication Log streaming (gRPC) with eventual consistency Watch API Best-effort change notifications (per-namespace) Consistency Single-primary writes, eventual consistency for replicas Durability WAL-first with configurable fsync Deployment Modes Server (writable primary) \u0026amp; Relayer (read-only replica) Core Concepts# 1. Log-Native Design# The Write-Ahead Log (WAL) is a first-class citizen, not just a recovery mechanism.\nImplications:\nReplication = Log streaming: No separate replication protocol Recovery = Log replay: State reconstructed from WAL Time-travel enabled: Historic snapshots from log Single source of truth: All operations flow through WAL 2. Operational Modes# UnisonDB instances run in one of two modes:\nServer Mode (Primary)# Writable instance that maintains the authoritative WAL.\n┌─────────────────────────────────────┐ │ Server Mode Instance │ │ │ │ • Accepts writes (HTTP/gRPC API) │ │ • Maintains authoritative WAL │ │ • Streams to relayers (gRPC) │ │ • Publishes watch events (local) │ │ • Serves reads from local storage │ └─────────────────────────────────────┘Relayer Mode (Replica)# Read-only instance that streams changes from upstream servers.\n┌─────────────────────────────────────┐ │ Relayer Mode Instance │ │ │ │ • Connects to upstream (gRPC) │ │ • Receives WAL segment streams │ │ • Applies to local storage (RO) │ │ • Can relay to downstream nodes │ │ • Publishes watch events (local) │ │ • Serves reads (data locality) │ └─────────────────────────────────────┘3. Dual Communication Channels# UnisonDB separates distribution from local reactivity:\nChannel Purpose Use Case Scope gRPC Durable replication Node-to-node WAL streaming Network (cross-machine) Watch API Best-effort notifications Local application reactivity IPC (same machine) Design Rationale:\ngRPC Replication: Network-tolerant, authenticated, durable, at-least-once delivery Watch API: Lightweight, fire-and-forget, at-most-once, local-only System Architecture# ┌────────────────────────────────────────────────────────────────────┐ │ UnisonDB Instance │ ├────────────────────────────────────────────────────────────────────┤ │ Client APIs │ │ ┌──────────────┐ ┌──────────────┐ ┌──────────────┐ │ │ │ HTTP REST │ │ Transactions │ │ Admin/Stats │ │ │ └──────┬───────┘ └──────┬───────┘ └──────┬───────┘ │ │ └──────────────────┴──────────────────┘ │ ├────────────────────────────────────────────────────────────────────┤ │ Storage Models │ │ ┌─────────────┐ ┌──────────────┐ ┌──────────────┐ │ │ │ Key-Value │ │ Wide-Column │ │ Large Object │ │ │ └──────┬──────┘ └──────┬───────┘ └──────┬───────┘ │ │ └─────────────────┴──────────────────┘ │ ├────────────────────────────────────────────────────────────────────┤ │ Storage Engine (dbkernel) │ │ │ │ Write: WAL (append) → MemTable → B+Tree (LMDB) │ │ Read: MemTable ⊕ B+Tree → Response │ ├────────────────────────────────────────────────────────────────────┤ │ Distribution Layer │ │ ┌──────────────────────┐ ┌──────────────────────┐ │ │ │ gRPC Replicator │ │ Watch API │ │ │ │ • WAL streaming │ │ • Change events │ │ │ │ • TLS/mTLS │ │ • Per-namespace │ │ │ │ • At-least-once │ │ • At-most-once │ │ │ └──────────┬───────────┘ └──────────┬───────────┘ │ │ ↓ ↓ │ │ Remote Relayers Local Applications │ └────────────────────────────────────────────────────────────────────┘Core Components# Write-Ahead Log (WAL)# Append-only transaction log that serves as the source of truth.\nProperty Implementation Structure Segmented files (configurable size, default 16MB) Format Binary with CRC32 checksums per entry Lifecycle Write → fsync → MemTable → B+Tree → Segment cleanup Purpose Durability, replication streaming, crash recovery Segment Format:\n┌─────────────────────────────────┐ │ Header (magic, segment#, ts) │ ├─────────────────────────────────┤ │ Entry: [type|ns|key|val|crc] │ │ Entry: [type|ns|key|val|crc] │ │ ... │ └─────────────────────────────────┘Storage Engine# Multi-modal storage built on B+Trees with MemTable overlay.\nComponent Technology Purpose MemTable In-memory hash map Write buffer, recent reads B+Tree LMDB/BoltDB Persistent sorted storage Encoding Model-specific key schemas Namespace isolation Storage Models:\nKey-Value: \u0026lt;key\u0026gt; → \u0026lt;value\u0026gt; Wide-Column: \u0026lt;rowKey\u0026gt;:\u0026lt;colName\u0026gt; → \u0026lt;colValue\u0026gt; Large Object: \u0026lt;objectKey\u0026gt;:chunk:\u0026lt;N\u0026gt; → \u0026lt;chunk_data\u0026gt;Replication System# gRPC-based WAL streaming with bidirectional flow control.\nServer Role (Primary):\nStreams WAL segments to connected relayers Monitors lag per relayer connection Handles backpressure and catch-up requests Relayer Role (Replica):\nConsumes from one or more upstream servers Applies segments in order to local storage Can fan-out to downstream relayers (multi-hop) Guarantees:\nConsistency Model: Eventual (all replicas converge) Delivery: At-least-once with gap detection Ordering: Strict segment sequence enforcement Security: TLS/mTLS (insecure mode for dev only) Namespace Watch API# Lightweight, best-effort notification system for real-time change awareness.\nEvent Structure:\n{ \u0026#34;namespace\u0026#34;: \u0026#34;users\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;user:123\u0026#34;, \u0026#34;entry_type\u0026#34;: \u0026#34;PUT\u0026#34;, \u0026#34;seq\u0026#34;: 42 } Field Description namespace Namespace where change occurred key Exact key that changed entry_type Operation: PUT, DELETE, UPDATE seq Monotonic sequence number (per-namespace ordering) Characteristics:\nPer-namespace streams: Independent event streams per namespace Ordered delivery: Events delivered in sequence order (if received) Fire-and-forget: No acknowledgments, best-effort delivery Ring buffer: Configurable size-based event retention Backpressure handling: Events dropped if subscribers can\u0026rsquo;t keep up Transport-agnostic: ZeroMQ PUB/SUB, gRPC streams, etc. Delivery Guarantees:\n⚠️ At-most-once: Events may be dropped under load ⚠️ Not resumable: Cannot replay from a specific sequence ⚠️ No gap detection: Subscribers not notified of missed events ✅ Ordered: Events delivered in sequence order (within what you receive) Use Cases:\n✅ Cache invalidation across microservices ✅ Real-time dashboards and monitoring ✅ Trigger event-driven workflows ✅ Local application coordination ❌ NOT for audit logs (use gRPC replication instead) ❌ NOT for critical workflows requiring guaranteed delivery Example Flow:\nApplication A: PUT user:123 → UnisonDB appends to WAL → Watch stream publishes: {namespace:\u0026#34;users\u0026#34;, key:\u0026#34;user:123\u0026#34;, entry_type:\u0026#34;PUT\u0026#34;, seq:42} → Applications B, C, D receive notification and reactDesign Principles# Multi-Modal Storage Examples# All storage models share the same WAL and B+Tree, differing only in key encoding:\nModel Example Key Encoding Key-Value user:123 → {\u0026quot;name\u0026quot;:\u0026quot;Alice\u0026quot;} \u0026lt;key\u0026gt; Wide-Column user:123 with columns name, email \u0026lt;rowKey\u0026gt;:\u0026lt;colName\u0026gt; Large Object video:abc as streamable chunks \u0026lt;objectKey\u0026gt;:chunk:\u0026lt;N\u0026gt; Edge-First Topology# Designed for hub-and-spoke, multi-hop replication with data locality:\n┌──────────────┐ │ Primary │ (Server Mode - accepts writes) │ (US-East) │ └──────┬───────┘ │ gRPC (durable replication) ┌────────┼────────┐ ↓ ↓ ↓ ┌───────┐┌───────┐┌───────┐ │Europe ││ Asia ││US-West│ (Relayer Mode - read replicas) │Relayer││Relayer││Relayer│ └───┬───┘└───┬───┘└───┬───┘ │ │ │ Watch API (local events) ↓ ↓ ↓ Local Local Local Apps Apps AppsData Flow# Write Path (Server Mode)# Write Request → API Handler → Storage Engine ↓ ┌───────────────┴───────────────┐ ↓ ↓ 1. WAL Append 2. MemTable Update (+ fsync) (in-memory) ↓ ↓ 3. Background Flush ────────────────→ B+Tree (LMDB) ↓ ┌───────┴────────┐ ↓ ↓ gRPC Stream Watch Event (to relayers) (to local apps)Latency Profile:\nWAL append + MemTable: \u0026lt; 1ms (sequential write) fsync: ~5-10ms (if enabled) B+Tree flush: Background, async Read Path# Read Request → API Handler → Storage Engine ↓ ┌───────┴───────┐ ↓ ↓ MemTable B+Tree (check first) (if not found) └───────┬───────┘ ↓ Merge \u0026amp; ResponseLatency: \u0026lt; 1ms (local read, no network)\nReplication Flow (gRPC)# Server (Primary) Relayer (Replica) │ │ │ ─── WAL Segment (gRPC stream) ────→ │ │ [metadata + binary + CRC] │ │ ↓ │ 1. Validate checksum │ 2. Append to local WAL │ 3. Apply to MemTable │ 4. Flush to B+Tree │ ↓ │ Can relay downstream │ Can notify local appsNetwork Requirements: TLS/mTLS (production), bidirectional streaming\nWatch Event Flow# WAL Write → Watch Event Builder → Ring Buffer → Transport Layer │ ┌──────────────┼──────────────┐ ↓ ↓ ↓ App A App B App C (subscriber) (subscriber) (subscriber)Event Details:\nTriggered on every WAL append (PUT/DELETE/UPDATE) Contains: namespace, key, entry_type, seq Buffered in ring buffer (configurable size) Events dropped if buffer full or subscriber slow Transport: ZeroMQ PUB/SUB (default), gRPC streams, etc. Storage Layout# data/ ├── \u0026lt;namespace\u0026gt;/ │ ├── wal/ │ │ ├── segment-00000000 # 16MB segments (configurable) │ │ ├── segment-00000001 │ │ └── ... │ ├── db/ │ │ ├── data.mdb # LMDB data file │ │ └── lock.mdb # LMDB lock file │ └── checkpoint/ │ └── last_applied # Recovery pointPer-Namespace Isolation: Each namespace has independent WAL, DB, and checkpoint state.\nSystem Characteristics# Consistency Model# Aspect Guarantee Writes Single primary (Server Mode) for linearizability Reads Eventually consistent across relayers Replication At-least-once delivery, ordered segments Isolation Per-namespace (independent namespaces) Durability \u0026amp; Recovery# Crash Recovery:\nScan WAL for uncommitted operations Replay WAL to rebuild MemTable and B+Tree Validate checkpoint consistency Resume operations Replication Recovery (Relayer Mode):\nDetermine last applied segment from checkpoint Reconnect to upstream at last offset Request missing segments (gap detection) Apply backlog before serving reads Data Durability:\nWAL with optional fsync (configurable) CRC32 checksums on all WAL entries Segment-level validation during replication Deployment Topologies# UnisonDB supports various deployment patterns. See the Deployment Guide for detailed configurations and examples.\nCommon Topologies# Pattern Use Case Complexity Single Server Development, small apps Low Primary + Replicas Read scaling, geographic distribution Medium Hub-and-Spoke Edge computing, IoT, retail Medium Multi-Hop Deep edge, hierarchical networks High Hybrid Microservices with local events Medium Example: Hub-and-Spoke# ┌──────────┐ │ Hub │ (Server Mode) └────┬─────┘ │ gRPC (durable replication) ┌──────────┼──────────┐ ↓ ↓ ↓ ┌──────┐ ┌──────┐ ┌──────┐ │Edge 1│ │Edge 2│ │Edge 3│ (Relayers) └──┬───┘ └──┬───┘ └──┬───┘ │ │ │ Watch API (local events) ↓ ↓ ↓ Local Local Local Apps Apps AppsKey characteristics:\nCentral hub accepts all writes Edge relayers provide data locality Local applications subscribe to watch events Best for: Edge computing, retail, IoT For detailed configurations, monitoring, and operational guidance, see the Deployment Topologies Guide .\nTradeoffs \u0026amp; Limitations# Design Tradeoffs# Aspect Tradeoff Rationale Write Scalability Single primary per namespace Ensures linearizable writes, simplifies conflict resolution Read Consistency Eventual consistency on relayers Enables high read scalability and data locality Replication Model At-least-once delivery Prioritizes availability over exactly-once semantics Watch Events At-most-once, best-effort Minimizes latency, lightweight notification for non-critical use cases Current Limitations# Write Scaling: Single primary per namespace (no multi-master) Consistency: No strong consistency guarantees for relayer reads Transactions: Limited to single-namespace operations Query Model: No complex queries (no SQL, joins, aggregations) Schema: Schema-less (application-managed structure) When to Use UnisonDB# Good Fit:\nEdge computing with hub-and-spoke topology Read-heavy workloads requiring data locality Event-driven architectures (via Watch API) Applications tolerating eventual consistency Key-value or wide-column access patterns Large object storage with streaming Not a Good Fit:\nStrong consistency requirements across replicas Complex relational queries (joins, aggregations) Multi-region active-active writes Workloads requiring ACID transactions across namespaces Summary# UnisonDB combines database semantics with streaming mechanics through:\nLog-Native Design: WAL as first-class citizen (replication = log streaming) Dual Communication: gRPC for distribution, Watch API for local reactivity Dual Modes: Server (writable primary) and Relayer (read replicas) Multi-Modal Storage: Key-Value, Wide-Column, Large Objects on shared B+Tree Architecture Strengths:\nData locality through edge replicas Event-driven integration via Watch API Simple operational model (log streaming) Flexible deployment topologies (hub-and-spoke, multi-hop) Best For: Edge computing, local-first applications, and read-scalable systems with eventual consistency tolerance.\n"},{"id":4,"href":"/docs/api/","title":"API Reference","section":"Documentation","content":"API Reference# UnisonDB provides API interfaces for client access:\nHTTP REST API # RESTful HTTP API with JSON payloads, supporting:\nKey-Value operations Wide-Column operations Large Object (LOB) operations Stateful transactions Metadata queries Next Steps# HTTP API Reference - Complete HTTP API documentation "},{"id":5,"href":"/docs/getting-started/configurations/","title":"Configuration","section":"Getting Started","content":"Configuration Guide# UnisonDB uses TOML for configuration. This guide covers all available configuration options for both server and relayer modes.\nTable of Contents# Server Mode Relayer Mode Configuration Reference Server Mode# Server mode runs UnisonDB as a primary instance that accepts writes and serves reads. Here\u0026rsquo;s a complete example:\n## Port of the http server http_port = 4000 listen_ip = \u0026#34;0.0.0.0\u0026#34; ## grpc config for replication [grpc_config] listen_ip = \u0026#34;0.0.0.0\u0026#34; port = 4001 # SSL/TLS certificate paths for gRPC server cert_path = \u0026#34;../../certs/server.crt\u0026#34; key_path = \u0026#34;../../certs/server.key\u0026#34; ca_path = \u0026#34;../../certs/ca.crt\u0026#34; # Allow insecure connections (no TLS) - ONLY for development! allow_insecure = false # StorageConfig stores all tunable parameters. [storage_config] base_dir = \u0026#34;/tmp/unisondb/server\u0026#34; # Base directory for storage namespaces = [\u0026#34;default\u0026#34;, \u0026#34;tenant_1\u0026#34;, \u0026#34;tenant_2\u0026#34;, \u0026#34;tenant_3\u0026#34;, \u0026#34;tenant_4\u0026#34;] bytes_per_sync = \u0026#34;1MB\u0026#34; segment_size = \u0026#34;16MB\u0026#34; arena_size = \u0026#34;4MB\u0026#34; wal_fsync_interval = \u0026#34;1s\u0026#34; ## WAL cleanup configuration [storage_config.wal_cleanup_config] enabled = false interval = \u0026#34;5m\u0026#34; max_age = \u0026#34;1h\u0026#34; min_segments = 5 max_segments = 10 ## Write notify config - coalesces notifications from WAL writers to readers [write_notify_config] enabled = true max_delay = \u0026#34;20ms\u0026#34; ## ZeroMQ notifier configuration (per-namespace) ## Publishes change notifications for local application consumption [notifier_config.default] bind_port = 5555 high_water_mark = 1000 linger_time = 1000 [notifier_config.tenant_1] bind_port = 5556 high_water_mark = 1000 linger_time = 1000 [pprof_config] enabled = true port = 6060 [log_config] log_level = \u0026#34;info\u0026#34; disable_timestamp = false ## This is for grpc logging only - controls sampling percentages per level [log_config.min_level_percents] debug = 100.0 info = 50.0 warn = 100.0 error = 100.0 ## Fuzzer configuration (for testing) [fuzz_config] ops_per_namespace = 400 workers_per_namespace = 50 local_relayer_count = 1000 startup_delay = \u0026#34;10s\u0026#34; enable_read_ops = falseRelayer Mode# Relayer mode runs UnisonDB as a replica that streams changes from one or more upstream servers. This provides read scalability and data locality.\n## Port of the http server http_port = 6000 [grpc_config] port = 6001 cert_path = \u0026#34;../../certs/server.crt\u0026#34; key_path = \u0026#34;../../certs/server.key\u0026#34; ca_path = \u0026#34;../../certs/ca.crt\u0026#34; [storage_config] base_dir = \u0026#34;/tmp/unisondb/relayer\u0026#34; namespaces = [\u0026#34;default\u0026#34;, \u0026#34;tenant_1\u0026#34;, \u0026#34;tenant_2\u0026#34;] bytes_per_sync = \u0026#34;1MB\u0026#34; ## IMPORTANT: segment_size must match upstream server! segment_size = \u0026#34;16MB\u0026#34; arena_size = \u0026#34;4MB\u0026#34; ## Relayer configuration - can have multiple upstreams [relayer_config] [relayer_config.relayer1] namespaces = [\u0026#34;default\u0026#34;, \u0026#34;tenant_1\u0026#34;, \u0026#34;tenant_2\u0026#34;] cert_path = \u0026#34;../../certs/client.crt\u0026#34; key_path = \u0026#34;../../certs/client.key\u0026#34; ca_path = \u0026#34;../../certs/ca.crt\u0026#34; upstream_address = \u0026#34;localhost:4001\u0026#34; segment_lag_threshold = 100 allow_insecure = false # Optional: custom gRPC service config JSON grpc_service_config = \u0026#34;\u0026#34; ## Optional: Add more relayers for different upstream sources [relayer_config.relayer2] namespaces = [\u0026#34;tenant_3\u0026#34;] cert_path = \u0026#34;../../certs/client2.crt\u0026#34; key_path = \u0026#34;../../certs/client2.key\u0026#34; ca_path = \u0026#34;../../certs/ca.crt\u0026#34; upstream_address = \u0026#34;remote-server:4001\u0026#34; segment_lag_threshold = 100 [log_config] log_level = \u0026#34;info\u0026#34; [log_config.min_level_percents] debug = 0.01 info = 1.0 warn = 1.0 error = 1.0 Configuration Reference# Server Configuration# HTTP Server# http_port = 4000 listen_ip = \u0026#34;0.0.0.0\u0026#34;http_port# Type: Integer Default: 4000 Description: Port for the HTTP API server listen_ip# Type: String Default: \u0026quot;0.0.0.0\u0026quot; Description: IP address to bind HTTP server to Note: Use \u0026quot;127.0.0.1\u0026quot; for localhost-only access gRPC Configuration# [grpc_config] listen_ip = \u0026#34;0.0.0.0\u0026#34; port = 4001 cert_path = \u0026#34;/path/to/server.crt\u0026#34; key_path = \u0026#34;/path/to/server.key\u0026#34; ca_path = \u0026#34;/path/to/ca.crt\u0026#34; allow_insecure = falselisten_ip# Type: String Default: \u0026quot;0.0.0.0\u0026quot; Description: IP address to bind gRPC server to port# Type: Integer Default: 4001 Description: Port for the gRPC server (used for replication) cert_path# Type: String Default: \u0026quot;\u0026quot; Description: Path to TLS certificate file (PEM format) Required: Yes (unless allow_insecure = true) key_path# Type: String Default: \u0026quot;\u0026quot; Description: Path to TLS private key file (PEM format) Required: Yes (unless allow_insecure = true) ca_path# Type: String Default: \u0026quot;\u0026quot; Description: Path to CA certificate file for mTLS Required: Yes (unless allow_insecure = true) allow_insecure# Type: Boolean Default: false Description: Allow insecure connections without TLS Warning: ONLY use in development! Always enable TLS in production Storage Configuration# [storage_config] base_dir = \u0026#34;./data\u0026#34; namespaces = [\u0026#34;default\u0026#34;, \u0026#34;app\u0026#34;] bytes_per_sync = \u0026#34;1MB\u0026#34; segment_size = \u0026#34;16MB\u0026#34; arena_size = \u0026#34;4MB\u0026#34; wal_fsync_interval = \u0026#34;1s\u0026#34; disable_entry_type_check = falsebase_dir# Type: String Default: \u0026quot;./data\u0026quot; Description: Base directory for all data files (WAL segments, LMDB) Note: Must have write permissions namespaces# Type: Array of Strings Default: [\u0026quot;default\u0026quot;] Description: List of namespaces to create on startup Example: [\u0026quot;default\u0026quot;, \u0026quot;users\u0026quot;, \u0026quot;metrics\u0026quot;, \u0026quot;logs\u0026quot;] Note: Each namespace is isolated with separate WAL and storage bytes_per_sync# Type: String (with unit) Default: \u0026quot;1MB\u0026quot; Valid Units: KB, MB, GB Description: Number of bytes to write before forcing fsync segment_size# Type: String (with unit) Default: \u0026quot;16MB\u0026quot; Valid Units: KB, MB, GB Range: 1MB to 1GB Description: Size of each WAL segment file Important: Must match across server and relayer! arena_size# Type: String (with unit) Default: \u0026quot;4MB\u0026quot; Valid Units: KB, MB, GB Range: 1MB to 64MB Description: Size of the write buffer (memtable) Performance: Larger = fewer flushes, more memory usage wal_fsync_interval# Type: String (duration) Default: \u0026quot;1s\u0026quot; Valid Units: ms, s, m Description: Interval for periodic WAL fsync Trade-off: Lower = better durability, higher = better performance WAL Cleanup Configuration# [storage_config.wal_cleanup_config] enabled = false interval = \u0026#34;5m\u0026#34; max_age = \u0026#34;1h\u0026#34; min_segments = 5 max_segments = 10enabled# Type: Boolean Default: false Description: Enable automatic WAL segment cleanup interval# Type: String (duration) Default: \u0026quot;5m\u0026quot; Description: How often to run cleanup max_age# Type: String (duration) Default: \u0026quot;1h\u0026quot; Description: Maximum age of segments before cleanup min_segments# Type: Integer Default: 5 Description: Minimum number of segments to keep max_segments# Type: Integer Default: 10 Description: Trigger cleanup when this many segments exist Write Notification Configuration# Write notifications coalesce updates from WAL writers to readers, reducing notification overhead.\n[write_notify_config] enabled = true max_delay = \u0026#34;20ms\u0026#34;enabled# Type: Boolean Default: true Description: Enable write notification coalescing max_delay# Type: String (duration) Default: \u0026quot;20ms\u0026quot; Valid Units: ms, s Description: Maximum delay before notifying readers Trade-off: Higher = better batching, higher latency for reads ZeroMQ Notifier Configuration# UnisonDB can publish change notifications via ZeroMQ PUB/SUB sockets. This allows local applications to subscribe to real-time change notifications for specific namespaces.\nUse Case: Applications running on the same machine can subscribe to a namespace\u0026rsquo;s ZeroMQ socket and receive notifications whenever data changes, enabling reactive architectures.\n## Each namespace can have its own ZeroMQ notifier [notifier_config.default] bind_port = 5555 high_water_mark = 1000 linger_time = 1000 [notifier_config.tenant_1] bind_port = 5556 high_water_mark = 2000 linger_time = 500bind_port# Type: Integer Required: Yes Description: Port to bind ZeroMQ PUB socket to Format: Applications subscribe to tcp://localhost:{bind_port} high_water_mark# Type: Integer Default: 1000 Description: Maximum number of queued messages before blocking Note: Higher values use more memory but reduce message loss linger_time# Type: Integer (milliseconds) Default: 1000 Description: How long to wait for pending messages on shutdown Range: 0 to 5000 ms Example Application Subscription:\nimport zmq context = zmq.Context() socket = context.socket(zmq.SUB) socket.connect(\u0026#34;tcp://localhost:5555\u0026#34;) # Connect to default namespace socket.setsockopt(zmq.SUBSCRIBE, b\u0026#34;\u0026#34;) # Subscribe to all messages while True: message = socket.recv() print(f\u0026#34;Received change notification: {message}\u0026#34;) Relayer Configuration# Relayer configuration allows a UnisonDB instance to stream WAL changes from one or more upstream servers. This is useful for:\nRead scaling: Run multiple read replicas Data locality: Keep data close to consumers in different regions Backup: Maintain hot standbys [relayer_config] [relayer_config.relayer1] namespaces = [\u0026#34;default\u0026#34;, \u0026#34;tenant_1\u0026#34;] cert_path = \u0026#34;../../certs/client.crt\u0026#34; key_path = \u0026#34;../../certs/client.key\u0026#34; ca_path = \u0026#34;../../certs/ca.crt\u0026#34; upstream_address = \u0026#34;primary-server:4001\u0026#34; segment_lag_threshold = 100 allow_insecure = false grpc_service_config = \u0026#34;\u0026#34;Map Key (e.g., relayer1)# Type: String Description: Unique identifier for this relayer connection Note: Multiple relayers can be configured with different keys namespaces# Type: Array of Strings Required: Yes Description: List of namespaces to replicate from this upstream Note: Namespaces must exist on both upstream and local instance cert_path# Type: String Description: Path to client TLS certificate for mTLS Required: Yes (unless allow_insecure = true) key_path# Type: String Description: Path to client private key for mTLS Required: Yes (unless allow_insecure = true) ca_path# Type: String Description: Path to CA certificate to verify upstream server Required: Yes (unless allow_insecure = true) upstream_address# Type: String Required: Yes Description: Address of upstream gRPC server Format: host:port (e.g., \u0026quot;localhost:4001\u0026quot;, \u0026quot;10.0.1.5:4001\u0026quot;) segment_lag_threshold# Type: Integer Default: 100 Description: Maximum segment lag before logging warnings Note: Helps monitor replication health allow_insecure# Type: Boolean Default: false Description: Allow insecure connection to upstream (no TLS) Warning: Only for development! grpc_service_config# Type: String (JSON) Default: \u0026quot;\u0026quot; (uses built-in defaults) Description: Custom gRPC service configuration JSON Advanced: See gRPC documentation for format Logging Configuration# [log_config] log_level = \u0026#34;info\u0026#34; disable_timestamp = false [log_config.min_level_percents] debug = 100.0 info = 50.0 warn = 100.0 error = 100.0log_level# Type: String Valid Values: \u0026quot;debug\u0026quot;, \u0026quot;info\u0026quot;, \u0026quot;warn\u0026quot;, \u0026quot;error\u0026quot; Default: \u0026quot;info\u0026quot; Description: Minimum log level to output disable_timestamp# Type: Boolean Default: false Description: Disable timestamps in log output Use Case: When running under systemd/journal (timestamps added automatically) min_level_percents# Type: Map of String to Float Description: Sampling percentages for gRPC logging per level Range: 0.0 to 100.0 Purpose: Reduce log volume in high-traffic scenarios Example: info = 1.0 means sample 1% of info logs Log Levels:\ndebug: 100.0 = log all debug messages info: 50.0 = log 50% of info messages (randomly sampled) warn: 100.0 = log all warnings error: 100.0 = log all errors PProf Configuration# [pprof_config] enabled = true port = 6060enabled# Type: Boolean Default: false Description: Enable pprof HTTP server for profiling port# Type: Integer Default: 6060 Description: Port for pprof HTTP server Access: http://localhost:6060/debug/pprof/ Available Profiles:\n/debug/pprof/heap - Memory allocation /debug/pprof/goroutine - Goroutine stack traces /debug/pprof/profile - CPU profile /debug/pprof/trace - Execution trace Fuzzer Configuration# Built-in fuzzer for testing and stress testing UnisonDB.\n[fuzz_config] ops_per_namespace = 400 workers_per_namespace = 50 local_relayer_count = 1000 startup_delay = \u0026#34;10s\u0026#34; enable_read_ops = falseops_per_namespace# Type: Integer Default: 400 Description: Number of operations to perform per namespace workers_per_namespace# Type: Integer Default: 50 Description: Number of concurrent workers per namespace local_relayer_count# Type: Integer Default: 1000 Description: Number of local relayer goroutines to simulate startup_delay# Type: String (duration) Default: \u0026quot;10s\u0026quot; Description: Delay before starting fuzzer Purpose: Allow infrastructure to fully initialize enable_read_ops# Type: Boolean Default: false Description: Include read operations in fuzzing Note: Generates mixed read/write workload when enabled "},{"id":6,"href":"/docs/deployment/","title":"Deployment","section":"Documentation","content":"Deployment Topologies# This guide covers common deployment patterns for UnisonDB, from simple single-server setups to complex multi-region, multi-hop configurations.\nOverview# UnisonDB\u0026rsquo;s dual-mode architecture (Server and Relayer) enables flexible deployment topologies:\nServer Mode: Writable primary instances Relayer Mode: Read-only replicas that stream from upstream Watch API: Local event notifications for applications gRPC Replication: Durable WAL streaming between nodes Quick Reference# Topology Use Case Complexity Availability Read Scaling Single Server Development, small apps Low None 1x Primary + Replicas Read-heavy workloads Medium Medium Nx Hub-and-Spoke Edge computing, IoT Medium High Nx Multi-Hop Deep edge, hierarchical High Very High Nx Hybrid Microservices, event-driven Medium Medium Nx 1. Single Server# Simplest deployment for development, testing, or small standalone applications.\n┌──────────────────────────┐ │ UnisonDB Server │ │ (Server Mode) │ │ │ │ • HTTP API :8080 │ │ • gRPC API :9090 │ │ • Watch API :5555 │ │ │ │ Capabilities: │ │ ✓ Reads \u0026amp; Writes │ │ ✓ Local watch events │ │ ✗ No replication │ │ ✗ No high availability │ └──────────────────────────┘Configuration# Server config (server.toml):\n[server] mode = \u0026#34;server\u0026#34; data_dir = \u0026#34;./data\u0026#34; http_addr = \u0026#34;0.0.0.0:8080\u0026#34; grpc_addr = \u0026#34;0.0.0.0:9090\u0026#34; [wal] segment_size = \u0026#34;16MB\u0026#34; fsync_enabled = true [watch] enabled = true transport = \u0026#34;zeromq\u0026#34; bind_addr = \u0026#34;tcp://*:5555\u0026#34; buffer_size = 10000When to Use# ✅ Good for:\nDevelopment and testing Proof-of-concept deployments Small applications with low traffic Standalone services on single machine ❌ Avoid for:\nProduction systems requiring HA Read-heavy workloads Distributed applications Monitoring# # Check server health curl http://localhost:8080/health # View server stats curl http://localhost:8080/stats # Monitor watch subscribers curl http://localhost:8080/stats/watch 2. Primary + Read Replicas# Primary handles writes, replicas provide read scalability and geographic distribution.\n┌─────────────────────┐ │ Primary Server │ (Server Mode) │ US-East │ │ • Writes │ │ • Reads │ └──────────┬──────────┘ │ gRPC replication (TLS) ┌────────┼────────┬────────┐ ↓ ↓ ↓ ↓ ┌────────┐┌────────┐┌────────┐┌────────┐ │Relayer ││Relayer ││Relayer ││Relayer │ │US-West ││Europe ││Asia ││Canada │ │ ││ ││ ││ │ │Read- ││Read- ││Read- ││Read- │ │only ││only ││only ││only │ └────────┘└────────┘└────────┘└────────┘Configuration# Primary server (primary.toml):\n[server] mode = \u0026#34;server\u0026#34; data_dir = \u0026#34;/data/unisondb\u0026#34; http_addr = \u0026#34;0.0.0.0:8080\u0026#34; grpc_addr = \u0026#34;0.0.0.0:9090\u0026#34; [replication] enabled = true tls_cert = \u0026#34;/etc/unisondb/tls/server.crt\u0026#34; tls_key = \u0026#34;/etc/unisondb/tls/server.key\u0026#34; tls_ca = \u0026#34;/etc/unisondb/tls/ca.crt\u0026#34; [watch] enabled = true bind_addr = \u0026#34;tcp://*:5555\u0026#34; buffer_size = 10000Relayer (relayer-us-west.toml):\n[server] mode = \u0026#34;relayer\u0026#34; data_dir = \u0026#34;/data/unisondb\u0026#34; http_addr = \u0026#34;0.0.0.0:8080\u0026#34; [relayer] upstreams = [ \u0026#34;primary.us-east.example.com:9090\u0026#34; ] tls_cert = \u0026#34;/etc/unisondb/tls/client.crt\u0026#34; tls_key = \u0026#34;/etc/unisondb/tls/client.key\u0026#34; tls_ca = \u0026#34;/etc/unisondb/tls/ca.crt\u0026#34; # Relayer can also publish local watch events [watch] enabled = true bind_addr = \u0026#34;tcp://*:5555\u0026#34; buffer_size = 10000Load Balancing# Read traffic (round-robin across replicas):\nupstream unisondb_reads { server relayer-us-west.example.com:8080; server relayer-europe.example.com:8080; server relayer-asia.example.com:8080; server relayer-canada.example.com:8080; } server { location /api/read { proxy_pass http://unisondb_reads; } }Write traffic (always to primary):\nupstream unisondb_writes { server primary-us-east.example.com:8080; } server { location /api/write { proxy_pass http://unisondb_writes; } }When to Use# ✅ Good for:\nRead-heavy workloads (10:1 read/write ratio or higher) Geographic distribution (data locality) High availability for reads Scaling read throughput ❌ Avoid for:\nWrite-heavy workloads (single primary bottleneck) Strong consistency requirements across regions Low-latency writes from multiple regions Operational Considerations# Aspect Recommendation Replication lag Monitor via /stats/replication endpoint TLS/mTLS Required for production (use cert rotation) Failover Manual primary promotion (no automatic failover) Backup Backup primary WAL + snapshot regularly 3. Hub-and-Spoke (Edge Computing)# Central hub replicates to many edge nodes, each serving local applications.\n┌──────────────────┐ │ Central Hub │ (Server Mode) │ (Cloud/DC) │ │ • All writes │ └────────┬─────────┘ │ gRPC replication ┌────────────────┼────────────────┐ ↓ ↓ ↓ ┌─────────┐ ┌─────────┐ ┌─────────┐ │ Edge 1 │ │ Edge 2 │ │ Edge 3 │ (Relayers) │ Store A │ │ Store B │ │ Store C │ └────┬────┘ └────┬────┘ └────┬────┘ │ Watch API │ Watch API │ Watch API ↓ ↓ ↓ ┌─────────┐ ┌─────────┐ ┌─────────┐ │ POS │ │ POS │ │ POS │ Local apps │ Inv Mgmt│ │ Inv Mgmt│ │ Inv Mgmt│ (subscribers) │ Display │ │ Display │ │ Display │ └─────────┘ └─────────┘ └─────────┘Configuration# Hub (central server):\n[server] mode = \u0026#34;server\u0026#34; data_dir = \u0026#34;/data/unisondb\u0026#34; http_addr = \u0026#34;0.0.0.0:8080\u0026#34; grpc_addr = \u0026#34;0.0.0.0:9090\u0026#34; [namespaces] # Different namespaces for different data types inventory = { wal_segment_size = \u0026#34;16MB\u0026#34; } orders = { wal_segment_size = \u0026#34;8MB\u0026#34; } analytics = { wal_segment_size = \u0026#34;32MB\u0026#34; } [replication] enabled = true tls_enabled = true max_connections = 1000 # Support many edge nodesEdge relayer (edge-store-001.toml):\n[server] mode = \u0026#34;relayer\u0026#34; data_dir = \u0026#34;/data/unisondb\u0026#34; http_addr = \u0026#34;127.0.0.1:8080\u0026#34; # Local only [relayer] upstreams = [\u0026#34;hub.central.example.com:9090\u0026#34;] tls_enabled = true reconnect_interval = \u0026#34;5s\u0026#34; buffer_size = \u0026#34;100MB\u0026#34; # Handle disconnections # Edge nodes publish local watch events [watch] enabled = true namespaces = [\u0026#34;inventory\u0026#34;, \u0026#34;orders\u0026#34;] # Only needed namespaces bind_addr = \u0026#34;tcp://127.0.0.1:5555\u0026#34; # Local IPC only buffer_size = 5000Local application (subscribing to watch events):\n# Python app running on edge device import zmq context = zmq.Context() socket = context.socket(zmq.SUB) socket.connect(\u0026#34;tcp://localhost:5555\u0026#34;) socket.setsockopt_string(zmq.SUBSCRIBE, \u0026#34;inventory:\u0026#34;) # Filter namespace while True: event = socket.recv_json() # {\u0026#34;namespace\u0026#34;: \u0026#34;inventory\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;item:12345\u0026#34;, # \u0026#34;entry_type\u0026#34;: \u0026#34;PUT\u0026#34;, \u0026#34;seq\u0026#34;: 1042} if event[\u0026#34;entry_type\u0026#34;] == \u0026#34;PUT\u0026#34;: # Invalidate local cache cache.invalidate(event[\u0026#34;key\u0026#34;]) # Update display update_inventory_display(event[\u0026#34;key\u0026#34;])Use Cases# Retail:\nCentral inventory management Edge stores sync locally POS systems react to inventory changes IoT:\nCentral data aggregation Edge devices with local caches Real-time sensor data processing CDN-like distribution:\nCentral content updates Edge nodes serve local users Low-latency local reads When to Use# ✅ Good for:\nEdge computing (retail stores, warehouses, factories) IoT deployments with central coordination Applications requiring data locality Intermittent connectivity scenarios ❌ Avoid for:\nLow-latency writes from edge (writes go to hub) Strong consistency requirements at edge Edge-to-edge communication needs 4. Multi-Hop Relay (Deep Edge)# Hierarchical replication for deep edge deployments or bandwidth-constrained networks.\n┌──────────────┐ │ Primary │ (Server Mode - Cloud) │ (Cloud) │ └──────┬───────┘ │ gRPC ↓ ┌──────────────┐ │ Tier 1 │ (Relayer - Regional DC) │ Regional │ └──────┬───────┘ │ gRPC ┌───────┴────────┐ ↓ ↓ ┌─────────┐ ┌─────────┐ │ Tier 2 │ │ Tier 2 │ (Relayer - Edge Cluster) │ West │ │ East │ └────┬────┘ └────┬────┘ │ │ ┌───┴───┐ ┌───┴───┐ ↓ ↓ ↓ ↓ Tier 3 Tier 3 Tier 3 Tier 3 (Relayer - Leaf Nodes) Store1 Store2 Store3 Store4 ↓ ↓ ↓ ↓ Local Local Local Local Apps Apps Apps AppsConfiguration# Tier 1 (Regional relayer):\n[server] mode = \u0026#34;relayer\u0026#34; data_dir = \u0026#34;/data/unisondb\u0026#34; grpc_addr = \u0026#34;0.0.0.0:9090\u0026#34; # Accept downstream connections [relayer] upstreams = [\u0026#34;primary.cloud.example.com:9090\u0026#34;] # This relayer can also relay to downstream enable_relay = true tls_enabled = trueTier 2 (Edge cluster relayer):\n[server] mode = \u0026#34;relayer\u0026#34; data_dir = \u0026#34;/data/unisondb\u0026#34; grpc_addr = \u0026#34;0.0.0.0:9090\u0026#34; [relayer] upstreams = [\u0026#34;tier1-regional.example.com:9090\u0026#34;] enable_relay = true # Relay to Tier 3 tls_enabled = trueTier 3 (Leaf relayer):\n[server] mode = \u0026#34;relayer\u0026#34; data_dir = \u0026#34;/data/unisondb\u0026#34; http_addr = \u0026#34;127.0.0.1:8080\u0026#34; [relayer] upstreams = [\u0026#34;tier2-west.example.com:9090\u0026#34;] enable_relay = false # Leaf node, no downstream tls_enabled = true [watch] enabled = true bind_addr = \u0026#34;tcp://127.0.0.1:5555\u0026#34;When to Use# ✅ Good for:\nHierarchical networks (corporate, military, logistics) Bandwidth-constrained links (satellite, cellular) CDN-like distribution patterns Multi-tier edge deployments ❌ Avoid for:\nLow-latency requirements (multi-hop adds latency) Simple topologies (unnecessary complexity) Flat network architectures Operational Considerations# Aspect Impact Replication lag Compounds at each tier (monitor carefully) Network partitions Tier isolation (regional failures don\u0026rsquo;t affect others) Bandwidth usage Reduced (1 upstream stream → many downstream) Complexity Higher operational overhead 5. Hybrid: Replication + Local Events# Combines durable replication with local event-driven applications.\n┌───────────────────────────────────┐ │ Primary Server │ │ (Server Mode) │ │ │ │ ┌───────────────────┐ │ │ │ Storage Engine │ │ │ └─────────┬─────────┘ │ │ │ │ │ ┌───────┴────────┐ │ │ ↓ ↓ │ │ [gRPC] [Watch API] │ │ :9090 :5555 │ └────┬──────────────────┬───────────┘ │ │ │ └──────┐ ↓ ↓ ┌─────────┐ ┌─────────────┐ │ Remote │ │ Local Apps │ │Relayers │ │ │ └─────────┘ │ • Cache │ │ • Analytics │ │ • Audit Log │ │ • Dashboard │ └─────────────┘Configuration# Primary with both channels:\n[server] mode = \u0026#34;server\u0026#34; data_dir = \u0026#34;/data/unisondb\u0026#34; http_addr = \u0026#34;0.0.0.0:8080\u0026#34; grpc_addr = \u0026#34;0.0.0.0:9090\u0026#34; # gRPC replication for remote relayers [replication] enabled = true tls_enabled = true # Watch API for local applications [watch] enabled = true transport = \u0026#34;zeromq\u0026#34; namespaces = [\u0026#34;users\u0026#34;, \u0026#34;sessions\u0026#34;, \u0026#34;metrics\u0026#34;] # Per-namespace watch configuration [watch.users] bind_addr = \u0026#34;tcp://*:5555\u0026#34; buffer_size = 10000 [watch.sessions] bind_addr = \u0026#34;tcp://*:5556\u0026#34; buffer_size = 20000 # High-frequency updates [watch.metrics] bind_addr = \u0026#34;tcp://*:5557\u0026#34; buffer_size = 50000Local Application Examples# Cache invalidation service:\n// Go service that invalidates Redis cache on changes package main import ( \u0026#34;context\u0026#34; \u0026#34;encoding/json\u0026#34; zmq \u0026#34;github.com/pebbe/zmq4\u0026#34; \u0026#34;github.com/redis/go-redis/v9\u0026#34; ) type WatchEvent struct { Namespace string `json:\u0026#34;namespace\u0026#34;` Key string `json:\u0026#34;key\u0026#34;` EntryType string `json:\u0026#34;entry_type\u0026#34;` Seq int64 `json:\u0026#34;seq\u0026#34;` } func main() { // Subscribe to UnisonDB watch events subscriber, _ := zmq.NewSocket(zmq.SUB) subscriber.Connect(\u0026#34;tcp://localhost:5555\u0026#34;) subscriber.SetSubscribe(\u0026#34;users:\u0026#34;) // Only user namespace // Redis client rdb := redis.NewClient(\u0026amp;redis.Options{ Addr: \u0026#34;localhost:6379\u0026#34;, }) for { msg, _ := subscriber.RecvBytes(0) var event WatchEvent json.Unmarshal(msg, \u0026amp;event) // Invalidate Redis cache rdb.Del(context.Background(), event.Key) log.Printf(\u0026#34;Invalidated cache for %s\u0026#34;, event.Key) } }Real-time analytics aggregator:\n# Python service aggregating metrics import zmq import time from collections import defaultdict context = zmq.Context() socket = context.socket(zmq.SUB) socket.connect(\u0026#34;tcp://localhost:5557\u0026#34;) socket.setsockopt_string(zmq.SUBSCRIBE, \u0026#34;metrics:\u0026#34;) # Aggregate metrics every 5 seconds metrics = defaultdict(int) last_flush = time.time() while True: try: event = socket.recv_json(flags=zmq.NOBLOCK) metrics[event[\u0026#34;key\u0026#34;]] += 1 except zmq.Again: pass # Flush every 5 seconds if time.time() - last_flush \u0026gt; 5: publish_to_dashboard(metrics) metrics.clear() last_flush = time.time()When to Use# ✅ Good for:\nMicroservices architectures Event-driven applications Real-time dashboards and monitoring Cache invalidation workflows Applications needing both local reactivity and durability ❌ Avoid for:\nSimple applications (unnecessary complexity) No local event-driven needs Operational Best Practices# Monitoring# Key metrics to track:\nMetric Description Alert Threshold replication_lag_seconds Time behind primary \u0026gt; 60s wal_segments_behind Segments not yet applied \u0026gt; 10 watch_events_dropped Events lost due to backpressure \u0026gt; 0 (investigate) grpc_connections_active Active relayer connections \u0026lt; expected http_request_latency_p99 Read/write latency \u0026gt; 100ms Prometheus metrics endpoint:\ncurl http://localhost:8080/metrics # Example output: unisondb_replication_lag_seconds{upstream=\u0026#34;primary\u0026#34;} 2.5 unisondb_wal_segments_behind{upstream=\u0026#34;primary\u0026#34;} 0 unisondb_watch_events_total{namespace=\u0026#34;users\u0026#34;} 150234 unisondb_watch_events_dropped{namespace=\u0026#34;users\u0026#34;} 0 unisondb_watch_subscribers{namespace=\u0026#34;users\u0026#34;} 3Security# TLS/mTLS for replication:\n# Generate CA openssl genrsa -out ca.key 4096 openssl req -new -x509 -key ca.key -out ca.crt -days 3650 # Generate server cert openssl genrsa -out server.key 4096 openssl req -new -key server.key -out server.csr openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key \\ -CAcreateserial -out server.crt -days 365 # Generate client cert (for relayers) openssl genrsa -out client.key 4096 openssl req -new -key client.key -out client.csr openssl x509 -req -in client.csr -CA ca.crt -CAkey ca.key \\ -CAcreateserial -out client.crt -days 365Watch API security:\nBind to 127.0.0.1 for local-only access Use firewall rules for network isolation Consider authentication layer for sensitive namespaces Backup and Recovery# Primary server backup:\n# Backup WAL and snapshot unisondb backup --data-dir /data/unisondb \\ --output /backup/unisondb-$(date +%Y%m%d).tar.gz # Restore from backup unisondb restore --input /backup/unisondb-20250108.tar.gz \\ --data-dir /data/unisondbRelayer recovery:\n# Relayers auto-recover by catching up from primary # Monitor replication lag during recovery watch -n 1 \u0026#39;curl -s http://localhost:8080/stats/replication\u0026#39; Choosing the Right Topology# Decision Matrix# Requirement Recommended Topology Development/Testing Single Server Read-heavy workload Primary + Replicas Geographic distribution Primary + Replicas or Hub-and-Spoke Edge computing Hub-and-Spoke IoT deployments Hub-and-Spoke or Multi-Hop Microservices + events Hybrid Bandwidth constraints Multi-Hop High availability (reads) Primary + Replicas Local event reactivity Any with Watch API enabled Scalability Limits# Component Limit Notes Primary writes ~10K writes/sec Single primary bottleneck Relayers per primary ~1000 gRPC connection limit Watch subscribers ~100 per namespace Transport dependent Namespaces Unlimited Independent isolation Replication lag Typically \u0026lt; 1s Network dependent Next Steps# Configuration Reference - Detailed config options HTTP API - API documentation Architecture Overview - System design details "},{"id":7,"href":"/docs/examples/","title":"Examples","section":"Documentation","content":"Examples# Explore practical examples that showcase UnisonDB’s capabilities — from single-node setups to multi-datacenter replication and real-time CRDT synchronization.\n1. Building Conflict-Free Multi-Datacenter Systems with CRDTs and UnisonDB # "},{"id":8,"href":"/docs/examples/multi-dc-crdts/","title":"Multi-DC CRDT Replication","section":"Examples","content":"Building Real-Time Multi-Datacenter Applications with CRDTs and Edge Notifications in UnisonDB# Introduction: The Challenge of Distributed State Management# Imagine you\u0026rsquo;re building a globally distributed application where users across different continents need to see consistent data think user presence status, live dashboards, or real-time collaboration features. Traditional databases force you to choose between consistency and availability, but what if there was a better way?\nConflict-free Replicated Data Types (CRDTs) offer a mathematical approach to distributed state management where conflicts are resolved automatically through well-defined merge operations. When combined with edge notifications, you get a powerful pattern: write anywhere, replicate everywhere, and get notified of changes in real-time.\nIn this post, we\u0026rsquo;ll build a multi-datacenter system using UnisonDB that demonstrates:\nConcurrent writes to multiple datacenters Automatic conflict resolution using CRDTs Real-time change notifications via ZeroMQ Eventual consistency across all nodes Architecture Overview# Our demo system consists of three UnisonDB nodes:\n+---------------------------------------------------------------+ | Multi-DC CRDT Architecture | +---------------------------------------------------------------+ Writes Writes | | v v +----------------+ +----------------+ | Datacenter 1 | | Datacenter 2 | | (Primary) | | (Primary) | | | | | | HTTP: 8001 | | HTTP: 8002 | | gRPC: 4001 | | gRPC: 4002 | +--------+-------+ +--------+-------+ | | | gRPC Replication | +---------------------+-------------------+ | v +---------------------+ | Relayer | | (Read-Only) | | | | HTTP: 8003 | | ZMQ dc1: 5555 ---\u0026gt; |----+ | ZMQ dc2: 5556 ---\u0026gt; |----+ Change +---------------------+ | Notifications | v +--------------------+ | CRDT Client | | (Go / Node.js) | | | | Converged State | +--------------------+Component Roles# Component Role Namespace HTTP Port gRPC Port ZMQ Ports DC1 Primary (accepts writes) ad-campaign-dc1 8001 4001 - DC2 Primary (accepts writes) ad-campaign-dc2 8002 4002 - Relayer Read-only replica ad-campaign-dc1, ad-campaign-dc2 8003 - 5555, 5556 Building and Running UnisonDB# Prerequisites# # Ensure you have Go 1.21+ and CGO enabled go version # go version go1.21.0 or higherStep 1: Build UnisonDB# # Clone the repository git clone https://github.com/ankur-anand/unisondb.git cd unisondb # Build the binary (CGO required for RocksDB) CGO_ENABLED=1 go build -o unisondb ./cmd/unisondbStep 2: Start the Multi-DC Cluster# Open three separate terminal windows and run:\nTerminal 1: Start Datacenter 1\n./unisondb -config .cmd/examples/crdt-multi-dc/configs/dc1.toml replicatorTerminal 2: Start Datacenter 2\n./unisondb -config .cmd/examples/crdt-multi-dc/configs/dc2.toml replicatorTerminal 3: Start Relayer\n./unisondb -config .cmd/examples/crdt-multi-dc/configs/relayer.toml relayerYou should see output indicating each node is ready:\nINFO: HTTP server listening on :8001 INFO: gRPC server listening on :4001 INFO: Namespace \u0026#39;ad-campaign-dc1\u0026#39; initializedStep 3: Start the CRDT Client# Open a fourth terminal to run the client that will observe CRDT state:\ncd cmd/examples/golang-crdt-client go run main.goExpected output:\nWaiting for change notifications... Connecting to ZeroMQ ad-campaign-dc1: tcp://localhost:5555 Connecting to ZeroMQ ad-campaign-dc2: tcp://localhost:5556 ZeroMQ listener started for namespace: ad-campaign-dc1 ZeroMQ listener started for namespace: ad-campaign-dc2Your system is now ready!\nUnderstanding CRDTs: Two Types in Action# 1. LWW-Register (Last-Write-Wins Register)# Use Cases: User profiles, configuration settings, feature flags\nHow it works:\nEach write includes a timestamp and replica ID Conflicts are resolved by choosing the write with the latest timestamp If timestamps are equal, the lexicographically higher replica ID wins Data Format:\n{ \u0026#34;value\u0026#34;: \u0026#34;actual data\u0026#34;, \u0026#34;timestamp\u0026#34;: 1698765432000, \u0026#34;replica\u0026#34;: \u0026#34;ad-campaign-dc1\u0026#34; }2. G-Counter (Grow-Only Counter)# Use Cases: Page views, API calls, distributed metrics (monotonically increasing)\nHow it works:\nEach replica maintains its own counter Merging takes the maximum count per replica Total value is the sum of all replica counters Can only increase (never decrease) Data Format:\n{ \u0026#34;replica\u0026#34;: \u0026#34;ad-campaign-dc1\u0026#34;, \u0026#34;count\u0026#34;: 5 }Demo Scenarios with curl Examples# Scenario 1: Basic LWW-Register Update# Let\u0026rsquo;s update a user\u0026rsquo;s status across two datacenters:\nWrite \u0026ldquo;online\u0026rdquo; to DC1 (timestamp: 1698765432000)\ncurl -X PUT \u0026#34;http://localhost:8001/api/v1/ad-campaign-dc1/kv/lww:user-status\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#39;{\u0026#34;value\u0026#34;:\u0026#34;online\u0026#34;,\u0026#34;timestamp\u0026#34;:1698765432000,\u0026#34;replica\u0026#34;:\u0026#34;ad-campaign-dc1\u0026#34;}\u0026#39; | base64)\u0026#39;\u0026#34;}\u0026#39;Client Output:\nChange notification received Topic: ad-campaign-dc1.kv Key: lww:user-status Operation: put Processing update: lww:user-status LWW-Register updated: lww:user-status Value: online Timestamp: 1698765432000 Replica: ad-campaign-dc1 CURRENT CRDT STATE LWW-Registers: lww:user-status: Value: online Timestamp: 1698765432000 Replica: ad-campaign-dc1Now write \u0026ldquo;away\u0026rdquo; to DC2 with a newer timestamp:\nWrite \u0026ldquo;away\u0026rdquo; to DC2 (timestamp: 1698765433000)\ncurl -X PUT \u0026#34;http://localhost:8002/api/v1/ad-campaign-dc2/kv/lww:user-status\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#39;{\u0026#34;value\u0026#34;:\u0026#34;away\u0026#34;,\u0026#34;timestamp\u0026#34;:1698765433000,\u0026#34;replica\u0026#34;:\u0026#34;ad-campaign-dc2\u0026#34;}\u0026#39; | base64)\u0026#39;\u0026#34;}\u0026#39;Client Output:\nChange notification received Topic: ad-campaign-dc2.kv Key: lww:user-status Operation: put Processing update: lww:user-status LWW-Register updated: lww:user-status Value: away Timestamp: 1698765433000 Replica: ad-campaign-dc2 CURRENT CRDT STATE LWW-Registers: lww:user-status: Value: away Timestamp: 1698765433000 Replica: ad-campaign-dc2What happened? The client automatically resolved the conflict! DC2\u0026rsquo;s write won because it had a newer timestamp (1698765433000 \u0026gt; 1698765432000).\nScenario 2: Concurrent Writes with Same Timestamp# What happens when two datacenters write at the exact same millisecond?\nWrite to DC1:\nTIMESTAMP=$(date +%s)000 curl -X PUT \u0026#34;http://localhost:8001/api/v1/ad-campaign-dc1/kv/lww:config\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#34;{\\\u0026#34;value\\\u0026#34;:\\\u0026#34;DC1 wins?\\\u0026#34;,\\\u0026#34;timestamp\\\u0026#34;:$TIMESTAMP,\\\u0026#34;replica\\\u0026#34;:\\\u0026#34;ad-campaign-dc1\\\u0026#34;}\u0026#34; | base64)\u0026#39;\u0026#34;}\u0026#39;Write to DC2 (same timestamp):\ncurl -X PUT \u0026#34;http://localhost:8002/api/v1/ad-campaign-dc2/kv/lww:config\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#34;{\\\u0026#34;value\\\u0026#34;:\\\u0026#34;DC2 wins!\\\u0026#34;,\\\u0026#34;timestamp\\\u0026#34;:$TIMESTAMP,\\\u0026#34;replica\\\u0026#34;:\\\u0026#34;ad-campaign-dc2\\\u0026#34;}\u0026#34; | base64)\u0026#39;\u0026#34;}\u0026#39;Result: ad-campaign-dc2 wins because lexicographically \u0026quot;ad-campaign-dc2\u0026quot; \u0026gt; \u0026quot;ad-campaign-dc1\u0026quot;. This ensures deterministic conflict resolution across all replicas.\nScenario 3: Distributed Counter (G-Counter)# Let\u0026rsquo;s track page views across two datacenters:\nDC1 serves 5 requests:\ncurl -X PUT \u0026#34;http://localhost:8001/api/v1/ad-campaign-dc1/kv/counter:page-views\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#39;{\u0026#34;replica\u0026#34;:\u0026#34;ad-campaign-dc1\u0026#34;,\u0026#34;count\u0026#34;:5}\u0026#39; | base64)\u0026#39;\u0026#34;}\u0026#39;DC2 serves 3 requests:\ncurl -X PUT \u0026#34;http://localhost:8002/api/v1/ad-campaign-dc2/kv/counter:page-views\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#39;{\u0026#34;replica\u0026#34;:\u0026#34;ad-campaign-dc2\u0026#34;,\u0026#34;count\u0026#34;:3}\u0026#39; | base64)\u0026#39;\u0026#34;}\u0026#39;Client Output:\nCURRENT CRDT STATE G-Counters: counter:page-views: Replica Counts: {\u0026#34;ad-campaign-dc1\u0026#34;:5,\u0026#34;ad-campaign-dc2\u0026#34;:3} Total: 8Result: Total = 8 (5 from DC1 + 3 from DC2). The counters from both datacenters are automatically merged!\nScenario 4: Out-of-Order Delivery (Stale Write)# What if network delays cause an old write to arrive after a newer one?\nWrite NEW value to DC1:\ncurl -X PUT \u0026#34;http://localhost:8001/api/v1/ad-campaign-dc1/kv/lww:feature-flag\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#39;{\u0026#34;value\u0026#34;:true,\u0026#34;timestamp\u0026#34;:2000,\u0026#34;replica\u0026#34;:\u0026#34;ad-campaign-dc1\u0026#34;}\u0026#39; | base64)\u0026#39;\u0026#34;}\u0026#39;Write OLD value to DC2 (stale):\ncurl -X PUT \u0026#34;http://localhost:8002/api/v1/ad-campaign-dc2/kv/lww:feature-flag\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#39;{\u0026#34;value\u0026#34;:false,\u0026#34;timestamp\u0026#34;:1000,\u0026#34;replica\u0026#34;:\u0026#34;ad-campaign-dc2\u0026#34;}\u0026#39; | base64)\u0026#39;\u0026#34;}\u0026#39;Client Output:\nProcessing update: lww:feature-flag LWW-Register ignored (stale): lww:feature-flag Incoming timestamp: 1000 Current timestamp: 2000Result: The stale write is automatically ignored. The CRDT logic ensures we never regress to an older state!\nScenario 5: Multiple Counters Operating Independently# # Track different metrics curl -X PUT \u0026#34;http://localhost:8001/api/v1/ad-campaign-dc1/kv/counter:api-calls\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#39;{\u0026#34;replica\u0026#34;:\u0026#34;ad-campaign-dc1\u0026#34;,\u0026#34;count\u0026#34;:100}\u0026#39; | base64)\u0026#39;\u0026#34;}\u0026#39; curl -X PUT \u0026#34;http://localhost:8002/api/v1/ad-campaign-dc2/kv/counter:api-calls\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#39;{\u0026#34;replica\u0026#34;:\u0026#34;ad-campaign-dc2\u0026#34;,\u0026#34;count\u0026#34;:75}\u0026#39; | base64)\u0026#39;\u0026#34;}\u0026#39; curl -X PUT \u0026#34;http://localhost:8001/api/v1/ad-campaign-dc1/kv/counter:db-queries\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#39;{\u0026#34;replica\u0026#34;:\u0026#34;ad-campaign-dc1\u0026#34;,\u0026#34;count\u0026#34;:250}\u0026#39; | base64)\u0026#39;\u0026#34;}\u0026#39;Client Output:\nCURRENT CRDT STATE G-Counters: counter:api-calls: Replica Counts: {\u0026#34;ad-campaign-dc1\u0026#34;:100,\u0026#34;ad-campaign-dc2\u0026#34;:75} Total: 175 counter:db-queries: Replica Counts: {\u0026#34;ad-campaign-dc1\u0026#34;:250} Total: 250Each counter operates independently with its own convergence!\nReading Data from the Relayer# The relayer provides read-only access to both datacenter namespaces:\nRead from DC1 namespace:\ncurl \u0026#34;http://localhost:8003/api/v1/ad-campaign-dc1/kv/lww:user-status\u0026#34; | jqResponse:\n{ \u0026#34;value\u0026#34;: \u0026#34;eyJ2YWx1ZSI6ImF3YXkiLCJ0aW1lc3RhbXAiOjE2OTg3NjU0MzMwMDAsInJlcGxpY2EiOiJhZC1jYW1wYWlnbi1kYzIifQ==\u0026#34;, \u0026#34;found\u0026#34;: true }Decode the base64 value:\necho \u0026#34;eyJ2YWx1ZSI6ImF3YXkiLCJ0aW1lc3RhbXAiOjE2OTg3NjU0MzMwMDAsInJlcGxpY2EiOiJhZC1jYW1wYWlnbi1kYzIifQ==\u0026#34; | base64 -d | jqOutput:\n{ \u0026#34;value\u0026#34;: \u0026#34;away\u0026#34;, \u0026#34;timestamp\u0026#34;: 1698765433000, \u0026#34;replica\u0026#34;: \u0026#34;ad-campaign-dc2\u0026#34; }How Conflict Resolution Works Under the Hood# LWW-Register Algorithm# The conflict resolution logic in lww_register.go:30-39:\nfunc (r *LWWRegister) Update(value interface{}, timestamp int64, replica string) bool { // Rule 1: Accept if timestamp is newer if timestamp \u0026gt; r.Timestamp { r.Value = value r.Timestamp = timestamp r.Replica = replica return true } // Rule 2: If timestamps equal, use replica ID as tiebreaker if timestamp == r.Timestamp \u0026amp;\u0026amp; replica \u0026gt; r.Replica { r.Value = value r.Replica = replica return true } // Rule 3: Reject stale updates return false }Key Properties:\nCommutative: Order of updates doesn\u0026rsquo;t matter Associative: Grouping of updates doesn\u0026rsquo;t matter Idempotent: Applying the same update multiple times is safe Deterministic: All replicas converge to the same value G-Counter Merge Algorithm# The merge logic in g_counter.go:\nfunc (c *GCounter) Merge(replica string, count int64) bool { current := c.Counts[replica] // Only accept higher counts (monotonic) if count \u0026gt; current { c.Counts[replica] = count return true } return false } func (c *GCounter) GetValue() int64 { total := int64(0) for _, count := range c.Counts { total += count } return total }Key Properties:\nMonotonic: Values only increase Convergent: All replicas reach the same total Partition-tolerant: Works across network splits Real-World Use Cases# 1. User Presence System# # User goes online in US datacenter curl -X PUT \u0026#34;http://localhost:8001/api/v1/ad-campaign-dc1/kv/lww:user:alice:status\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#34;{\\\u0026#34;value\\\u0026#34;:\\\u0026#34;online\\\u0026#34;,\\\u0026#34;timestamp\\\u0026#34;:$(date +%s)000,\\\u0026#34;replica\\\u0026#34;:\\\u0026#34;us-east-1\\\u0026#34;}\u0026#34; | base64)\u0026#39;\u0026#34;}\u0026#39; # User goes away in EU datacenter (newer timestamp wins) curl -X PUT \u0026#34;http://localhost:8002/api/v1/ad-campaign-dc2/kv/lww:user:alice:status\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#34;{\\\u0026#34;value\\\u0026#34;:\\\u0026#34;away\\\u0026#34;,\\\u0026#34;timestamp\\\u0026#34;:$(($(date +%s)+5))000,\\\u0026#34;replica\\\u0026#34;:\\\u0026#34;eu-west-1\\\u0026#34;}\u0026#34; | base64)\u0026#39;\u0026#34;}\u0026#39;All clients worldwide see the latest status in real-time!\n2. Distributed Analytics# # Track impressions across regions curl -X PUT \u0026#34;http://localhost:8001/api/v1/ad-campaign-dc1/kv/counter:campaign-123:impressions\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#39;{\u0026#34;replica\u0026#34;:\u0026#34;us-east-1\u0026#34;,\u0026#34;count\u0026#34;:1500}\u0026#39; | base64)\u0026#39;\u0026#34;}\u0026#39; curl -X PUT \u0026#34;http://localhost:8002/api/v1/ad-campaign-dc2/kv/counter:campaign-123:impressions\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#39;{\u0026#34;replica\u0026#34;:\u0026#34;eu-west-1\u0026#34;,\u0026#34;count\u0026#34;:2300}\u0026#39; | base64)\u0026#39;\u0026#34;}\u0026#39; # Global total: 3800 impressions3. Feature Flags# # Enable feature in production curl -X PUT \u0026#34;http://localhost:8001/api/v1/ad-campaign-dc1/kv/lww:feature:new-ui\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#34;{\\\u0026#34;value\\\u0026#34;:true,\\\u0026#34;timestamp\\\u0026#34;:$(date +%s)000,\\\u0026#34;replica\\\u0026#34;:\\\u0026#34;control-plane\\\u0026#34;}\u0026#34; | base64)\u0026#39;\u0026#34;}\u0026#39;Feature flag changes propagate globally within milliseconds!\nTry It Yourself# # Clone and run the example git clone https://github.com/ankur-anand/unisondb.git cd unisondb CGO_ENABLED=1 go build -o unisondb ./cmd/unisondb # Start the demo cd cmd/examples/crdt-multi-dcWatch the magic happen as conflicts resolve themselves and state converges across datacenters!\nAdditional Resources# UnisonDB GitHub Repository CRDT Research Papers ZeroMQ Guide Have questions or want to contribute? Open an issue on GitHub or join our community discussions!\nBuilt with UnisonDB.\n"}]