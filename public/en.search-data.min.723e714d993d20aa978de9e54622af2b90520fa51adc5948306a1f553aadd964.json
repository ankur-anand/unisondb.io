[{"id":0,"href":"/blog/","title":"Blog","section":"UnisonDB – Log-Native Database For AI And Edge Computing","content":"UnisonDB Blog# Welcome to the UnisonDB Blog — your source for updates, insights, and technical deep dives on log-native databases, Edge AI, and real-time distributed systems.\nDiscover how UnisonDB brings together database durability and message-bus-style replication to power the next generation of edge computing and AI-driven infrastructure.\n"},{"id":1,"href":"/docs/","title":"UnisonDB Documentation","section":"UnisonDB – Log-Native Database For AI And Edge Computing","content":"UnisonDB Documentation# This guide covers everything you need to install, configure, and operate UnisonDB at scale.\nDocumentation Sections# Getting Started # Step-by-step installation and quick-start setup for UnisonDB, including replicator and relayer modes.\nArchitecture # Dive into UnisonDB’s core concepts — log-native storage, WAL streaming replication, and edge-first design.\nDeployment # Explore deployment topologies and best practices for production, including hub-and-spoke and multi-region setups.\nAPI Reference # Detailed HTTP documentation for integrating UnisonDB with your applications and services.\n"},{"id":2,"href":"/docs/getting-started/","title":"Getting Started","section":"UnisonDB Documentation","content":"Getting Started with UnisonDB# This guide will walk you through installing UnisonDB, configuring it, and running it in both Replicator and Relayer modes.\n┌────────────────┐ │ Replicator │ │ (Primary) │ │ Writes → WAL │ │ Streams gRPC │ └──────┬─────────┘ │ WAL Stream (gRPC) │ ┌─────────────┴──────────────┐ ↓ ↓ ┌───────────┐ ┌───────────┐ │ Relayer 1 │ │ Relayer 2 │ │ (Replica) │ │ (Replica) │ │ Local DB │ │ Local DB │ │ Watch API │ │ Watch API │ └───────────┘ └───────────┘Table of Contents# Prerequisites Installation Running in Server Mode Running in Relayer Mode Prerequisites# System Requirements# Operating System: Linux or macOS Go: Version 1.24 or higher Installation# Building from Source# UnisonDB requires CGO to be enabled for LMDB bindings.\n# Clone the repository git clone https://github.com/ankur-anand/unisondb.git cd unisondb # Enable CGO (required for LMDB) export CGO_ENABLED=1 # Build the binary go build -o unisondb ./cmd/unisondb # Verify installation ./unisondb --helpExpected output:\n_ _ _ ___ ___ | | | | _ _ (_) ___ ___ _ _ | \\ | _ ) | |_| | | \u0026#39; \\ | | (_-\u0026lt; / _ \\ | \u0026#39; \\ | |) | | _ \\ \\___/ |_||_| |_| /__/ \\___/ |_||_| |___/ |___/ Database + Message Bus. Built for Edge. https://unisondb.io NAME: unisondb - Run UnisonDB USAGE: unisondb [global options] command [command options] COMMANDS: replicator Run in replicator mode relayer Run in relayer mode fuzzer This is a testing-only feature (disabled in production builds) help, h Shows a list of commands or help for one command GLOBAL OPTIONS: --config value, -c value Path to TOML config file (default: \u0026#34;./config.toml\u0026#34;) [$UNISON_CONFIG] --env value, -e value Environment: dev, staging, prod (default: \u0026#34;dev\u0026#34;) [$UNISON_ENV] --grpc, -G Enable gRPC server in Relayer Mode (default: false) [$UNISON_GRPC_ENABLED] --help, -h show helpBuilding with Fuzzer Support (Optional)# For testing and development, you can build with fuzzer support:\nCGO_ENABLED=1 go build -tags fuzz -o unisondb ./cmd/unisondbNote: When built with -tags fuzz:\nfuzzer command is available replicator command is disabled for safety To run fuzzer mode:\n./unisondb --config config.toml fuzzerInstallation to System Path (Optional)# # Move to system path sudo mv unisondb /usr/local/bin/ # Verify unisondb --helpRunning in Replicator Mode# Replicator Mode runs UnisonDB as a primary instance that accepts writes and serves reads.\n1. Generate TLS Certificates (Recommended)# For production or multi-node setups, generate TLS certificates for gRPC:\nUsing OpenSSL:\nmkdir -p certs \u0026amp;\u0026amp; cd certs # Generate CA openssl genrsa -out ca.key 4096 openssl req -new -x509 -key ca.key -sha256 -subj \u0026#34;/CN=UnisonDB CA\u0026#34; -days 365 -out ca.crt # Generate server certificate openssl genrsa -out server.key 4096 openssl req -new -key server.key -out server.csr -subj \u0026#34;/CN=localhost\u0026#34; openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out server.crt -days 365 -sha256 # Generate client certificate openssl genrsa -out client.key 4096 openssl req -new -key client.key -out client.csr -subj \u0026#34;/CN=client\u0026#34; openssl x509 -req -in client.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out client.crt -days 365 -sha256 cd ..2. Create Server Configuration# Create a server.toml configuration file:\n## HTTP API port http_port = 4000 listen_ip = \u0026#34;0.0.0.0\u0026#34; ## gRPC configuration (for replication) [grpc_config] listen_ip = \u0026#34;0.0.0.0\u0026#34; port = 4001 cert_path = \u0026#34;./certs/server.crt\u0026#34; key_path = \u0026#34;./certs/server.key\u0026#34; ca_path = \u0026#34;./certs/ca.crt\u0026#34; # For development only - allows insecure connections allow_insecure = false ## Storage configuration [storage_config] base_dir = \u0026#34;./data/server\u0026#34; namespaces = [\u0026#34;default\u0026#34;, \u0026#34;users\u0026#34;, \u0026#34;products\u0026#34;] bytes_per_sync = \u0026#34;1MB\u0026#34; segment_size = \u0026#34;16MB\u0026#34; arena_size = \u0026#34;4MB\u0026#34; wal_fsync_interval = \u0026#34;1s\u0026#34; ## WAL cleanup (prevents disk exhaustion) [storage_config.wal_cleanup_config] enabled = true interval = \u0026#34;5m\u0026#34; max_age = \u0026#34;1h\u0026#34; min_segments = 5 max_segments = 100 ## Write notification coalescing [write_notify_config] enabled = true max_delay = \u0026#34;20ms\u0026#34; ## ZeroMQ notifications (optional - for local apps) [notifier_config.default] bind_port = 5555 high_water_mark = 1000 linger_time = 1000 [notifier_config.users] bind_port = 5556 high_water_mark = 1000 linger_time = 1000 ## Profiling endpoint [pprof_config] enabled = true port = 6060 ## Logging [log_config] log_level = \u0026#34;info\u0026#34; [log_config.min_level_percents] debug = 100.0 info = 100.0 warn = 100.0 error = 100.03. Start the Replicator Server# Replicator Mode (using replicator command):\n./unisondb --config server.toml replicator4. Verify Server is Running# Check HTTP health endpoint:\ncurl http://localhost:4000/healthDevelopment Mode (Insecure)# For quick local testing without TLS:\nserver-dev.toml:\nhttp_port = 4000 [grpc_config] port = 4001 allow_insecure = true # WARNING: Development only! [storage_config] base_dir = \u0026#34;./data/server\u0026#34; namespaces = [\u0026#34;default\u0026#34;] [log_config] log_level = \u0026#34;debug\u0026#34;./unisondb --config server-dev.toml replicatorRunning in Relayer Mode# Relayer Mode runs UnisonDB as a replica that streams changes from an upstream server.\n1. Create Relayer Configuration# Create a relayer.toml configuration file:\n## HTTP API port (different from server) http_port = 5000 listen_ip = \u0026#34;0.0.0.0\u0026#34; ## gRPC config (can accept downstream relayers) [grpc_config] listen_ip = \u0026#34;0.0.0.0\u0026#34; port = 5001 cert_path = \u0026#34;./certs/server.crt\u0026#34; key_path = \u0026#34;./certs/server.key\u0026#34; ca_path = \u0026#34;./certs/ca.crt\u0026#34; ## Storage configuration [storage_config] base_dir = \u0026#34;./data/relayer\u0026#34; namespaces = [\u0026#34;default\u0026#34;, \u0026#34;users\u0026#34;, \u0026#34;products\u0026#34;] bytes_per_sync = \u0026#34;1MB\u0026#34; # IMPORTANT: segment_size MUST match upstream server! segment_size = \u0026#34;16MB\u0026#34; arena_size = \u0026#34;4MB\u0026#34; ## Relayer configuration - connects to upstream [relayer_config.primary] namespaces = [\u0026#34;default\u0026#34;, \u0026#34;users\u0026#34;, \u0026#34;products\u0026#34;] cert_path = \u0026#34;./certs/client.crt\u0026#34; key_path = \u0026#34;./certs/client.key\u0026#34; ca_path = \u0026#34;./certs/ca.crt\u0026#34; upstream_address = \u0026#34;localhost:4001\u0026#34; segment_lag_threshold = 100 allow_insecure = false ## Optional: Connect to multiple upstreams # [relayer_config.secondary] # namespaces = [\u0026#34;products\u0026#34;] # upstream_address = \u0026#34;other-server:4001\u0026#34; # cert_path = \u0026#34;./certs/client.crt\u0026#34; # key_path = \u0026#34;./certs/client.key\u0026#34; # ca_path = \u0026#34;./certs/ca.crt\u0026#34; ## ZeroMQ notifications (optional) [notifier_config.default] bind_port = 6555 high_water_mark = 1000 linger_time = 1000 ## Logging [log_config] log_level = \u0026#34;info\u0026#34; [log_config.min_level_percents] debug = 1.0 # Sample 1% of debug logs info = 10.0 # Sample 10% of info logs warn = 100.0 error = 100.02. Start the Relayer Server# Start relayer:\n./unisondb --config relayer.toml relayer3. Enable gRPC Server on Relayer (Multi-Hop)# To allow downstream relayers to connect to this relayer:\n./unisondb --config relayer.toml --grpc relayerThis enables the relayer to act as both a consumer (from upstream) and a producer (to downstream).\nDevelopment Mode (Insecure)# relayer-dev.toml:\nhttp_port = 5000 [grpc_config] port = 5001 [storage_config] base_dir = \u0026#34;./data/relayer\u0026#34; namespaces = [\u0026#34;default\u0026#34;] segment_size = \u0026#34;16MB\u0026#34; # Must match server! [relayer_config.primary] namespaces = [\u0026#34;default\u0026#34;] upstream_address = \u0026#34;localhost:4001\u0026#34; allow_insecure = true # WARNING: Development only! segment_lag_threshold = 100 [log_config] log_level = \u0026#34;debug\u0026#34;./unisondb --config relayer-dev.toml relayerBasic Operations# Writing Data# Key-Value Write (via HTTP):\n# Put a key-value pair curl -X POST http://localhost:4000/api/v1/namespaces/default/kv \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;key\u0026#34;: \u0026#34;user:123\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;eyJuYW1lIjoiQWxpY2UiLCJlbWFpbCI6ImFsaWNlQGV4YW1wbGUuY29tIn0=\u0026#34; }\u0026#39;Batch Write:\ncurl -X POST http://localhost:4000/api/v1/namespaces/default/kv/batch \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;operations\u0026#34;: [ {\u0026#34;key\u0026#34;: \u0026#34;user:1\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;...\u0026#34;}, {\u0026#34;key\u0026#34;: \u0026#34;user:2\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;...\u0026#34;}, {\u0026#34;key\u0026#34;: \u0026#34;user:3\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;...\u0026#34;} ] }\u0026#39;Reading Data# Read from Server:\ncurl http://localhost:4000/api/v1/namespaces/default/kv/user:123Read from Relayer (same API):\ncurl http://localhost:5000/api/v1/namespaces/default/kv/user:123Subscribing to Changes (ZeroMQ)# Build UnisonDB with ZeroMQ Lib This needs Zero MQ Installed Make Sure You\u0026rsquo;ve have it Installed. Install ZeroMQ dependency # Clone the repository git clone https://github.com/ankur-anand/unisondb.git cd unisondb # Build the binary (CGO required for RocksDB) CGO_ENABLED=1 go build -tags zeromq ./cmd/unisondbPython example (install pyzmq first):\nimport zmq context = zmq.Context() socket = context.socket(zmq.SUB) # Subscribe to \u0026#39;default\u0026#39; namespace socket.connect(\u0026#34;tcp://localhost:5555\u0026#34;) socket.setsockopt(zmq.SUBSCRIBE, b\u0026#34;\u0026#34;) # Subscribe to all messages print(\u0026#34;Listening for changes on namespace \u0026#39;default\u0026#39;...\u0026#34;) while True: message = socket.recv() print(f\u0026#34;Change notification: {message}\u0026#34;)Run the subscriber:\npython subscriber.pyNow any writes to the default namespace will trigger notifications!\nCommon Deployment Patterns# 1. Single Server (Development)# # Terminal 1: Start server ./unisondb --config server-dev.toml replicator2. Server + Single Relayer (Read Scaling)# # Terminal 1: Start server ./unisondb --config server.toml replicator # Terminal 2: Start relayer ./unisondb --config relayer.toml relayer3. Server + Multiple Relayers (Edge Computing)# # Terminal 1: Start server ./unisondb --config server.toml replicator # Terminal 2: Start relayer 1 ./unisondb --config relayer1.toml relayer # Terminal 3: Start relayer 2 ./unisondb --config relayer2.toml relayer # Terminal 4: Start relayer 3 ./unisondb --config relayer3.toml relayer4. Multi-Hop (Relayer → Relayer)# # Terminal 1: Primary server ./unisondb --config server.toml replicator # Terminal 2: L1 relayer (with gRPC enabled for downstream) ./unisondb --config relayer-l1.toml --grpc relayer # Terminal 3: L2 relayer (connects to L1) # Update relayer-l2.toml upstream_address to point to L1 (localhost:5001) ./unisondb --config relayer-l2.toml relayerHappy building with UnisonDB!\n"},{"id":3,"href":"/docs/architecture/","title":"Architecture Overview","section":"UnisonDB Documentation","content":"Architecture Overview# UnisonDB is a log-native, real-time database that replicates like a message bus — merging transactional consistency with streaming replication. It’s purpose-built for Edge AI , Edge Computing , and local-first distributed systems that demand low-latency synchronization across nodes.\nKey Characteristics# Aspect Design Choice Storage Model Multi-modal: Key-Value, Wide-Column, Large Objects (LOB) Storage Engine B+Tree (LMDB/BoltDB) with in-memory MemTable overlay Replication Log streaming (gRPC) with eventual consistency Watch API Best-effort change notifications (per-namespace) Consistency Single-primary writes, eventual consistency for replicas Durability WAL-first with configurable fsync Deployment Modes Replicator (writable primary) \u0026amp; Relayer (read-only replica) Core Concepts# 1. Log-Native Design# The Write-Ahead Log (WAL) is a first-class citizen, not just a recovery mechanism.\nReplication = Log streaming: No separate replication protocol Recovery = Log replay: State reconstructed from WAL Time-travel enabled: Historic snapshots from log Single source of truth: All operations flow through WAL 2. Operational Modes# UnisonDB instances run in one of two modes:\nReplicator Mode (Primary)# Writable instance that maintains the authoritative WAL.\n┌─────────────────────────────────────┐ │ Replicator Mode Instance │ │ │ │ • Accepts writes (HTTP API) │ │ • Maintains authoritative WAL │ │ • Streams to relayers (gRPC) │ │ • Publishes watch events (local) │ │ • Serves reads from local storage │ └─────────────────────────────────────┘Relayer Mode (Replica)# Read-only instance that streams changes from upstream replicators.\n┌─────────────────────────────────────┐ │ Relayer Mode Instance │ │ │ │ • Connects to upstream (gRPC) │ │ • Receives WAL segment streams │ │ • Applies to local storage (RO) │ │ • Can relay to downstream nodes │ │ • Publishes watch events (local) │ │ • Serves reads (data locality) │ └─────────────────────────────────────┘3. Dual Communication Channels# UnisonDB separates distribution from local reactivity:\nChannel Purpose Use Case Scope gRPC Durable replication Node-to-node WAL streaming Network (cross-machine) Watch API Best-effort notifications Local application reactivity IPC (same machine) Design Rationale:\ngRPC Replication: Network-tolerant, authenticated, durable, at-least-once delivery Watch API: Lightweight, fire-and-forget, at-most-once, local-only System Architecture# ┌────────────────────────────────────────────────────────────────────┐ │ UnisonDB Instance │ ├────────────────────────────────────────────────────────────────────┤ │ Client APIs │ │ ┌──────────────┐ ┌──────────────┐ ┌──────────────┐ │ │ │ HTTP REST │ │ Transactions │ │ Admin/Stats │ │ │ └──────┬───────┘ └──────┬───────┘ └──────┬───────┘ │ │ └─────────────────┴─────────────────┘ │ ├────────────────────────────────────────────────────────────────────┤ │ Storage Models │ │ ┌─────────────┐ ┌──────────────┐ ┌──────────────┐ │ │ │ Key-Value │ │ Wide-Column │ │ Large Object │ │ │ └──────┬──────┘ └──────┬───────┘ └──────┬───────┘ │ │ └────────────────┴─────────────────┘ │ ├────────────────────────────────────────────────────────────────────┤ │ Storage Engine (dbkernel) │ │ │ │ Write: WAL (append) → MemTable → B+Tree (LMDB) │ │ Read: MemTable + B+Tree → Response │ ├────────────────────────────────────────────────────────────────────┤ │ Distribution Layer │ │ ┌──────────────────────┐ ┌──────────────────────┐ │ │ │ gRPC Replicator │ │ Watch API │ │ │ │ • WAL streaming │ │ • Change events │ │ │ │ • TLS/mTLS │ │ • Per-namespace │ │ │ │ • At-least-once │ │ • At-most-once │ │ │ └──────────┬───────────┘ └──────────┬───────────┘ │ │ v v │ │ Remote Relayers Local Applications │ └────────────────────────────────────────────────────────────────────┘Core Components# Write-Ahead Log (WAL)# Append-only transaction log that serves as the source of truth.\nProperty Implementation Structure Segmented files (configurable size, default 16MB) Format Binary with CRC32 checksums per entry Lifecycle Write → fsync → MemTable → B+Tree → Segment cleanup Purpose Durability, replication streaming, crash recovery Segment Format:\n┌─────────────────────────────────┐ │ Header (magic, segment#, ts) │ ├─────────────────────────────────┤ │ Entry: flatbuffer-encoded │ │ Entry: flatbuffer-encoded │ │ ... │ └─────────────────────────────────┘Storage Engine# Multi-modal storage built on B+Trees with MemTable overlay.\nComponent Technology Purpose MemTable In-memory skip list Write buffer, recent reads B+Tree LMDB/BoltDB Persistent sorted storage Encoding Model-specific key schemas Key Space isolation Storage Models:\nKey-Value: \u0026lt;key\u0026gt; -\u0026gt; \u0026lt;value\u0026gt; Wide-Column: \u0026lt;rowKey\u0026gt;:\u0026lt;colName\u0026gt; -\u0026gt; \u0026lt;colValue\u0026gt; Large Object: \u0026lt;objectKey\u0026gt;:chunk:\u0026lt;N\u0026gt; -\u0026gt; \u0026lt;chunk_data\u0026gt;Replication System# gRPC-based WAL streaming with bidirectional flow control.\nReplicator Role (Primary):\nStreams WAL segments to connected relayers Relayer Role (Replica):\nConsumes from one or more upstream servers Applies segments in order to local storage Can fan-out to downstream relayers (multi-hop) Guarantees:\nConsistency Model: Eventual (all replicas converge) Delivery: At-least-once with gap detection Ordering: Strict segment sequence enforcement Namespace Watch API# Lightweight, best-effort notification system for real-time change awareness.\nEvent Structure:\n{ \u0026#34;namespace\u0026#34;: \u0026#34;users\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;user:123\u0026#34;, \u0026#34;operation\u0026#34;: \u0026#34;put\u0026#34;, \u0026#34;entry_type\u0026#34;: \u0026#34;kv\u0026#34;, \u0026#34;seq_num\u0026#34;: 42 } Field Description namespace Namespace where change occurred key Exact key that changed operation Operation: put, delete, delete_row entry_type Operation: kv, lob, row seq Monotonic sequence number (per-namespace ordering of events) Characteristics:\nPer-namespace streams: Independent event streams per namespace Ordered delivery: Events delivered in sequence order (if received) Fire-and-forget: No acknowledgments, best-effort delivery Ring buffer: Configurable size-based event retention Backpressure handling: Events dropped if subscribers can\u0026rsquo;t keep up Delivery Guarantees:\nEvents may be dropped if consumer can\u0026rsquo;t keep up. Cannot replay from a specific sequence Subscribers not notified of missed events Use Cases:\nCache invalidation across microservices Trigger event-driven workflows Local application coordination NOT for critical workflows requiring guaranteed delivery (use gRPC replication instead) Example Flow:\nApplication A: PUT user:123 → UnisonDB appends to WAL → Watch stream publishes: {namespace:\u0026#34;users\u0026#34;, key:\u0026#34;user:123\u0026#34;, entry_type:\u0026#34;PUT\u0026#34;, seq:42} → Applications B, C, D receive notification and reactDesign Principles# Multi-Modal Storage Examples# All storage models share the same WAL and B+Tree, differing only in key encoding:\nModel Example Key Encoding Key-Value user:123 → {\u0026quot;name\u0026quot;:\u0026quot;Alice\u0026quot;} \u0026lt;key\u0026gt; Wide-Column user:123 with columns name, email \u0026lt;rowKey\u0026gt;:\u0026lt;colName\u0026gt; Large Object video:abc as streamable chunks \u0026lt;objectKey\u0026gt;:chunk:\u0026lt;N\u0026gt; Edge-First Topology# Designed for hub-and-spoke, multi-hop replication with data locality:\n┌──────────────┐ │ Primary │ (Repliactor Mode - accepts writes) │ (US-East) │ └──────┬───────┘ │ gRPC (durable replication) ┌────────┼────────┐ ↓ ↓ ↓ ┌───────┐┌───────┐┌───────┐ │Europe ││ Asia ││US-West│ (Relayer Mode - read replicas) │Relayer││Relayer││Relayer│ └───┬───┘└───┬───┘└───┬───┘ │ │ │ Watch API (local events) ↓ ↓ ↓ Local Local Local Apps Apps AppsData Flow# Write Path (Server Mode)# Write Request → API Handler → Storage Engine ↓ ┌───────────────┴───────────────┐ ↓ ↓ 1. WAL Append 2. MemTable Update (+ fsync) (in-memory) ↓ ↓ 3. Background Flush ────────────────→ B+Tree (LMDB) ↓ ┌───────┴────────┐ ↓ ↓ gRPC Stream Watch Event (to relayers) (to local apps)Read Path# Read Request → API Handler → Storage Engine ↓ ┌───────┴───────┐ ↓ ↓ MemTable B+Tree (check first) (if not found) └───────┬───────┘ ↓ Merge \u0026amp; ResponseReplication Flow (gRPC)# Replicator (Primary) Relayer (Replica) │ │ │ ─── WAL Segment (gRPC stream) ────→ | │ [metadata + binary + CRC] │ │ ↓ │ 1. Validate checksum │ 2. Append to local WAL │ 3. Apply to MemTable │ 4. Flush to B+Tree │ ↓ │ Can relay downstream │ Can notify local appsWatch Event Flow# WAL Write → Watch Event Builder → Ring Buffer → Transport Layer │ ┌──────────────┼──────────────┐ ↓ ↓ ↓ App A App B App C (subscriber) (subscriber) (subscriber)Event Details:\nTriggered on every WAL append (PUT/DELETE/UPDATE) Buffered in ring buffer (configurable size) Events dropped if buffer full or subscriber slow Storage Layout# data/ ├── \u0026lt;namespace\u0026gt;/ │ ├── wal/ │ │ ├── segment-00000000 # 16MB segments (configurable) │ │ ├── segment-00000001 │ │ └── ... │ ├── db/ │ │ ├── data.mdb # LMDB data file │ │ └── lock.mdb # LMDB lock file │ └── checkpoint/ │ └── last_applied # Recovery pointPer-Namespace Isolation: Each namespace has independent WAL, DB, and checkpoint state.\nSystem Characteristics# Consistency Model# Aspect Guarantee Writes Single primary (Replicator Mode) for linearizability Reads Eventually consistent across relayers Replication At-least-once delivery, ordered segments Isolation Per-namespace (independent namespaces) Durability \u0026amp; Recovery# Crash Recovery:\nScan WAL for uncommitted operations Replay WAL to rebuild MemTable and B+Tree Validate checkpoint consistency Resume operations Replication Recovery (Relayer Mode):\nDetermine last applied segment from checkpoint Reconnect to upstream at last offset Request missing segments (gap detection) Apply backlog before serving reads Data Durability:\nWAL with optional fsync (configurable) CRC32 checksums on all WAL entries Segment-level validation during replication Deployment Topologies# UnisonDB supports various deployment patterns. See the Deployment Guide for detailed configurations and examples.\nExample: Hub-and-Spoke# ┌──────────┐ │ Hub │ (Replicator Mode) └────┬─────┘ │ gRPC (durable replication) ┌──────────┼──────────┐ ↓ ↓ ↓ ┌──────┐ ┌──────┐ ┌──────┐ │Edge 1│ │Edge 2│ │Edge 3│ (Relayers) └──┬───┘ └──┬───┘ └──┬───┘ │ │ │ Watch API (local events) ↓ ↓ ↓ Local Local Local Apps Apps AppsKey characteristics:\nCentral hub accepts all writes Edge relayers provide data locality Local applications subscribe to watch events For detailed configurations, monitoring, and operational guidance, see the Deployment Topologies Guide .\nTradeoffs \u0026amp; Limitations# Design Tradeoffs# Aspect Tradeoff Rationale Write Scalability Single primary per namespace Ensures linearizable writes, simplifies conflict resolution Read Consistency Eventual consistency on relayers Enables high read scalability and data locality Replication Model At-least-once delivery Prioritizes availability over exactly-once semantics Watch Events At-most-once, best-effort Minimizes latency, lightweight notification for non-critical use cases Current Limitations# Write Scaling: Single primary per namespace (no multi-master) Consistency: No strong consistency guarantees for relayer reads Transactions: Limited to single-namespace operations Query Model: No complex queries (no SQL, joins, aggregations) Schema: Schema-less (application-managed structure) When to Use UnisonDB# Good Fit:\nEdge computing with hub-and-spoke topology Read-heavy workloads requiring data locality Event-driven architectures (via Watch API) Applications tolerating eventual consistency Key-value or wide-column access patterns Large object storage with streaming Not a Good Fit:\nStrong consistency requirements across replicas Complex relational queries (joins, aggregations) Multi-region active-active writes Workloads requiring ACID transactions across namespaces Summary# UnisonDB combines database semantics with streaming mechanics through:\nLog-Native Design: WAL as first-class citizen (replication = log streaming) Dual Communication: gRPC for distribution, Watch API for local reactivity Dual Modes: Server (writable primary) and Relayer (read replicas) Multi-Modal Storage: Key-Value, Wide-Column, Large Objects on shared B+Tree Architecture Strengths:\nData locality through edge replicas Event-driven integration via Watch API Simple operational model (log streaming) Flexible deployment topologies (hub-and-spoke, multi-hop) Best For: Edge computing, local-first applications, and read-scalable systems with eventual consistency tolerance.\n"},{"id":4,"href":"/docs/api/","title":"API Reference","section":"UnisonDB Documentation","content":"API Reference# UnisonDB provides API interfaces for client access:\nHTTP REST API # RESTful HTTP API with JSON payloads, supporting:\nKey-Value operations Wide-Column operations Large Object (LOB) operations Stateful transactions Metadata queries Next Steps# HTTP API Reference - Complete HTTP API documentation "},{"id":5,"href":"/docs/getting-started/configurations/","title":"Configuration","section":"Getting Started","content":"Configuration Guide# UnisonDB uses TOML for configuration. This guide covers all available configuration options for both replicator and relayer modes.\nTable of Contents# Replicator Mode Relayer Mode Configuration Reference Replicator Mode# Replicator mode runs UnisonDB as a primary instance that accepts writes and serves reads. Here\u0026rsquo;s a complete example:\n## Port of the http server http_port = 4000 listen_ip = \u0026#34;0.0.0.0\u0026#34; ## grpc config for replication [grpc_config] listen_ip = \u0026#34;0.0.0.0\u0026#34; port = 4001 # SSL/TLS certificate paths for gRPC server cert_path = \u0026#34;../../certs/server.crt\u0026#34; key_path = \u0026#34;../../certs/server.key\u0026#34; ca_path = \u0026#34;../../certs/ca.crt\u0026#34; # Allow insecure connections (no TLS) - ONLY for development! allow_insecure = false # StorageConfig stores all tunable parameters. [storage_config] base_dir = \u0026#34;/tmp/unisondb/server\u0026#34; # Base directory for storage namespaces = [\u0026#34;default\u0026#34;, \u0026#34;tenant_1\u0026#34;, \u0026#34;tenant_2\u0026#34;, \u0026#34;tenant_3\u0026#34;, \u0026#34;tenant_4\u0026#34;] bytes_per_sync = \u0026#34;1MB\u0026#34; segment_size = \u0026#34;16MB\u0026#34; arena_size = \u0026#34;4MB\u0026#34; wal_fsync_interval = \u0026#34;1s\u0026#34; ## WAL cleanup configuration [storage_config.wal_cleanup_config] enabled = false interval = \u0026#34;5m\u0026#34; max_age = \u0026#34;1h\u0026#34; min_segments = 5 max_segments = 10 ## Write notify config - coalesces notifications from WAL writers to readers [write_notify_config] enabled = true max_delay = \u0026#34;20ms\u0026#34; ## ZeroMQ notifier configuration (per-namespace) ## Publishes change notifications for local application consumption [notifier_config.default] bind_port = 5555 high_water_mark = 1000 linger_time = 1000 [notifier_config.tenant_1] bind_port = 5556 high_water_mark = 1000 linger_time = 1000 [pprof_config] enabled = true port = 6060 [log_config] log_level = \u0026#34;info\u0026#34; disable_timestamp = false ## This is for grpc logging only - controls sampling percentages per level [log_config.min_level_percents] debug = 100.0 info = 50.0 warn = 100.0 error = 100.0 ## Fuzzer configuration (for testing) [fuzz_config] ops_per_namespace = 400 workers_per_namespace = 50 local_relayer_count = 1000 startup_delay = \u0026#34;10s\u0026#34; enable_read_ops = falseRelayer Mode# Relayer mode runs UnisonDB as a replica that streams changes from one or more upstream servers. This provides read scalability and data locality.\n## Port of the http server http_port = 6000 [grpc_config] port = 6001 cert_path = \u0026#34;../../certs/server.crt\u0026#34; key_path = \u0026#34;../../certs/server.key\u0026#34; ca_path = \u0026#34;../../certs/ca.crt\u0026#34; [storage_config] base_dir = \u0026#34;/tmp/unisondb/relayer\u0026#34; namespaces = [\u0026#34;default\u0026#34;, \u0026#34;tenant_1\u0026#34;, \u0026#34;tenant_2\u0026#34;] bytes_per_sync = \u0026#34;1MB\u0026#34; ## IMPORTANT: segment_size must match upstream server! segment_size = \u0026#34;16MB\u0026#34; arena_size = \u0026#34;4MB\u0026#34; ## Relayer configuration - can have multiple upstreams [relayer_config] [relayer_config.relayer1] namespaces = [\u0026#34;default\u0026#34;, \u0026#34;tenant_1\u0026#34;, \u0026#34;tenant_2\u0026#34;] cert_path = \u0026#34;../../certs/client.crt\u0026#34; key_path = \u0026#34;../../certs/client.key\u0026#34; ca_path = \u0026#34;../../certs/ca.crt\u0026#34; upstream_address = \u0026#34;localhost:4001\u0026#34; segment_lag_threshold = 100 allow_insecure = false # Optional: custom gRPC service config JSON grpc_service_config = \u0026#34;\u0026#34; ## Optional: Add more relayers for different upstream sources [relayer_config.relayer2] namespaces = [\u0026#34;tenant_3\u0026#34;] cert_path = \u0026#34;../../certs/client2.crt\u0026#34; key_path = \u0026#34;../../certs/client2.key\u0026#34; ca_path = \u0026#34;../../certs/ca.crt\u0026#34; upstream_address = \u0026#34;remote-server:4001\u0026#34; segment_lag_threshold = 100 [log_config] log_level = \u0026#34;info\u0026#34; [log_config.min_level_percents] debug = 0.01 info = 1.0 warn = 1.0 error = 1.0 Configuration Reference# Server Configuration# HTTP Server# http_port = 4000 listen_ip = \u0026#34;0.0.0.0\u0026#34;http_port# Type: Integer Default: 4000 Description: Port for the HTTP API server listen_ip# Type: String Default: \u0026quot;0.0.0.0\u0026quot; Description: IP address to bind HTTP server to Note: Use \u0026quot;127.0.0.1\u0026quot; for localhost-only access gRPC Configuration# [grpc_config] listen_ip = \u0026#34;0.0.0.0\u0026#34; port = 4001 cert_path = \u0026#34;/path/to/server.crt\u0026#34; key_path = \u0026#34;/path/to/server.key\u0026#34; ca_path = \u0026#34;/path/to/ca.crt\u0026#34; allow_insecure = falselisten_ip# Type: String Default: \u0026quot;0.0.0.0\u0026quot; Description: IP address to bind gRPC server to port# Type: Integer Default: 4001 Description: Port for the gRPC server (used for replication) cert_path# Type: String Default: \u0026quot;\u0026quot; Description: Path to TLS certificate file (PEM format) Required: Yes (unless allow_insecure = true) key_path# Type: String Default: \u0026quot;\u0026quot; Description: Path to TLS private key file (PEM format) Required: Yes (unless allow_insecure = true) ca_path# Type: String Default: \u0026quot;\u0026quot; Description: Path to CA certificate file for mTLS Required: Yes (unless allow_insecure = true) allow_insecure# Type: Boolean Default: false Description: Allow insecure connections without TLS Warning: ONLY use in development! Always enable TLS in production Storage Configuration# [storage_config] base_dir = \u0026#34;./data\u0026#34; namespaces = [\u0026#34;default\u0026#34;, \u0026#34;app\u0026#34;] bytes_per_sync = \u0026#34;1MB\u0026#34; segment_size = \u0026#34;16MB\u0026#34; arena_size = \u0026#34;4MB\u0026#34; wal_fsync_interval = \u0026#34;1s\u0026#34; disable_entry_type_check = falsebase_dir# Type: String Default: \u0026quot;./data\u0026quot; Description: Base directory for all data files (WAL segments, LMDB) Note: Must have write permissions namespaces# Type: Array of Strings Default: [\u0026quot;default\u0026quot;] Description: List of namespaces to create on startup Example: [\u0026quot;default\u0026quot;, \u0026quot;users\u0026quot;, \u0026quot;metrics\u0026quot;, \u0026quot;logs\u0026quot;] Note: Each namespace is isolated with separate WAL and storage bytes_per_sync# Type: String (with unit) Default: \u0026quot;1MB\u0026quot; Valid Units: KB, MB, GB Description: Number of bytes to write before forcing fsync segment_size# Type: String (with unit) Default: \u0026quot;16MB\u0026quot; Valid Units: KB, MB, GB Range: 1MB to 1GB Description: Size of each WAL segment file Important: Must match across server and relayer! arena_size# Type: String (with unit) Default: \u0026quot;4MB\u0026quot; Valid Units: KB, MB, GB Range: 1MB to 64MB Description: Size of the write buffer (memtable) Performance: Larger = fewer flushes, more memory usage wal_fsync_interval# Type: String (duration) Default: \u0026quot;1s\u0026quot; Valid Units: ms, s, m Description: Interval for periodic WAL fsync Trade-off: Lower = better durability, higher = better performance WAL Cleanup Configuration# [storage_config.wal_cleanup_config] enabled = false interval = \u0026#34;5m\u0026#34; max_age = \u0026#34;1h\u0026#34; min_segments = 5 max_segments = 10enabled# Type: Boolean Default: false Description: Enable automatic WAL segment cleanup interval# Type: String (duration) Default: \u0026quot;5m\u0026quot; Description: How often to run cleanup max_age# Type: String (duration) Default: \u0026quot;1h\u0026quot; Description: Maximum age of segments before cleanup min_segments# Type: Integer Default: 5 Description: Minimum number of segments to keep max_segments# Type: Integer Default: 10 Description: Trigger cleanup when this many segments exist Write Notification Configuration# Write notifications coalesce updates from WAL writers to readers, reducing notification overhead.\n[write_notify_config] enabled = true max_delay = \u0026#34;20ms\u0026#34;enabled# Type: Boolean Default: true Description: Enable write notification coalescing max_delay# Type: String (duration) Default: \u0026quot;20ms\u0026quot; Valid Units: ms, s Description: Maximum delay before notifying readers Trade-off: Higher = better batching, higher latency for reads ZeroMQ Notifier Configuration# UnisonDB can publish change notifications via ZeroMQ PUB/SUB sockets. This allows local applications to subscribe to real-time change notifications for specific namespaces.\nUse Case: Applications running on the same machine can subscribe to a namespace\u0026rsquo;s ZeroMQ socket and receive notifications whenever data changes, enabling reactive architectures.\n## Each namespace can have its own ZeroMQ notifier [notifier_config.default] bind_port = 5555 high_water_mark = 1000 linger_time = 1000 [notifier_config.tenant_1] bind_port = 5556 high_water_mark = 2000 linger_time = 500bind_port# Type: Integer Required: Yes Description: Port to bind ZeroMQ PUB socket to Format: Applications subscribe to tcp://localhost:{bind_port} high_water_mark# Type: Integer Default: 1000 Description: Maximum number of queued messages before blocking Note: Higher values use more memory but reduce message loss linger_time# Type: Integer (milliseconds) Default: 1000 Description: How long to wait for pending messages on shutdown Range: 0 to 5000 ms Example Application Subscription:\nimport zmq context = zmq.Context() socket = context.socket(zmq.SUB) socket.connect(\u0026#34;tcp://localhost:5555\u0026#34;) # Connect to default namespace socket.setsockopt(zmq.SUBSCRIBE, b\u0026#34;\u0026#34;) # Subscribe to all messages while True: message = socket.recv() print(f\u0026#34;Received change notification: {message}\u0026#34;) Relayer Configuration# Relayer configuration allows a UnisonDB instance to stream WAL changes from one or more upstream servers. This is useful for:\nRead scaling: Run multiple read replicas Data locality: Keep data close to consumers in different regions Backup: Maintain hot standbys [relayer_config] [relayer_config.relayer1] namespaces = [\u0026#34;default\u0026#34;, \u0026#34;tenant_1\u0026#34;] cert_path = \u0026#34;../../certs/client.crt\u0026#34; key_path = \u0026#34;../../certs/client.key\u0026#34; ca_path = \u0026#34;../../certs/ca.crt\u0026#34; upstream_address = \u0026#34;primary-server:4001\u0026#34; segment_lag_threshold = 100 allow_insecure = false grpc_service_config = \u0026#34;\u0026#34;Map Key (e.g., relayer1)# Type: String Description: Unique identifier for this relayer connection Note: Multiple relayers can be configured with different keys namespaces# Type: Array of Strings Required: Yes Description: List of namespaces to replicate from this upstream Note: Namespaces must exist on both upstream and local instance cert_path# Type: String Description: Path to client TLS certificate for mTLS Required: Yes (unless allow_insecure = true) key_path# Type: String Description: Path to client private key for mTLS Required: Yes (unless allow_insecure = true) ca_path# Type: String Description: Path to CA certificate to verify upstream server Required: Yes (unless allow_insecure = true) upstream_address# Type: String Required: Yes Description: Address of upstream gRPC server Format: host:port (e.g., \u0026quot;localhost:4001\u0026quot;, \u0026quot;10.0.1.5:4001\u0026quot;) segment_lag_threshold# Type: Integer Default: 100 Description: Maximum segment lag before logging warnings Note: Helps monitor replication health allow_insecure# Type: Boolean Default: false Description: Allow insecure connection to upstream (no TLS) Warning: Only for development! grpc_service_config# Type: String (JSON) Default: \u0026quot;\u0026quot; (uses built-in defaults) Description: Custom gRPC service configuration JSON Advanced: See gRPC documentation for format Logging Configuration# [log_config] log_level = \u0026#34;info\u0026#34; disable_timestamp = false [log_config.min_level_percents] debug = 100.0 info = 50.0 warn = 100.0 error = 100.0log_level# Type: String Valid Values: \u0026quot;debug\u0026quot;, \u0026quot;info\u0026quot;, \u0026quot;warn\u0026quot;, \u0026quot;error\u0026quot; Default: \u0026quot;info\u0026quot; Description: Minimum log level to output disable_timestamp# Type: Boolean Default: false Description: Disable timestamps in log output Use Case: When running under systemd/journal (timestamps added automatically) min_level_percents# Type: Map of String to Float Description: Sampling percentages for gRPC logging per level Range: 0.0 to 100.0 Purpose: Reduce log volume in high-traffic scenarios Example: info = 1.0 means sample 1% of info logs Log Levels:\ndebug: 100.0 = log all debug messages info: 50.0 = log 50% of info messages (randomly sampled) warn: 100.0 = log all warnings error: 100.0 = log all errors PProf Configuration# [pprof_config] enabled = true port = 6060enabled# Type: Boolean Default: false Description: Enable pprof HTTP server for profiling port# Type: Integer Default: 6060 Description: Port for pprof HTTP server Access: http://localhost:6060/debug/pprof/ Available Profiles:\n/debug/pprof/heap - Memory allocation /debug/pprof/goroutine - Goroutine stack traces /debug/pprof/profile - CPU profile /debug/pprof/trace - Execution trace Fuzzer Configuration# Built-in fuzzer for testing and stress testing UnisonDB.\n[fuzz_config] ops_per_namespace = 400 workers_per_namespace = 50 local_relayer_count = 1000 startup_delay = \u0026#34;10s\u0026#34; enable_read_ops = falseops_per_namespace# Type: Integer Default: 400 Description: Number of operations to perform per namespace workers_per_namespace# Type: Integer Default: 50 Description: Number of concurrent workers per namespace local_relayer_count# Type: Integer Default: 1000 Description: Number of local relayer goroutines to simulate startup_delay# Type: String (duration) Default: \u0026quot;10s\u0026quot; Description: Delay before starting fuzzer Purpose: Allow infrastructure to fully initialize enable_read_ops# Type: Boolean Default: false Description: Include read operations in fuzzing Note: Generates mixed read/write workload when enabled "},{"id":6,"href":"/docs/deployment/","title":"Deployment Topologies","section":"UnisonDB Documentation","content":"Deployment Topologies# Scalable, resilient, and edge-ready — UnisonDB adapts to your deployment architecture.\nDesign your own data mesh with UnisonDB’s log-native architecture. Replicate, stream, and sync data effortlessly across edge and cloud environments.\nOverview# UnisonDB uses a dual-mode architecture — Server (Replicator) for writes and Relayer for reads — allowing flexible, distributed deployments.\nCore Components# Replicator Mode – Primary node responsible for writes and WAL-based streaming Relayer Mode – Read-only node replicating from one or more upstreams Watch API – Real-time notifications for local apps or edge devices gRPC Replication – Durable WAL streaming protocol for replication and recovery 1. Single Server# Simplest deployment for development, testing, or small standalone applications.\n┌──────────────────────────┐ │ UnisonDB Server │ │ (Replicator Mode) │ │ │ │ • HTTP API :8080 │ │ • gRPC API :9090 │ │ • Watch API :5555 │ │ │ │ Capabilities: │ │ * Reads \u0026amp; Writes │ │ * Local watch events │ │ * No replication │ │ * No high availability │ └──────────────────────────┘Configuration# Server config (server.toml):\n[server] mode = \u0026#34;server\u0026#34; data_dir = \u0026#34;./data\u0026#34; http_addr = \u0026#34;0.0.0.0:8080\u0026#34; grpc_addr = \u0026#34;0.0.0.0:9090\u0026#34; [wal] segment_size = \u0026#34;16MB\u0026#34; fsync_enabled = true [watch] enabled = true transport = \u0026#34;zeromq\u0026#34; bind_addr = \u0026#34;tcp://*:5555\u0026#34; buffer_size = 10000Monitoring# # Check server health curl http://localhost:8080/health # View server stats curl http://localhost:8080/stats # Monitor watch subscribers curl http://localhost:8080/stats/watch 2. Replicator + Read Replicas# Replicator handles writes, replicas provide read scalability and geographic distribution.\n┌─────────────────────┐ │ Replicator Server │ (Replicator Mode) │ US-East │ │ • Writes │ │ • Reads │ └──────────┬──────────┘ │ gRPC replication (TLS) ┌────────┼────────┬────────┐ ↓ ↓ ↓ ↓ ┌────────┐┌────────┐┌────────┐┌────────┐ │Relayer ││Relayer ││Relayer ││Relayer │ │US-West ││Europe ││Asia ││Canada │ │ ││ ││ ││ │ │Read- ││Read- ││Read- ││Read- │ │only ││only ││only ││only │ └────────┘└────────┘└────────┘└────────┘Configuration# Replicator server (primary.toml):\n[server] mode = \u0026#34;server\u0026#34; data_dir = \u0026#34;/data/unisondb\u0026#34; http_addr = \u0026#34;0.0.0.0:8080\u0026#34; grpc_addr = \u0026#34;0.0.0.0:9090\u0026#34; [replication] enabled = true tls_cert = \u0026#34;/etc/unisondb/tls/server.crt\u0026#34; tls_key = \u0026#34;/etc/unisondb/tls/server.key\u0026#34; tls_ca = \u0026#34;/etc/unisondb/tls/ca.crt\u0026#34; [watch] enabled = true bind_addr = \u0026#34;tcp://*:5555\u0026#34; buffer_size = 10000Relayer (relayer-us-west.toml):\n[server] mode = \u0026#34;relayer\u0026#34; data_dir = \u0026#34;/data/unisondb\u0026#34; http_addr = \u0026#34;0.0.0.0:8080\u0026#34; [relayer] upstreams = [ \u0026#34;primary.us-east.example.com:9090\u0026#34; ] tls_cert = \u0026#34;/etc/unisondb/tls/client.crt\u0026#34; tls_key = \u0026#34;/etc/unisondb/tls/client.key\u0026#34; tls_ca = \u0026#34;/etc/unisondb/tls/ca.crt\u0026#34; # Relayer can also publish local watch events [watch] enabled = true bind_addr = \u0026#34;tcp://*:5555\u0026#34; buffer_size = 10000 3. Hub-and-Spoke (Edge Computing)# Central hub replicates to many edge nodes, each serving local applications.\n┌──────────────────┐ │ Central Hub │ (Replicator Mode) │ (Cloud/DC) │ │ • All writes │ └────────┬─────────┘ │ gRPC replication ┌────────────────┼────────────────┐ ↓ ↓ ↓ ┌─────────┐ ┌─────────┐ ┌─────────┐ │ Edge 1 │ │ Edge 2 │ │ Edge 3 │ (Relayers) │ Store A │ │ Store B │ │ Store C │ └────┬────┘ └────┬────┘ └────┬────┘ │ Watch API │ Watch API │ Watch API ↓ ↓ ↓ ┌─────────┐ ┌─────────┐ ┌─────────┐ │ POS │ │ POS │ │ POS │ Local apps │ Inv Mgmt│ │ Inv Mgmt│ │ Inv Mgmt│ (subscribers) │ Display │ │ Display │ │ Display │ └─────────┘ └─────────┘ └─────────┘Configuration# Hub (central Replicator server):\n[server] mode = \u0026#34;server\u0026#34; data_dir = \u0026#34;/data/unisondb\u0026#34; http_addr = \u0026#34;0.0.0.0:8080\u0026#34; grpc_addr = \u0026#34;0.0.0.0:9090\u0026#34; [namespaces] # Different namespaces for different data types inventory = { wal_segment_size = \u0026#34;16MB\u0026#34; } orders = { wal_segment_size = \u0026#34;8MB\u0026#34; } analytics = { wal_segment_size = \u0026#34;32MB\u0026#34; } [replication] enabled = true tls_enabled = true max_connections = 1000 # Support many edge nodesEdge relayer (edge-store-001.toml):\n[server] mode = \u0026#34;relayer\u0026#34; data_dir = \u0026#34;/data/unisondb\u0026#34; http_addr = \u0026#34;127.0.0.1:8080\u0026#34; # Local only [relayer] upstreams = [\u0026#34;hub.central.example.com:9090\u0026#34;] tls_enabled = true reconnect_interval = \u0026#34;5s\u0026#34; buffer_size = \u0026#34;100MB\u0026#34; # Handle disconnections # Edge nodes publish local watch events [watch] enabled = true namespaces = [\u0026#34;inventory\u0026#34;, \u0026#34;orders\u0026#34;] # Only needed namespaces bind_addr = \u0026#34;tcp://127.0.0.1:5555\u0026#34; # Local IPC only buffer_size = 5000 4. Multi-Hop Relay (Deep Edge)# Hierarchical replication for deep edge deployments or bandwidth-constrained networks.\n┌──────────────┐ │ Primary │ (Replicator Mode - Cloud) │ (Cloud) │ └──────┬───────┘ │ gRPC ↓ ┌──────────────┐ │ Tier 1 │ (Relayer - Regional DC) │ Regional │ └──────┬───────┘ │ gRPC ┌───────┴────────┐ ↓ ↓ ┌─────────┐ ┌─────────┐ │ Tier 2 │ │ Tier 2 │ (Relayer - Edge Cluster) │ West │ │ East │ └────┬────┘ └────┬────┘ │ │ ┌───┴───┐ ┌───┴───┐ ↓ ↓ ↓ ↓ Tier 3 Tier 3 Tier 3 Tier 3 (Relayer - Leaf Nodes) Store1 Store2 Store3 Store4 ↓ ↓ ↓ ↓ Local Local Local Local Apps Apps Apps AppsConfiguration# Tier 1 (Regional relayer):\n[server] mode = \u0026#34;relayer\u0026#34; data_dir = \u0026#34;/data/unisondb\u0026#34; grpc_addr = \u0026#34;0.0.0.0:9090\u0026#34; # Accept downstream connections [relayer] upstreams = [\u0026#34;primary.cloud.example.com:9090\u0026#34;] # This relayer can also relay to downstream enable_relay = true tls_enabled = trueTier 2 (Edge cluster relayer):\n[server] mode = \u0026#34;relayer\u0026#34; data_dir = \u0026#34;/data/unisondb\u0026#34; grpc_addr = \u0026#34;0.0.0.0:9090\u0026#34; [relayer] upstreams = [\u0026#34;tier1-regional.example.com:9090\u0026#34;] enable_relay = true # Relay to Tier 3 tls_enabled = trueTier 3 (Leaf relayer):\n[server] mode = \u0026#34;relayer\u0026#34; data_dir = \u0026#34;/data/unisondb\u0026#34; http_addr = \u0026#34;127.0.0.1:8080\u0026#34; [relayer] upstreams = [\u0026#34;tier2-west.example.com:9090\u0026#34;] enable_relay = false # Leaf node, no downstream tls_enabled = true [watch] enabled = true bind_addr = \u0026#34;tcp://127.0.0.1:5555\u0026#34; 5. Hybrid: Replication + Local Events# Combines durable replication with local event-driven applications.\n┌───────────────────────────────────┐ │ Replicator Server │ │ (Replicator Mode) │ │ │ │ ┌───────────────────┐ │ │ │ Storage Engine │ │ │ └─────────┬─────────┘ │ │ │ │ │ ┌───────┴────────┐ │ │ ↓ ↓ │ │ [gRPC] [Watch API] │ │ :9090 :5555 │ └────┬──────────────────┬───────────┘ │ │ │ └──────┐ ↓ ↓ ┌─────────┐ ┌─────────────┐ │ Remote │ │ Local Apps │ │Relayers │ │ │ └─────────┘ │ • Cache │ │ • Analytics │ │ • Audit Log │ │ • Dashboard │ └─────────────┘Configuration# Primary with both channels:\n[server] mode = \u0026#34;server\u0026#34; data_dir = \u0026#34;/data/unisondb\u0026#34; http_addr = \u0026#34;0.0.0.0:8080\u0026#34; grpc_addr = \u0026#34;0.0.0.0:9090\u0026#34; # gRPC replication for remote relayers [replication] enabled = true tls_enabled = true # Watch API for local applications [watch] enabled = true transport = \u0026#34;zeromq\u0026#34; namespaces = [\u0026#34;users\u0026#34;, \u0026#34;sessions\u0026#34;, \u0026#34;metrics\u0026#34;] # Per-namespace watch configuration [watch.users] bind_addr = \u0026#34;tcp://*:5555\u0026#34; buffer_size = 10000 [watch.sessions] bind_addr = \u0026#34;tcp://*:5556\u0026#34; buffer_size = 20000 # High-frequency updates [watch.metrics] bind_addr = \u0026#34;tcp://*:5557\u0026#34; buffer_size = 50000 Watch API security:\nBind to 127.0.0.1 for local-only access Next Steps# Configuration Reference - Detailed config options HTTP API - API documentation Architecture Overview - Architecture Overview "},{"id":7,"href":"/docs/operations/","title":"Operations","section":"UnisonDB Documentation","content":"Operations# Guides for operating UnisonDB.\nGuides# Backup and Restore # Complete guide to backing up and restoring UnisonDB using WAL segments and B-Tree snapshots.\n"},{"id":8,"href":"/docs/examples/","title":"Examples","section":"UnisonDB Documentation","content":"UnisonDB Examples# Get hands-on with UnisonDB through practical examples that demonstrate how to deploy, replicate, and synchronize data across distributed systems.\nEach tutorial highlights a real-world use case — from single-node configurations to multi-region CRDT replication and real-time streaming at the edge.\n1. How to Build Conflict-Free Multi-Datacenter Systems with CRDTs and UnisonDB # Learn how to design globally replicated systems that stay consistent without coordination.\nThis example walks you through using Conflict-Free Replicated Data Types (CRDTs) with UnisonDB’s WAL-based replication to achieve eventual consistency across data centers.\n"},{"id":9,"href":"/docs/operations/backup-restore/","title":"Backup and Restore","section":"Operations","content":"Backup and Restore# UnisonDB provides HTTP APIs for creating durable backups of both Write-Ahead Log (WAL) segments and B-Tree snapshots.\nOverview# UnisonDB\u0026rsquo;s backup system is designed around two complementary artifacts:\nComponent Purpose Recovery Capability Storage Size WAL Segments Incremental transaction logs Point-in-time recovery Small (append-only) B-Tree Snapshots Full database state Fast baseline restore Larger (full state) Key Principles:\nNamespace isolation: Each namespace has its own backup root (\u0026lt;dataDir\u0026gt;/backups/{namespace}) Crash-consistent: All backups are atomic and immediately usable BYO tooling: UnisonDB emits raw files; you control compression, encryption, and storage Security: Relative paths only—no directory traversal or cross-namespace access Backup APIs# WAL Segment Backup# Copies sealed WAL segments to a backup directory for incremental archival.\nEndpoint:\nPOST /api/v1/{namespace}/wal/backupRequest Body:\n{ \u0026#34;afterSegmentId\u0026#34;: 42, \u0026#34;backupDir\u0026#34;: \u0026#34;wal/customer-a\u0026#34; }Parameters:\nField Type Required Description afterSegmentId integer No Only copy segments with IDs greater than this value. Omit or set to 0 to copy all sealed segments. backupDir string Yes Relative path within \u0026lt;dataDir\u0026gt;/backups/{namespace}. Absolute paths or .. traversal are rejected. Response:\n{ \u0026#34;backups\u0026#34;: [ { \u0026#34;segmentId\u0026#34;: 43, \u0026#34;path\u0026#34;: \u0026#34;/var/unison/data/backups/users/wal/customer-a/000000043.wal\u0026#34; }, { \u0026#34;segmentId\u0026#34;: 44, \u0026#34;path\u0026#34;: \u0026#34;/var/unison/data/backups/users/wal/customer-a/000000044.wal\u0026#34; } ] }Example:\ncurl -X POST http://localhost:8080/api/v1/users/wal/backup \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;afterSegmentId\u0026#34;: 100, \u0026#34;backupDir\u0026#34;: \u0026#34;wal/daily\u0026#34; }\u0026#39; B-Tree Snapshot Backup# Creates a consistent snapshot of the entire B-Tree store.\nEndpoint:\nPOST /api/v1/{namespace}/btree/backupRequest Body:\n{ \u0026#34;path\u0026#34;: \u0026#34;snapshots/users-20250108.snapshot\u0026#34; }Parameters:\nField Type Required Description path string Yes Relative path within \u0026lt;dataDir\u0026gt;/backups/{namespace}. UnisonDB writes to {path}.tmp, fsyncs, then atomically renames. Response:\n{ \u0026#34;path\u0026#34;: \u0026#34;/var/unison/data/backups/users/snapshots/users-20250108.snapshot\u0026#34;, \u0026#34;bytes\u0026#34;: 73400320 }Example:\ncurl -X POST http://localhost:8080/api/v1/users/btree/backup \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;path\u0026#34;: \u0026#34;snapshots/users-\u0026#39;$(date +%Y%m%d)\u0026#39;.snapshot\u0026#34; }\u0026#39; Backup Strategies# 1. Full Backup (Snapshot Only)# Simple strategy for small databases or infrequent backups.\nWorkflow:\n# Daily full snapshot curl -X POST http://localhost:8080/api/v1/users/btree/backup \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;path\u0026#34;: \u0026#34;daily/snapshot-\u0026#39;$(date +%Y%m%d)\u0026#39;.db\u0026#34;}\u0026#39; 2. Incremental Backup (WAL + Periodic Snapshots)# Workflow:\nDaily baseline:\n# Full B-Tree snapshot curl -X POST http://localhost:8080/api/v1/users/btree/backup \\ -d \u0026#39;{\u0026#34;path\u0026#34;: \u0026#34;weekly/snapshot-\u0026#39;$(date +%Y%m%d)\u0026#39;.db\u0026#34;}\u0026#39;Hourly incremental:\n# Every hour: Copy new WAL segments LAST_SEGMENT=$(cat /var/backups/last_wal_id.txt || echo 0) curl -X POST http://localhost:8080/api/v1/users/wal/backup \\ -d \u0026#34;{\\\u0026#34;afterSegmentId\\\u0026#34;: $LAST_SEGMENT, \\\u0026#34;backupDir\\\u0026#34;: \\\u0026#34;wal/$(date +%Y%m%d-%H)\\\u0026#34;}\u0026#34; \\ | jq -r \u0026#39;.backups[-1].segmentId\u0026#39; \u0026gt; /var/backups/last_wal_id.txt 3. Continuous WAL Archival# Workflow:\nUse a cron job or systemd timer to continuously ship WAL segments:\n#!/bin/bash # /usr/local/bin/unison-wal-archive.sh NAMESPACE=\u0026#34;users\u0026#34; STATE_FILE=\u0026#34;/var/lib/unison/wal-archive-state.json\u0026#34; # Read last archived segment LAST_SEGMENT=$(jq -r \u0026#39;.lastSegmentId // 0\u0026#39; \u0026#34;$STATE_FILE\u0026#34; 2\u0026gt;/dev/null || echo 0) # Backup new segments RESPONSE=$(curl -s -X POST http://localhost:8080/api/v1/$NAMESPACE/wal/backup \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#34;{\\\u0026#34;afterSegmentId\\\u0026#34;: $LAST_SEGMENT, \\\u0026#34;backupDir\\\u0026#34;: \\\u0026#34;wal/archive\\\u0026#34;}\u0026#34;) # Extract latest segment ID NEW_LAST=$(echo \u0026#34;$RESPONSE\u0026#34; | jq -r \u0026#39;.backups[-1].segmentId // 0\u0026#39;) if [ \u0026#34;$NEW_LAST\u0026#34; != \u0026#34;0\u0026#34; ]; then # Update state echo \u0026#34;{\\\u0026#34;lastSegmentId\\\u0026#34;: $NEW_LAST, \\\u0026#34;timestamp\\\u0026#34;: \\\u0026#34;$(date -Iseconds)\\\u0026#34;}\u0026#34; \u0026gt; \u0026#34;$STATE_FILE\u0026#34; # Compress and upload to S3 find /var/unison/data/backups/$NAMESPACE/wal/archive -name \u0026#34;*.wal\u0026#34; \\ -mmin -15 -exec gzip {} \\; \\ -exec aws s3 cp {}.gz s3://backups/unison/$NAMESPACE/wal/ \\; fiCron schedule:\n*/15 * * * * /usr/local/bin/unison-wal-archive.sh Backup Automation# Systemd Timer Example# Service unit (/etc/systemd/system/unison-backup.service):\n[Unit] Description=UnisonDB Backup Service After=network.target [Service] Type=oneshot User=unison ExecStart=/usr/local/bin/unison-backup.sh StandardOutput=journal StandardError=journalTimer unit (/etc/systemd/system/unison-backup.timer):\n[Unit] Description=UnisonDB Backup Timer [Timer] OnCalendar=daily OnCalendar=*:0/6 # Every 6 hours Persistent=true [Install] WantedBy=timers.targetBackup script (/usr/local/bin/unison-backup.sh):\n#!/bin/bash set -euo pipefail NAMESPACE=\u0026#34;users\u0026#34; BACKUP_ROOT=\u0026#34;/var/unison/data/backups/$NAMESPACE\u0026#34; DATE=$(date +%Y%m%d-%H%M) # 1. B-Tree snapshot (daily at midnight) if [ \u0026#34;$(date +%H%M)\u0026#34; = \u0026#34;0000\u0026#34; ]; then echo \u0026#34;Creating B-Tree snapshot...\u0026#34; curl -X POST http://localhost:8080/api/v1/$NAMESPACE/btree/backup \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#34;{\\\u0026#34;path\\\u0026#34;: \\\u0026#34;snapshots/btree-$DATE.db\\\u0026#34;}\u0026#34; # Compress snapshot zstd -q --rm \u0026#34;$BACKUP_ROOT/snapshots/btree-$DATE.db\u0026#34; # Upload to S3 aws s3 cp \u0026#34;$BACKUP_ROOT/snapshots/btree-$DATE.db.zst\u0026#34; \\ s3://backups/unison/$NAMESPACE/snapshots/ fi # 2. WAL incremental (every run) echo \u0026#34;Backing up WAL segments...\u0026#34; STATE_FILE=\u0026#34;/var/lib/unison/wal-state-$NAMESPACE.json\u0026#34; LAST_SEGMENT=$(jq -r \u0026#39;.lastSegmentId // 0\u0026#39; \u0026#34;$STATE_FILE\u0026#34; 2\u0026gt;/dev/null || echo 0) RESPONSE=$(curl -s -X POST http://localhost:8080/api/v1/$NAMESPACE/wal/backup \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#34;{\\\u0026#34;afterSegmentId\\\u0026#34;: $LAST_SEGMENT, \\\u0026#34;backupDir\\\u0026#34;: \\\u0026#34;wal/$DATE\\\u0026#34;}\u0026#34;) # Update state NEW_LAST=$(echo \u0026#34;$RESPONSE\u0026#34; | jq -r \u0026#39;.backups[-1].segmentId // 0\u0026#39;) if [ \u0026#34;$NEW_LAST\u0026#34; != \u0026#34;0\u0026#34; ]; then echo \u0026#34;{\\\u0026#34;lastSegmentId\\\u0026#34;: $NEW_LAST, \\\u0026#34;timestamp\\\u0026#34;: \\\u0026#34;$(date -Iseconds)\\\u0026#34;}\u0026#34; \u0026gt; \u0026#34;$STATE_FILE\u0026#34; # Compress and upload WAL segments tar -czf \u0026#34;$BACKUP_ROOT/wal-$DATE.tar.gz\u0026#34; -C \u0026#34;$BACKUP_ROOT\u0026#34; \u0026#34;wal/$DATE\u0026#34; aws s3 cp \u0026#34;$BACKUP_ROOT/wal-$DATE.tar.gz\u0026#34; s3://backups/unison/$NAMESPACE/wal/ # Cleanup local WAL backups older than 7 days find \u0026#34;$BACKUP_ROOT/wal\u0026#34; -type d -mtime +7 -exec rm -rf {} + fi echo \u0026#34;Backup completed successfully\u0026#34;Enable the timer:\nsudo systemctl daemon-reload sudo systemctl enable --now unison-backup.timer sudo systemctl status unison-backup.timer Storage Integration Examples# Amazon S3# #!/bin/bash # Backup to S3 with encryption NAMESPACE=\u0026#34;users\u0026#34; BACKUP_DIR=\u0026#34;/var/unison/data/backups/$NAMESPACE\u0026#34; S3_BUCKET=\u0026#34;s3://company-backups/unison/$NAMESPACE\u0026#34; DATE=$(date +%Y%m%d) # 1. Create B-Tree snapshot curl -X POST http://localhost:8080/api/v1/$NAMESPACE/btree/backup \\ -d \u0026#34;{\\\u0026#34;path\\\u0026#34;: \\\u0026#34;snapshots/btree-$DATE.db\\\u0026#34;}\u0026#34; # 2. Compress and encrypt zstd -q \u0026#34;$BACKUP_DIR/snapshots/btree-$DATE.db\u0026#34; gpg --encrypt --recipient backup@company.com \\ \u0026#34;$BACKUP_DIR/snapshots/btree-$DATE.db.zst\u0026#34; # 3. Upload to S3 with server-side encryption aws s3 cp \u0026#34;$BACKUP_DIR/snapshots/btree-$DATE.db.zst.gpg\u0026#34; \\ \u0026#34;$S3_BUCKET/snapshots/\u0026#34; \\ --storage-class STANDARD_IA \\ --server-side-encryption AES256 # 4. Cleanup local files rm -f \u0026#34;$BACKUP_DIR/snapshots/btree-$DATE.db\u0026#34;* Restore Procedures# Full Restore from B-Tree Snapshot# Scenario: Complete data loss, restore from latest snapshot.\n# 1. Stop UnisonDB sudo systemctl stop unisondb # 2. Download snapshot from S3 aws s3 cp s3://backups/unison/users/snapshots/btree-20250108.db.zst \\ /tmp/restore.db.zst # 3. Decompress zstd -d /tmp/restore.db.zst -o /tmp/restore.db # 4. Replace existing database NAMESPACE=\u0026#34;users\u0026#34; DATA_DIR=\u0026#34;/var/unison/data/$NAMESPACE\u0026#34; rm -rf \u0026#34;$DATA_DIR/db\u0026#34;/* mkdir -p \u0026#34;$DATA_DIR/db\u0026#34; # Copy snapshot to data directory (engine-specific) # For LMDB: cp /tmp/restore.db \u0026#34;$DATA_DIR/db/data.mdb\u0026#34; # 5. Restart UnisonDB sudo systemctl start unisondb # 6. Verify curl http://localhost:8080/api/v1/users/kv/test-key Point-in-Time Recovery (PITR)# Scenario: Restore to a specific point in time using snapshot + WAL replay.\n#!/bin/bash # Restore to 2025-01-08 14:30:00 UTC TARGET_TIME=\u0026#34;2025-01-08T14:30:00Z\u0026#34; NAMESPACE=\u0026#34;users\u0026#34; # 1. Find baseline snapshot before target time SNAPSHOT=$(aws s3 ls s3://backups/unison/$NAMESPACE/snapshots/ | \\ awk \u0026#39;{print $4}\u0026#39; | \\ grep -E \u0026#39;btree-[0-9]{8}\u0026#39; | \\ sort | \\ awk -v target=\u0026#34;$(date -d \u0026#34;$TARGET_TIME\u0026#34; +%Y%m%d)\u0026#34; \u0026#39;$0 \u0026lt;= target\u0026#39; | \\ tail -1) echo \u0026#34;Using baseline snapshot: $SNAPSHOT\u0026#34; # 2. Download and restore snapshot aws s3 cp \u0026#34;s3://backups/unison/$NAMESPACE/snapshots/$SNAPSHOT\u0026#34; /tmp/ zstd -d \u0026#34;/tmp/$SNAPSHOT\u0026#34; -o /tmp/restore.db # 3. Download WAL segments after snapshot SNAPSHOT_DATE=$(echo $SNAPSHOT | grep -oE \u0026#39;[0-9]{8}\u0026#39;) mkdir -p /tmp/wal-restore aws s3 sync \u0026#34;s3://backups/unison/$NAMESPACE/wal/\u0026#34; /tmp/wal-restore/ \\ --exclude \u0026#34;*\u0026#34; \\ --include \u0026#34;wal-${SNAPSHOT_DATE}*.tar.gz\u0026#34; # 4. Extract WAL segments for archive in /tmp/wal-restore/*.tar.gz; do tar -xzf \u0026#34;$archive\u0026#34; -C /tmp/wal-restore/ done # 5. Stop UnisonDB and restore sudo systemctl stop unisondb DATA_DIR=\u0026#34;/var/unison/data/$NAMESPACE\u0026#34; rm -rf \u0026#34;$DATA_DIR/db\u0026#34;/* \u0026#34;$DATA_DIR/wal\u0026#34;/* cp /tmp/restore.db \u0026#34;$DATA_DIR/db/data.mdb\u0026#34; # 6. Copy WAL segments up to target time # (UnisonDB will replay WAL on startup) find /tmp/wal-restore -name \u0026#34;*.wal\u0026#34; | sort | while read wal; do # Check WAL timestamp (implementation-specific) # Copy only if before target time cp \u0026#34;$wal\u0026#34; \u0026#34;$DATA_DIR/wal/\u0026#34; done # 7. Restart and verify sudo systemctl start unisondb Best Practices# Backup Schedule# Frequency Component Retention Hourly WAL segments 7 days local, 30 days remote Daily B-Tree snapshot 7 days local, 90 days remote Weekly Full backup (WAL + snapshot) 1 year Monthly Compliance archive 7 years (if required) Security# 1. Encrypt backups:\n# GPG encryption before upload gpg --encrypt --recipient backup-key@company.com backup.db2. Access control:\n# Restrict backup directory permissions chmod 700 /var/unison/data/backups chown unison:unison /var/unison/data/backups3. Audit logging:\n# Log all backup API calls curl -X POST http://localhost:8080/api/v1/users/wal/backup \\ -d \u0026#39;{\u0026#34;backupDir\u0026#34;: \u0026#34;wal/archive\u0026#34;}\u0026#39; \\ | tee -a /var/log/unison-backups.logTesting Restores# Monthly restore drill:\n#!/bin/bash # Test restore in isolated environment NAMESPACE=\u0026#34;users\u0026#34; TEST_DIR=\u0026#34;/tmp/restore-test-$(date +%s)\u0026#34; # 1. Create test environment mkdir -p \u0026#34;$TEST_DIR\u0026#34;/{db,wal} # 2. Download latest snapshot aws s3 cp s3://backups/unison/$NAMESPACE/snapshots/latest.db.zst \u0026#34;$TEST_DIR/\u0026#34; zstd -d \u0026#34;$TEST_DIR/latest.db.zst\u0026#34; -o \u0026#34;$TEST_DIR/db/data.mdb\u0026#34; # 3. Start UnisonDB in test mode unisondb --data-dir \u0026#34;$TEST_DIR\u0026#34; --http-addr localhost:9999 \u0026amp; PID=$! # 4. Verify data integrity sleep 5 curl http://localhost:9999/health curl http://localhost:9999/api/v1/$NAMESPACE/kv/test-key # 5. Cleanup kill $PID rm -rf \u0026#34;$TEST_DIR\u0026#34; echo \u0026#34;Restore test completed successfully\u0026#34;Monitoring# Key metrics to track:\n# Backup success rate curl http://localhost:8080/metrics | grep backup_success_total # Last backup timestamp curl http://localhost:8080/metrics | grep backup_last_timestamp_seconds # Backup size curl http://localhost:8080/metrics | grep backup_bytes_totalAlert on:\nBackup failures (2+ consecutive failures) Backup age \u0026gt; 24 hours Backup size anomalies (\u0026gt;50% change) WAL segment gap detection Troubleshooting# Backup Fails with Permission Denied# Symptom:\n{\u0026#34;error\u0026#34;: \u0026#34;permission denied: /var/unison/data/backups/users/wal\u0026#34;}Solution:\n# Ensure backup directory is writable sudo chown -R unison:unison /var/unison/data/backups sudo chmod -R 755 /var/unison/data/backups Backup Directory Full# Symptom:\n{\u0026#34;error\u0026#34;: \u0026#34;no space left on device\u0026#34;}Solution:\n# Clean up old local backups find /var/unison/data/backups -type f -mtime +7 -delete # Or mount separate volume sudo mount /dev/sdb1 /var/unison/data/backups Restore Fails with Corrupted Snapshot# Symptom:\nERROR: database file is corruptedSolution:\n# 1. Verify checksum (if available) sha256sum backup.db # Compare with original checksum # 2. Try previous snapshot aws s3 ls s3://backups/unison/users/snapshots/ | sort | tail -2 Next Steps# Deployment Topologies - High availability with relayers Monitoring - Tracking backup health HTTP API Reference - Complete API documentation "},{"id":10,"href":"/docs/examples/multi-dc-crdt/","title":"Multi-DC CRDT Replication","section":"Examples","content":" Introduction: The Challenge of Distributed State Management# Imagine you\u0026rsquo;re building a globally distributed application where users across different continents need to see consistent data think user presence status, live dashboards, or real-time collaboration features. Traditional databases force you to choose between consistency and availability, but what if there was a better way?\nConflict-free Replicated Data Types (CRDTs) offer a mathematical approach to distributed state management where conflicts are resolved automatically through well-defined merge operations. When combined with edge notifications, you get a powerful pattern: write anywhere, replicate everywhere, and get notified of changes in real-time.\nIn this post, we\u0026rsquo;ll build a multi-datacenter system using UnisonDB that demonstrates:\nConcurrent writes to multiple datacenters Automatic conflict resolution using CRDTs Real-time change notifications via ZeroMQ Eventual consistency across all nodes Architecture Overview# Our demo system consists of three UnisonDB nodes:\n+---------------------------------------------------------------+ | Multi-DC CRDT Architecture | +---------------------------------------------------------------+ Writes Writes | | v v +----------------+ +----------------+ | Datacenter 1 | | Datacenter 2 | | (Primary) | | (Primary) | | | | | | HTTP: 8001 | | HTTP: 8002 | | gRPC: 4001 | | gRPC: 4002 | +--------+-------+ +--------+-------+ | | | gRPC Replication | +---------------------+-------------------+ | v +---------------------+ | Relayer | | (Read-Only) | | | | HTTP: 8003 | | ZMQ dc1: 5555 ---\u0026gt; |----+ | ZMQ dc2: 5556 ---\u0026gt; |----+ Watch API +---------------------+ | Notifications | v +--------------------+ | CRDT Client | | (Go / Node.js) | | | | Converged State | +--------------------+Component Roles# Component Role Namespace HTTP Port gRPC Port ZMQ Ports DC1 Primary (accepts writes) ad-campaign-dc1 8001 4001 - DC2 Primary (accepts writes) ad-campaign-dc2 8002 4002 - Relayer Read-only replica ad-campaign-dc1, ad-campaign-dc2 8003 - 5555, 5556 Building and Running UnisonDB# Prerequisites# # Ensure you have Go 1.21+ and CGO enabled go version # go version go1.21.0 or higherStep 1: Build UnisonDB# This needs Zero MQ Installed Make Sure You\u0026rsquo;ve have it Installed. Install ZeroMQ dependency # Clone the repository git clone https://github.com/ankur-anand/unisondb.git cd unisondb # Build the binary (CGO required for RocksDB) CGO_ENABLED=1 go build -tags zeromq ./cmd/unisondbStep 2: Start the Multi-DC Cluster# Open three separate terminal windows and run:\nTerminal 1: Start Datacenter 1\n./unisondb -config ./cmd/examples/crdt-multi-dc/configs/dc1.toml replicatorTerminal 2: Start Datacenter 2\n./unisondb -config ./cmd/examples/crdt-multi-dc/configs/dc2.toml replicatorTerminal 3: Start Relayer\n./unisondb -config ./cmd/examples/crdt-multi-dc/configs/relayer.toml relayerYou should see output indicating each node is ready:\nINFO: HTTP server listening on :8001 INFO: gRPC server listening on :4001 INFO: Namespace \u0026#39;ad-campaign-dc1\u0026#39; initializedStep 3: Start the CRDT Client# Open a fourth terminal to run the client that will observe CRDT state:\ncd cmd/examples/golang-crdt-client go run main.goExpected output:\nWaiting for change notifications... Connecting to ZeroMQ ad-campaign-dc1: tcp://localhost:5555 Connecting to ZeroMQ ad-campaign-dc2: tcp://localhost:5556 ZeroMQ listener started for namespace: ad-campaign-dc1 ZeroMQ listener started for namespace: ad-campaign-dc2Your system is now ready!\nUnderstanding CRDTs: Two Types in Action# 1. LWW-Register (Last-Write-Wins Register)# Use Cases: User profiles, configuration settings, feature flags\nHow it works:\nEach write includes a timestamp and replica ID Conflicts are resolved by choosing the write with the latest timestamp If timestamps are equal, the lexicographically higher replica ID wins Data Format:\n{ \u0026#34;value\u0026#34;: \u0026#34;actual data\u0026#34;, \u0026#34;timestamp\u0026#34;: 1698765432000, \u0026#34;replica\u0026#34;: \u0026#34;ad-campaign-dc1\u0026#34; }2. G-Counter (Grow-Only Counter)# Use Cases: Page views, API calls, distributed metrics (monotonically increasing)\nHow it works:\nEach replica maintains its own counter Merging takes the maximum count per replica Total value is the sum of all replica counters Can only increase (never decrease) Data Format:\n{ \u0026#34;replica\u0026#34;: \u0026#34;ad-campaign-dc1\u0026#34;, \u0026#34;count\u0026#34;: 5 }Demo Scenarios with curl Examples# Scenario 1: Basic LWW-Register Update# Let\u0026rsquo;s update a user\u0026rsquo;s status across two datacenters:\nWrite \u0026ldquo;online\u0026rdquo; to DC1 (timestamp: 1698765432000)\ncurl -X PUT \u0026#34;http://localhost:8001/api/v1/ad-campaign-dc1/kv/lww:user-status\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#39;{\u0026#34;value\u0026#34;:\u0026#34;online\u0026#34;,\u0026#34;timestamp\u0026#34;:1698765432000,\u0026#34;replica\u0026#34;:\u0026#34;ad-campaign-dc1\u0026#34;}\u0026#39; | base64)\u0026#39;\u0026#34;}\u0026#39;Client Output:\nChange notification received Topic: ad-campaign-dc1.kv Key: lww:user-status Operation: put Processing update: lww:user-status LWW-Register updated: lww:user-status Value: online Timestamp: 1698765432000 Replica: ad-campaign-dc1 CURRENT CRDT STATE LWW-Registers: lww:user-status: Value: online Timestamp: 1698765432000 Replica: ad-campaign-dc1Now write \u0026ldquo;away\u0026rdquo; to DC2 with a newer timestamp:\nWrite \u0026ldquo;away\u0026rdquo; to DC2 (timestamp: 1698765433000)\ncurl -X PUT \u0026#34;http://localhost:8002/api/v1/ad-campaign-dc2/kv/lww:user-status\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#39;{\u0026#34;value\u0026#34;:\u0026#34;away\u0026#34;,\u0026#34;timestamp\u0026#34;:1698765433000,\u0026#34;replica\u0026#34;:\u0026#34;ad-campaign-dc2\u0026#34;}\u0026#39; | base64)\u0026#39;\u0026#34;}\u0026#39;Client Output:\nChange notification received Topic: ad-campaign-dc2.kv Key: lww:user-status Operation: put Processing update: lww:user-status LWW-Register updated: lww:user-status Value: away Timestamp: 1698765433000 Replica: ad-campaign-dc2 CURRENT CRDT STATE LWW-Registers: lww:user-status: Value: away Timestamp: 1698765433000 Replica: ad-campaign-dc2What happened? The client automatically resolved the conflict! DC2\u0026rsquo;s write won because it had a newer timestamp (1698765433000 \u0026gt; 1698765432000).\nScenario 2: Concurrent Writes with Same Timestamp# What happens when two datacenters write at the exact same millisecond?\nWrite to DC1:\nTIMESTAMP=$(date +%s)000 curl -X PUT \u0026#34;http://localhost:8001/api/v1/ad-campaign-dc1/kv/lww:config\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#34;{\\\u0026#34;value\\\u0026#34;:\\\u0026#34;DC1 wins?\\\u0026#34;,\\\u0026#34;timestamp\\\u0026#34;:$TIMESTAMP,\\\u0026#34;replica\\\u0026#34;:\\\u0026#34;ad-campaign-dc1\\\u0026#34;}\u0026#34; | base64)\u0026#39;\u0026#34;}\u0026#39;Write to DC2 (same timestamp):\ncurl -X PUT \u0026#34;http://localhost:8002/api/v1/ad-campaign-dc2/kv/lww:config\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#34;{\\\u0026#34;value\\\u0026#34;:\\\u0026#34;DC2 wins!\\\u0026#34;,\\\u0026#34;timestamp\\\u0026#34;:$TIMESTAMP,\\\u0026#34;replica\\\u0026#34;:\\\u0026#34;ad-campaign-dc2\\\u0026#34;}\u0026#34; | base64)\u0026#39;\u0026#34;}\u0026#39;Result: ad-campaign-dc2 wins because lexicographically \u0026quot;ad-campaign-dc2\u0026quot; \u0026gt; \u0026quot;ad-campaign-dc1\u0026quot;. This ensures deterministic conflict resolution across all replicas.\nScenario 3: Distributed Counter (G-Counter)# Let\u0026rsquo;s track page views across two datacenters:\nDC1 serves 5 requests:\ncurl -X PUT \u0026#34;http://localhost:8001/api/v1/ad-campaign-dc1/kv/counter:page-views\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#39;{\u0026#34;replica\u0026#34;:\u0026#34;ad-campaign-dc1\u0026#34;,\u0026#34;count\u0026#34;:5}\u0026#39; | base64)\u0026#39;\u0026#34;}\u0026#39;DC2 serves 3 requests:\ncurl -X PUT \u0026#34;http://localhost:8002/api/v1/ad-campaign-dc2/kv/counter:page-views\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#39;{\u0026#34;replica\u0026#34;:\u0026#34;ad-campaign-dc2\u0026#34;,\u0026#34;count\u0026#34;:3}\u0026#39; | base64)\u0026#39;\u0026#34;}\u0026#39;Client Output:\nCURRENT CRDT STATE G-Counters: counter:page-views: Replica Counts: {\u0026#34;ad-campaign-dc1\u0026#34;:5,\u0026#34;ad-campaign-dc2\u0026#34;:3} Total: 8Result: Total = 8 (5 from DC1 + 3 from DC2). The counters from both datacenters are automatically merged!\nScenario 4: Out-of-Order Delivery (Stale Write)# What if network delays cause an old write to arrive after a newer one?\nWrite NEW value to DC1:\ncurl -X PUT \u0026#34;http://localhost:8001/api/v1/ad-campaign-dc1/kv/lww:feature-flag\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#39;{\u0026#34;value\u0026#34;:true,\u0026#34;timestamp\u0026#34;:2000,\u0026#34;replica\u0026#34;:\u0026#34;ad-campaign-dc1\u0026#34;}\u0026#39; | base64)\u0026#39;\u0026#34;}\u0026#39;Write OLD value to DC2 (stale):\ncurl -X PUT \u0026#34;http://localhost:8002/api/v1/ad-campaign-dc2/kv/lww:feature-flag\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#39;{\u0026#34;value\u0026#34;:false,\u0026#34;timestamp\u0026#34;:1000,\u0026#34;replica\u0026#34;:\u0026#34;ad-campaign-dc2\u0026#34;}\u0026#39; | base64)\u0026#39;\u0026#34;}\u0026#39;Client Output:\nProcessing update: lww:feature-flag LWW-Register ignored (stale): lww:feature-flag Incoming timestamp: 1000 Current timestamp: 2000Result: The stale write is automatically ignored. The CRDT logic ensures we never regress to an older state!\nScenario 5: Multiple Counters Operating Independently# # Track different metrics curl -X PUT \u0026#34;http://localhost:8001/api/v1/ad-campaign-dc1/kv/counter:api-calls\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#39;{\u0026#34;replica\u0026#34;:\u0026#34;ad-campaign-dc1\u0026#34;,\u0026#34;count\u0026#34;:100}\u0026#39; | base64)\u0026#39;\u0026#34;}\u0026#39; curl -X PUT \u0026#34;http://localhost:8002/api/v1/ad-campaign-dc2/kv/counter:api-calls\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#39;{\u0026#34;replica\u0026#34;:\u0026#34;ad-campaign-dc2\u0026#34;,\u0026#34;count\u0026#34;:75}\u0026#39; | base64)\u0026#39;\u0026#34;}\u0026#39; curl -X PUT \u0026#34;http://localhost:8001/api/v1/ad-campaign-dc1/kv/counter:db-queries\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#39;{\u0026#34;replica\u0026#34;:\u0026#34;ad-campaign-dc1\u0026#34;,\u0026#34;count\u0026#34;:250}\u0026#39; | base64)\u0026#39;\u0026#34;}\u0026#39;Client Output:\nCURRENT CRDT STATE G-Counters: counter:api-calls: Replica Counts: {\u0026#34;ad-campaign-dc1\u0026#34;:100,\u0026#34;ad-campaign-dc2\u0026#34;:75} Total: 175 counter:db-queries: Replica Counts: {\u0026#34;ad-campaign-dc1\u0026#34;:250} Total: 250Each counter operates independently with its own convergence!\nReading Data from the Relayer# The relayer provides read-only access to both datacenter namespaces:\nRead from DC1 namespace:\ncurl \u0026#34;http://localhost:8003/api/v1/ad-campaign-dc1/kv/lww:user-status\u0026#34; | jqResponse:\n{ \u0026#34;value\u0026#34;: \u0026#34;eyJ2YWx1ZSI6ImF3YXkiLCJ0aW1lc3RhbXAiOjE2OTg3NjU0MzMwMDAsInJlcGxpY2EiOiJhZC1jYW1wYWlnbi1kYzIifQ==\u0026#34;, \u0026#34;found\u0026#34;: true }Decode the base64 value:\necho \u0026#34;eyJ2YWx1ZSI6ImF3YXkiLCJ0aW1lc3RhbXAiOjE2OTg3NjU0MzMwMDAsInJlcGxpY2EiOiJhZC1jYW1wYWlnbi1kYzIifQ==\u0026#34; | base64 -d | jqOutput:\n{ \u0026#34;value\u0026#34;: \u0026#34;away\u0026#34;, \u0026#34;timestamp\u0026#34;: 1698765433000, \u0026#34;replica\u0026#34;: \u0026#34;ad-campaign-dc2\u0026#34; }How Conflict Resolution Works Under the Hood# LWW-Register Algorithm# The conflict resolution logic in lww_register.go:30-39:\nfunc (r *LWWRegister) Update(value interface{}, timestamp int64, replica string) bool { // Rule 1: Accept if timestamp is newer if timestamp \u0026gt; r.Timestamp { r.Value = value r.Timestamp = timestamp r.Replica = replica return true } // Rule 2: If timestamps equal, use replica ID as tiebreaker if timestamp == r.Timestamp \u0026amp;\u0026amp; replica \u0026gt; r.Replica { r.Value = value r.Replica = replica return true } // Rule 3: Reject stale updates return false }Key Properties:\nCommutative: Order of updates doesn\u0026rsquo;t matter Associative: Grouping of updates doesn\u0026rsquo;t matter Idempotent: Applying the same update multiple times is safe Deterministic: All replicas converge to the same value G-Counter Merge Algorithm# The merge logic in g_counter.go:\nfunc (c *GCounter) Merge(replica string, count int64) bool { current := c.Counts[replica] // Only accept higher counts (monotonic) if count \u0026gt; current { c.Counts[replica] = count return true } return false } func (c *GCounter) GetValue() int64 { total := int64(0) for _, count := range c.Counts { total += count } return total }Key Properties:\nMonotonic: Values only increase Convergent: All replicas reach the same total Partition-tolerant: Works across network splits Real-World Use Cases# 1. User Presence System# # User goes online in US datacenter curl -X PUT \u0026#34;http://localhost:8001/api/v1/ad-campaign-dc1/kv/lww:user:alice:status\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#34;{\\\u0026#34;value\\\u0026#34;:\\\u0026#34;online\\\u0026#34;,\\\u0026#34;timestamp\\\u0026#34;:$(date +%s)000,\\\u0026#34;replica\\\u0026#34;:\\\u0026#34;us-east-1\\\u0026#34;}\u0026#34; | base64)\u0026#39;\u0026#34;}\u0026#39; # User goes away in EU datacenter (newer timestamp wins) curl -X PUT \u0026#34;http://localhost:8002/api/v1/ad-campaign-dc2/kv/lww:user:alice:status\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#34;{\\\u0026#34;value\\\u0026#34;:\\\u0026#34;away\\\u0026#34;,\\\u0026#34;timestamp\\\u0026#34;:$(($(date +%s)+5))000,\\\u0026#34;replica\\\u0026#34;:\\\u0026#34;eu-west-1\\\u0026#34;}\u0026#34; | base64)\u0026#39;\u0026#34;}\u0026#39;All clients worldwide see the latest status in real-time!\n2. Distributed Analytics# # Track impressions across regions curl -X PUT \u0026#34;http://localhost:8001/api/v1/ad-campaign-dc1/kv/counter:campaign-123:impressions\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#39;{\u0026#34;replica\u0026#34;:\u0026#34;us-east-1\u0026#34;,\u0026#34;count\u0026#34;:1500}\u0026#39; | base64)\u0026#39;\u0026#34;}\u0026#39; curl -X PUT \u0026#34;http://localhost:8002/api/v1/ad-campaign-dc2/kv/counter:campaign-123:impressions\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#39;{\u0026#34;replica\u0026#34;:\u0026#34;eu-west-1\u0026#34;,\u0026#34;count\u0026#34;:2300}\u0026#39; | base64)\u0026#39;\u0026#34;}\u0026#39; # Global total: 3800 impressions3. Feature Flags# # Enable feature in production curl -X PUT \u0026#34;http://localhost:8001/api/v1/ad-campaign-dc1/kv/lww:feature:new-ui\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$(echo -n \u0026#34;{\\\u0026#34;value\\\u0026#34;:true,\\\u0026#34;timestamp\\\u0026#34;:$(date +%s)000,\\\u0026#34;replica\\\u0026#34;:\\\u0026#34;control-plane\\\u0026#34;}\u0026#34; | base64)\u0026#39;\u0026#34;}\u0026#39;Feature flag changes propagate globally within milliseconds!\nTry It Yourself# # Clone and run the example git clone https://github.com/ankur-anand/unisondb.git cd unisondb CGO_ENABLED=1 go build -tags zeromq -o unisondb ./cmd/unisondb # Start the demo cd cmd/examples/crdt-multi-dcWatch the magic happen as conflicts resolve themselves and state converges across datacenters!\nAdditional Resources# UnisonDB GitHub Repository CRDT Research Papers ZeroMQ Guide Have questions or want to contribute? Open an issue on GitHub or join our community discussions!\n"},{"id":11,"href":"/blog/fast-multi-reader-write-ahead-log/","title":"How to Build a Fast, Multi-Reader Write-Ahead Log Without Sacrificing Writes","section":"Blog","content":"How to Build a Fast, Multi-Reader Write-Ahead Log Without Sacrificing Writes\nMost databases treat the Write-Ahead Log (WAL) as an implementation detail — a recovery mechanism that quietly ensures durability in the background.\nBut when your system needs to serve thousands of readers, real-time replication, and streaming analytics, the traditional WAL becomes a bottleneck.\nAt UnisonDB, we set out to design a WAL that could scale to hundreds of concurrent readers while maintaining millions of sequential writes per second — without sacrificing durability or consistency.\n1. The Challenge: When Your WAL Becomes a Stream# Traditional write-ahead logs were built for crash recovery, not continuous consumption.\nBut modern systems — databases, event stores, and edge platforms — rely on real-time data movement. The WAL now feeds:\nReplicators and relayers streaming logs across regions Change Data Capture (CDC) pipelines Analytics engines consuming continuous deltas Event-driven applications watching updates This transforms the WAL from a persistence detail into the core replication fabric.\nSo the question becomes:\nHow do you support thousands of concurrent readers without hurting write performance?\n2. The Design Goals# A high-performance, multi-reader Write-Ahead Log must achieve:\nSingle fast writer — sequential appends only Zero reader contention — readers don’t block writes Durability — data is never acknowledged before fsync Isolation — each reader tracks its own offset Observability — every record is addressable by {segment, offset, LSN} In UnisonDB, these constraints led to a segmented, mmap-backed WAL that allows readers and writers to share memory safely.\n3. Segment-Based Architecture# A segmented architecture keeps the log append-only and bounded:\n"},{"id":12,"href":"/docs/api/http-api/","title":"HTTP API Reference","section":"API Reference","content":"HTTP API Reference# Complete reference for UnisonDB\u0026rsquo;s HTTP REST API.\nBase URL# http://localhost:4000/api/v1/{namespace}Data Encoding# All binary values must be base64-encoded :\n# Encode a value echo -n \u0026#34;hello world\u0026#34; | base64 # Output: aGVsbG8gd29ybGQ= # Use in request curl -X PUT http://localhost:4000/api/v1/default/kv/greeting \\ -d \u0026#39;{\u0026#34;value\u0026#34;:\u0026#34;aGVsbG8gd29ybGQ=\u0026#34;}\u0026#39;Request Size Limits Restrictions# Maximum Request Size# To keep UnisonDB\u0026rsquo;s HTTP layer efficient, safe, and predictable, each HTTP request is limited to a maximum size of 1 MB.\nThis limit applies to the entire request body, including:\nJSON payload Base64-encoded values Any metadata sent in the body If your workload needs to store data larger than 1 MB, you should not push it as a single HTTP write. Instead, use UnisonDB\u0026rsquo;s transactional (Txn) write path , which is designed to handle larger logical operations safely and atomically.\nKey-Value Operations# Put KV# Store a key-value pair.\nRequest:\nPUT /api/v1/{namespace}/kv/{key} Content-Type: application/json { \u0026#34;value\u0026#34;: \u0026#34;base64-encoded-value\u0026#34; }Example:\ncurl -X PUT http://localhost:4000/api/v1/default/kv/user:123 \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;value\u0026#34;: \u0026#34;eyJuYW1lIjoiSm9obiIsImFnZSI6MzB9\u0026#34; }\u0026#39;Response (200 OK):\n{ \u0026#34;success\u0026#34;: true }Errors:\n400 Bad Request: Invalid base64 encoding 404 Not Found: Namespace not found 500 Internal Server Error: Engine error Get KV# Retrieve a value by key.\nRequest:\nGET /api/v1/{namespace}/kv/{key}Example:\ncurl http://localhost:4000/api/v1/default/kv/user:123Response (200 OK):\n{ \u0026#34;value\u0026#34;: \u0026#34;eyJuYW1lIjoiSm9obiIsImFnZSI6MzB9\u0026#34;, \u0026#34;found\u0026#34;: true }Response (404 Not Found):\n{ \u0026#34;value\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;found\u0026#34;: false } Delete KV# Delete a key.\nRequest:\nDELETE /api/v1/{namespace}/kv/{key}Example:\ncurl -X DELETE http://localhost:4000/api/v1/default/kv/user:123Response (200 OK):\n{ \u0026#34;success\u0026#34;: true } Batch KV Operations# Perform multiple operations in one request.\nBatch Put# Request:\nPOST /api/v1/{namespace}/kv/batch Content-Type: application/json { \u0026#34;operation\u0026#34;: \u0026#34;put\u0026#34;, \u0026#34;items\u0026#34;: [ {\u0026#34;key\u0026#34;: \u0026#34;key1\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;dmFsdWUx\u0026#34;}, {\u0026#34;key\u0026#34;: \u0026#34;key2\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;dmFsdWUy\u0026#34;} ] }Example:\ncurl -X POST http://localhost:4000/api/v1/default/kv/batch \\ -d \u0026#39;{ \u0026#34;operation\u0026#34;: \u0026#34;put\u0026#34;, \u0026#34;items\u0026#34;: [ {\u0026#34;key\u0026#34;: \u0026#34;user:1\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;dXNlcjE=\u0026#34;}, {\u0026#34;key\u0026#34;: \u0026#34;user:2\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;dXNlcjI=\u0026#34;} ] }\u0026#39;Response (200 OK):\n{ \u0026#34;success\u0026#34;: true, \u0026#34;processed\u0026#34;: 2 }Batch Delete# Request:\nPOST /api/v1/{namespace}/kv/batch Content-Type: application/json { \u0026#34;operation\u0026#34;: \u0026#34;delete\u0026#34;, \u0026#34;keys\u0026#34;: [\u0026#34;key1\u0026#34;, \u0026#34;key2\u0026#34;] }Response (200 OK):\n{ \u0026#34;success\u0026#34;: true, \u0026#34;processed\u0026#34;: 2 } Wide-Column Operations# Put Row# Store a row with multiple columns.\nRequest:\nPUT /api/v1/{namespace}/row/{rowKey} Content-Type: application/json { \u0026#34;columns\u0026#34;: { \u0026#34;column1\u0026#34;: \u0026#34;base64-value1\u0026#34;, \u0026#34;column2\u0026#34;: \u0026#34;base64-value2\u0026#34; } }Example:\ncurl -X PUT http://localhost:4000/api/v1/default/row/user:john \\ -d \u0026#39;{ \u0026#34;columns\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Sm9obiBEb2U=\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;am9obkBleGFtcGxlLmNvbQ==\u0026#34;, \u0026#34;age\u0026#34;: \u0026#34;MzA=\u0026#34; } }\u0026#39;Response (200 OK):\n{ \u0026#34;success\u0026#34;: true } Get Row# Retrieve all columns for a row.\nRequest:\nGET /api/v1/{namespace}/row/{rowKey}Example:\ncurl http://localhost:4000/api/v1/default/row/user:johnResponse (200 OK):\n{ \u0026#34;rowKey\u0026#34;: \u0026#34;user:john\u0026#34;, \u0026#34;columns\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Sm9obiBEb2U=\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;am9obkBleGFtcGxlLmNvbQ==\u0026#34;, \u0026#34;age\u0026#34;: \u0026#34;MzA=\u0026#34; }, \u0026#34;found\u0026#34;: true } Get Row Columns# Retrieve specific columns only.\nRequest:\nGET /api/v1/{namespace}/row/{rowKey}?columns=col1,col2Example:\ncurl \u0026#34;http://localhost:4000/api/v1/default/row/user:john?columns=name,email\u0026#34;Response (200 OK):\n{ \u0026#34;rowKey\u0026#34;: \u0026#34;user:john\u0026#34;, \u0026#34;columns\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Sm9obiBEb2U=\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;am9obkBleGFtcGxlLmNvbQ==\u0026#34; }, \u0026#34;found\u0026#34;: true } Delete Row# Delete an entire row.\nRequest:\nDELETE /api/v1/{namespace}/row/{rowKey}Example:\ncurl -X DELETE http://localhost:4000/api/v1/default/row/user:johnResponse (200 OK):\n{ \u0026#34;success\u0026#34;: true } Delete Row Columns# Delete specific columns from a row.\nRequest:\nDELETE /api/v1/{namespace}/row/{rowKey}/columns?columns=col1,col2Example:\ncurl -X DELETE \u0026#34;http://localhost:4000/api/v1/default/row/user:john/columns?columns=age,city\u0026#34;Response (200 OK):\n{ \u0026#34;success\u0026#34;: true } Batch Row Operations# Batch Put Rows# Request:\nPOST /api/v1/{namespace}/row/batch Content-Type: application/json { \u0026#34;operation\u0026#34;: \u0026#34;put\u0026#34;, \u0026#34;rows\u0026#34;: [ { \u0026#34;rowKey\u0026#34;: \u0026#34;user:1\u0026#34;, \u0026#34;columns\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;QWxpY2U=\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;YWxpY2VAZXhhbXBsZS5jb20=\u0026#34; } } ] }Response (200 OK):\n{ \u0026#34;success\u0026#34;: true, \u0026#34;processed\u0026#34;: 1 } Large Object (LOB) Operations# Put LOB# Upload a large binary object. Check Transaction API.\nGet LOB# Download a large binary object.\nRequest:\nGET /api/v1/{namespace}/lob?key={key}Example:\ncurl \u0026#34;http://localhost:4000/api/v1/default/lob?key=file:doc.pdf\u0026#34; \\ --output document.pdfResponse: Binary data stream\nTransaction Operations# Transactions allow atomic operations across multiple keys.\nTransaction Lifecycle# 1. BEGIN → Get transaction ID 2. APPEND → Add operations (multiple times) 3. COMMIT → Apply atomically OR 3. ABORT → Cancel transactionTransaction Type Restrictions# Transactions are bound to the entryType specified during BEGIN. Once opened, a transaction can only accept operations matching its type:\nentryType Allowed Operations Endpoint kv Key-Value only POST /tx/{txnId}/kv row Wide-Column only POST /tx/{txnId}/row lob Large Objects only POST /tx/{txnId}/lob Begin Transaction# Start a new transaction.\nRequest:\nPOST /api/v1/{namespace}/tx/begin Content-Type: application/json { \u0026#34;operation\u0026#34;: \u0026#34;put\u0026#34;, \u0026#34;entryType\u0026#34;: \u0026#34;kv\u0026#34; }Parameters:\noperation: \u0026quot;put\u0026quot;, \u0026quot;update\u0026quot;, or \u0026quot;delete\u0026quot; entryType: \u0026quot;kv\u0026quot;, \u0026quot;row\u0026quot;, or \u0026quot;lob\u0026quot; Example:\ncurl -X POST http://localhost:4000/api/v1/default/tx/begin \\ -d \u0026#39;{ \u0026#34;operation\u0026#34;: \u0026#34;put\u0026#34;, \u0026#34;entryType\u0026#34;: \u0026#34;kv\u0026#34; }\u0026#39;Response (200 OK):\n{ \u0026#34;txnId\u0026#34;: \u0026#34;2a3b4c5d6e7f8g9h0i1j2k3l4m5n6o7p\u0026#34;, \u0026#34;success\u0026#34;: true }Save the txnId - you\u0026rsquo;ll need it for subsequent requests!\nAppend KV to Transaction# Add a key-value operation to the transaction.\nRequest:\nPOST /api/v1/{namespace}/tx/{txnId}/kv Content-Type: application/json { \u0026#34;key\u0026#34;: \u0026#34;mykey\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;bXl2YWx1ZQ==\u0026#34; }Example:\n# Use the txnId from BEGIN response curl -X POST http://localhost:4000/api/v1/default/tx/2a3b4c.../kv \\ -d \u0026#39;{ \u0026#34;key\u0026#34;: \u0026#34;account:alice\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;MTAwMA==\u0026#34; }\u0026#39;Response (200 OK):\n{ \u0026#34;success\u0026#34;: true }Call this endpoint multiple times to add multiple operations to the same transaction.\nAppend Row to Transaction# Add a row operation to the transaction.\nRequest:\nPOST /api/v1/{namespace}/tx/{txnId}/row Content-Type: application/json { \u0026#34;rowKey\u0026#34;: \u0026#34;user:1\u0026#34;, \u0026#34;columns\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;QWxpY2U=\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;YWN0aXZl\u0026#34; } }Example:\ncurl -X POST http://localhost:4000/api/v1/default/tx/{txnId}/row \\ -d \u0026#39;{ \u0026#34;rowKey\u0026#34;: \u0026#34;user:charlie\u0026#34;, \u0026#34;columns\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Q2hhcmxpZQ==\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;Y2hhcmxpZUBleGFtcGxlLmNvbQ==\u0026#34; } }\u0026#39;Response (200 OK):\n{ \u0026#34;success\u0026#34;: true } Append LOB to Transaction# Add a large object to the transaction.\nRequest:\nPOST /api/v1/{namespace}/tx/{txnId}/lob?key={key} Content-Type: application/octet-stream \u0026lt;binary data\u0026gt;Example:\ncurl -X POST \u0026#34;http://localhost:4000/api/v1/default/tx/{txnId}/lob?key=file:backup.tar.gz\u0026#34; \\ --data-binary @backup.tar.gzResponse (200 OK):\n{ \u0026#34;success\u0026#34;: true }Handling Large Objects (LOB \u0026gt; 1MB)# For files or data larger than 1MB, use LOB transactions with chunking:\nExample: Upload a 5MB File# #!/bin/bash FILE=\u0026#34;large-file.bin\u0026#34; KEY=\u0026#34;files:backup-20250108.tar.gz\u0026#34; CHUNK_SIZE=1048576 # 1MB chunks # 1. Begin LOB transaction RESPONSE=$(curl -s -X POST http://localhost:4000/api/v1/default/tx/begin \\ -d \u0026#39;{\u0026#34;operation\u0026#34;:\u0026#34;put\u0026#34;,\u0026#34;entryType\u0026#34;:\u0026#34;lob\u0026#34;}\u0026#39;) TXN_ID=$(echo $RESPONSE | jq -r \u0026#39;.txnId\u0026#39;) # 2. Split file into 1MB chunks and upload split -b $CHUNK_SIZE \u0026#34;$FILE\u0026#34; /tmp/chunk_ for CHUNK in /tmp/chunk_*; do echo \u0026#34;Uploading $CHUNK...\u0026#34; curl -X POST \u0026#34;http://localhost:4000/api/v1/default/tx/$TXN_ID/lob?key=$KEY\u0026#34; \\ --data-binary @\u0026#34;$CHUNK\u0026#34; done # 3. Commit transaction curl -X POST http://localhost:4000/api/v1/default/tx/$TXN_ID/commit # 4. Cleanup rm /tmp/chunk_* echo \u0026#34;Large file uploaded successfully!\u0026#34; Commit Transaction# Apply all operations atomically.\nRequest:\nPOST /api/v1/{namespace}/tx/{txnId}/commitExample:\ncurl -X POST http://localhost:4000/api/v1/default/tx/2a3b4c.../commitResponse (200 OK):\n{ \u0026#34;success\u0026#34;: true }After commit:\nAll operations are applied atomically Transaction ID is no longer valid Data is durable and replicated Abort Transaction# Cancel the transaction without applying changes.\nRequest:\nPOST /api/v1/{namespace}/tx/{txnId}/abortExample:\ncurl -X POST http://localhost:4000/api/v1/default/tx/2a3b4c.../abortResponse (200 OK):\n{ \u0026#34;success\u0026#34;: true }After abort:\nNo operations are applied Transaction ID is no longer valid Metadata Operations# Get Current Offset# Get the current WAL position.\nRequest:\nGET /api/v1/{namespace}/offsetExample:\ncurl http://localhost:4000/api/v1/default/offsetResponse (200 OK):\n{ \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;segmentId\u0026#34;: 5, \u0026#34;offset\u0026#34;: 12345 } Get Engine Statistics# Get engine performance statistics.\nRequest:\nGET /api/v1/{namespace}/statsExample:\ncurl http://localhost:4000/api/v1/default/statsResponse (200 OK):\n{ \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;opsReceived\u0026#34;: 15234, \u0026#34;opsFlushed\u0026#34;: 15100, \u0026#34;currentSegment\u0026#34;: 5, \u0026#34;currentOffset\u0026#34;: 12345, \u0026#34;lastFlushTime\u0026#34;: \u0026#34;2024-01-15T10:30:45Z\u0026#34; } Get Checkpoint# Get the last checkpoint position.\nRequest:\nGET /api/v1/{namespace}/checkpointExample:\ncurl http://localhost:4000/api/v1/default/checkpointResponse (200 OK):\n{ \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;recordProcessed\u0026#34;: 15000, \u0026#34;segmentId\u0026#34;: 5, \u0026#34;offset\u0026#34;: 12000 } Backup Operations# UnisonDB provides APIs for creating durable backups of both WAL segments and B-Tree snapshots. All backup paths must be relative to the server\u0026rsquo;s backup root (\u0026lt;dataDir\u0026gt;/backups/{namespace}).\nWAL Segment Backup# Create an incremental backup by copying sealed WAL segments.\nRequest:\nPOST /api/v1/{namespace}/wal/backup Content-Type: application/json { \u0026#34;afterSegmentId\u0026#34;: 42, \u0026#34;backupDir\u0026#34;: \u0026#34;wal/customer-a\u0026#34; }Parameters:\nafterSegmentId (optional): Only copy segments with IDs greater than this value. Omit or set to 0 to copy all sealed segments. backupDir (required): Relative path within the backup root. Absolute paths or .. traversal are rejected. Example:\n# Backup all segments after segment 100 curl -X POST http://localhost:4000/api/v1/default/wal/backup \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;afterSegmentId\u0026#34;: 100, \u0026#34;backupDir\u0026#34;: \u0026#34;wal/daily\u0026#34; }\u0026#39;Response (200 OK):\n{ \u0026#34;backups\u0026#34;: [ { \u0026#34;segmentId\u0026#34;: 101, \u0026#34;path\u0026#34;: \u0026#34;/var/unison/data/backups/default/wal/daily/000000101.wal\u0026#34; }, { \u0026#34;segmentId\u0026#34;: 102, \u0026#34;path\u0026#34;: \u0026#34;/var/unison/data/backups/default/wal/daily/000000102.wal\u0026#34; } ] }Use Cases:\nIncremental backups for point-in-time recovery Compliance archival of transaction logs Shipping WAL segments to remote storage Errors:\n400 Bad Request: Invalid path (absolute or contains ..) 404 Not Found: Namespace not found 500 Internal Server Error: Filesystem error, permission denied B-Tree Snapshot Backup# Create a full snapshot of the B-Tree store.\nRequest:\nPOST /api/v1/{namespace}/btree/backup Content-Type: application/json { \u0026#34;path\u0026#34;: \u0026#34;snapshots/users-20250108.snapshot\u0026#34; }Parameters:\npath (required): Relative path within the backup root. The server writes to {path}.tmp, fsyncs, then atomically renames. Example:\n# Create a snapshot with today\u0026#39;s date curl -X POST http://localhost:4000/api/v1/default/btree/backup \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;path\u0026#34;: \u0026#34;snapshots/backup-\u0026#39;$(date +%Y%m%d)\u0026#39;.db\u0026#34; }\u0026#39;Response (200 OK):\n{ \u0026#34;path\u0026#34;: \u0026#34;/var/unison/data/backups/default/snapshots/backup-20250108.db\u0026#34;, \u0026#34;bytes\u0026#34;: 73400320 }Use Cases:\nFull database backups for disaster recovery Creating read-only replicas for analytics Migrating data to new servers Errors:\n400 Bad Request: Invalid path (absolute or contains ..) 404 Not Found: Namespace not found 500 Internal Server Error: Filesystem error, disk full Backup Workflow Example# Complete backup automation script:\n#!/bin/bash # Daily backup script NAMESPACE=\u0026#34;users\u0026#34; DATE=$(date +%Y%m%d) STATE_FILE=\u0026#34;/var/lib/unison/wal-state.json\u0026#34; # 1. B-Tree snapshot (daily) echo \u0026#34;Creating B-Tree snapshot...\u0026#34; curl -X POST http://localhost:4000/api/v1/$NAMESPACE/btree/backup \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#34;{\\\u0026#34;path\\\u0026#34;: \\\u0026#34;snapshots/btree-$DATE.db\\\u0026#34;}\u0026#34; # 2. WAL incremental echo \u0026#34;Backing up WAL segments...\u0026#34; LAST_SEGMENT=$(jq -r \u0026#39;.lastSegmentId // 0\u0026#39; \u0026#34;$STATE_FILE\u0026#34; 2\u0026gt;/dev/null || echo 0) RESPONSE=$(curl -s -X POST http://localhost:4000/api/v1/$NAMESPACE/wal/backup \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#34;{\\\u0026#34;afterSegmentId\\\u0026#34;: $LAST_SEGMENT, \\\u0026#34;backupDir\\\u0026#34;: \\\u0026#34;wal/$DATE\\\u0026#34;}\u0026#34;) # 3. Update state NEW_LAST=$(echo \u0026#34;$RESPONSE\u0026#34; | jq -r \u0026#39;.backups[-1].segmentId // 0\u0026#39;) if [ \u0026#34;$NEW_LAST\u0026#34; != \u0026#34;0\u0026#34; ]; then echo \u0026#34;{\\\u0026#34;lastSegmentId\\\u0026#34;: $NEW_LAST, \\\u0026#34;timestamp\\\u0026#34;: \\\u0026#34;$(date -Iseconds)\\\u0026#34;}\u0026#34; \u0026gt; \u0026#34;$STATE_FILE\u0026#34; fi # 4. Compress and upload to S3 (optional) BACKUP_ROOT=\u0026#34;/var/unison/data/backups/$NAMESPACE\u0026#34; tar -czf \u0026#34;/tmp/backup-$DATE.tar.gz\u0026#34; -C \u0026#34;$BACKUP_ROOT\u0026#34; . aws s3 cp \u0026#34;/tmp/backup-$DATE.tar.gz\u0026#34; \u0026#34;s3://backups/unison/$NAMESPACE/\u0026#34; echo \u0026#34;Backup completed successfully\u0026#34;For detailed backup strategies, automation, and restore procedures, see the Backup and Restore Guide .\nError Responses# All errors follow this format:\n{ \u0026#34;error\u0026#34;: \u0026#34;error message description\u0026#34; }HTTP Status Codes# Code Meaning Example 200 Success Operation completed 400 Bad Request Invalid base64, malformed JSON 404 Not Found Namespace not found, key not found, transaction not found 500 Internal Server Error Engine error, disk full, WAL error Common Errors# Namespace not found:\n{ \u0026#34;error\u0026#34;: \u0026#34;namespace not found: invalid-ns\u0026#34; }Status: 404 Not Found\nTransaction not found:\n{ \u0026#34;error\u0026#34;: \u0026#34;transaction not found: 2a3b4c5d...\u0026#34; }Status: 404 Not Found\nInvalid base64:\n{ \u0026#34;error\u0026#34;: \u0026#34;invalid base64 encoding\u0026#34; }Status: 400 Bad Request\nEngine error:\n{ \u0026#34;error\u0026#34;: \u0026#34;failed to write: disk full\u0026#34; }Status: 500 Internal Server Error\nComplete Transaction Example# #!/bin/bash # 1. Begin transaction RESPONSE=$(curl -s -X POST http://localhost:4000/api/v1/default/tx/begin \\ -d \u0026#39;{\u0026#34;operation\u0026#34;:\u0026#34;put\u0026#34;,\u0026#34;entryType\u0026#34;:\u0026#34;kv\u0026#34;}\u0026#39;) TXN_ID=$(echo $RESPONSE | jq -r \u0026#39;.txnId\u0026#39;) echo \u0026#34;Transaction ID: $TXN_ID\u0026#34; # 2. Append multiple operations curl -X POST http://localhost:4000/api/v1/default/tx/$TXN_ID/kv \\ -d \u0026#39;{\u0026#34;key\u0026#34;:\u0026#34;account:alice:balance\u0026#34;,\u0026#34;value\u0026#34;:\u0026#34;MTAwMA==\u0026#34;}\u0026#39; curl -X POST http://localhost:4000/api/v1/default/tx/$TXN_ID/kv \\ -d \u0026#39;{\u0026#34;key\u0026#34;:\u0026#34;account:bob:balance\u0026#34;,\u0026#34;value\u0026#34;:\u0026#34;MjAwMA==\u0026#34;}\u0026#39; curl -X POST http://localhost:4000/api/v1/default/tx/$TXN_ID/kv \\ -d \u0026#39;{\u0026#34;key\u0026#34;:\u0026#34;transfer:log:123\u0026#34;,\u0026#34;value\u0026#34;:\u0026#34;YWxpY2UgLT4gYm9iOiAxMDA=\u0026#34;}\u0026#39; # 3. Commit curl -X POST http://localhost:4000/api/v1/default/tx/$TXN_ID/commit echo \u0026#34;Transaction committed!\u0026#34; # 4. Verify curl http://localhost:4000/api/v1/default/kv/account:alice:balance "}]